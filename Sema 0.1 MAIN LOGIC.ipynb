{"cells":[{"cell_type":"markdown","metadata":{"id":"0N0o6eLorq6K","deepnote_app_block_visible":true,"cell_id":"10feb2e288194cd4be7eadcd85a07f4d","deepnote_cell_type":"markdown"},"source":"# SEMA Semantic Agent. Arxiv search powered by LLMs","block_group":"5433b36a96a84de4a5e7f3fd312964bc"},{"cell_type":"markdown","metadata":{"id":"DZtnkrGgcPZd","deepnote_app_block_visible":true,"cell_id":"8fd14e7465dc48159f0d4493170ace6a","deepnote_cell_type":"markdown"},"source":"What it does:\n- Convert user query into keyword search queries\n- Google search top 10 results with SERP API\n- Scrape html for each result, convert to markdown\n- Structure output using function calling -> json to get paper, title\n- Call arxiv to get paper, abstract, metadata\n- Call Google Scholar to get citations, ...\n- Use LLM to answer user query based on the paper, evaluate answer relevance\n- Rank results based on citations, relevance to user query\n- Print results in structured format, give links to download, or to use in notebook LM","block_group":"8ac24ea88e3c4de9bd85ad5825ecaadf"},{"cell_type":"markdown","metadata":{"id":"xLlMdqkpwwFV","deepnote_app_block_visible":true,"cell_id":"9b284d467385405697127d5f6d19d5dc","deepnote_cell_type":"markdown"},"source":"# Setup","block_group":"a4064cec25cc4539b8783c1c50a4ff7b"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"a7aa33f707ad49f290f4ac570e4994a3","deepnote_cell_type":"text-cell-p"},"source":"load secret variables","block_group":"61701f62befd4d9188a9d672a2342f2c"},{"cell_type":"code","metadata":{"id":"udzkaNSJlQtZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8afda221-8b65-4c34-cb39-a98c4c8f5957","source_hash":"a0195894","execution_start":1710468013173,"execution_millis":11809,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"f544246d8b4b4dad9a505de9fd73b72e","deepnote_cell_type":"code"},"source":"import os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nopenai_api_key = os.environ.get(\"OPENAI_API_KEY\")\nserp_api_key = os.environ.get(\"SERP_API_KEY\")\ngemini_api_key = os.environ.get(\"GEMINI_API_KEY\")\nllamaindex_api_key = os.environ.get(\"LLAMAINDEX_API_KEY\")\nperplexity_api_key = os.environ.get(\"PERPLEXITY\")\n\n# Hide part of the key\nopenai_api_key_hidden = openai_api_key[:3] + \"*\" * (len(openai_api_key) - 6) + openai_api_key[-3:]\nserp_api_key_hidden = serp_api_key[:3] + \"*\" * (len(serp_api_key) - 6) + serp_api_key[-3:]\ngemini_api_key_hidden = gemini_api_key[:3] + \"*\" * (len(gemini_api_key) - 6) + gemini_api_key[-3:]\nllamaindex_api_key_hidden = llamaindex_api_key[:3] + \"*\" * (len(llamaindex_api_key) - 6) + llamaindex_api_key[-3:]\nperplexity_api_key_hidden = perplexity_api_key[:3] + \"*\" * (len(perplexity_api_key) - 6) + perplexity_api_key[-3:]\n\n# Print the hidden keys\nprint(f\"OpenAI API Key (hidden): {openai_api_key_hidden}\")\nprint(f\"Serp API Key (hidden): {serp_api_key_hidden}\")\nprint(f\"Gemini API Key (hidden): {gemini_api_key_hidden}\")\nprint(f\"Llamaindex API Key (hidden): {llamaindex_api_key_hidden}\")\nprint(f\"Perplexity API Key (hidden): {perplexity_api_key_hidden}\")","block_group":"ec6919e96c944f4ca1306d0196fb4368","execution_count":212,"outputs":[{"name":"stdout","text":"OpenAI API Key (hidden): sk-*********************************************0jF\nSerp API Key (hidden): 68c**********************************************************266\nGemini API Key (hidden): AIz*********************************MUc\nLlamaindex API Key (hidden): llx**********************************************hA3\nPerplexity API Key (hidden): ppl***********************************************ffa\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/e064a6f9-f660-4bc4-8326-d543e82f0392"},{"cell_type":"markdown","metadata":{"id":"f5hOKcVZGXDC","deepnote_app_block_visible":true,"cell_id":"edd872556d404844a30c5cd6893483a4","deepnote_cell_type":"markdown"},"source":"### Put data into a local database","block_group":"5cb36415784b4a82b85ac5ad8e784e5a"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"3581664c78784171bb50391685060a4d","deepnote_cell_type":"text-cell-p"},"source":"establish db connection","block_group":"bbce0bed79c448a8a74c0b1b48b453b6"},{"cell_type":"code","metadata":{"source_hash":"3ae08e6d","execution_start":1710468013288,"execution_millis":11969,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"9bc33b69c73a45e19ec264a0048ab348","deepnote_cell_type":"code"},"source":"import psycopg2\nimport os\n\ndef connection():\n    \"\"\"Creates and returns a new database connection.\"\"\"\n    try:\n        conn = psycopg2.connect(\n            user=os.environ[\"MY_INTEGRATION_USER\"],\n            password=os.environ[\"MY_INTEGRATION_PASSWORD\"],\n            host=os.environ[\"MY_INTEGRATION_HOST\"],\n            port=os.environ[\"MY_INTEGRATION_PORT\"],\n            database=os.environ[\"MY_INTEGRATION_DATABASE\"]\n        )\n        \n        # Test the connection\n        with conn.cursor() as cursor:\n            cursor.execute(\"SELECT version();\")\n            record = cursor.fetchone()\n            # print(\"You are connected to - \", record)\n        \n        return conn  # Return the connection object if successful\n\n    except (Exception, psycopg2.Error) as error:\n        print(\"Error while connecting to database\", error)\n        return None  # Return None if connection was not successful\n\nconn = connection()","block_group":"7e1738211d2d488c96250a71c0085ae8","execution_count":213,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"e638495c","execution_start":1710468013793,"execution_millis":11464,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"a2572799a8234ac38da29b8bda300cbc","deepnote_cell_type":"code"},"source":"DSN = (\n    f\"dbname={os.environ['MY_INTEGRATION_DATABASE']} \"\n    f\"user={os.environ['MY_INTEGRATION_USER']} \"\n    f\"password={os.environ['MY_INTEGRATION_PASSWORD']} \"\n    f\"host={os.environ['MY_INTEGRATION_HOST']} \"\n    f\"port={os.environ['MY_INTEGRATION_PORT']}\"\n)","block_group":"2643c8a774b7412c96faa26cfc72dfd9","execution_count":214,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"cb5db21b","execution_start":1710468013798,"execution_millis":11459,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"70492ce7597342ed9c27db0807c987d0","deepnote_cell_type":"code"},"source":"import psycopg2\nimport os\n\n# Function to create tables in the database\ndef create_tables():\n    # Define your SQL statements for creating tables\n    sql_commands = [\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS google_search_results (\n            url TEXT PRIMARY KEY,\n            html TEXT,\n            scraping_status TEXT,\n            processed_markdown TEXT,\n            query TEXT\n            title TEXT,\n            snippet TEXT,\n            job_id INT\n        );\n        \"\"\",\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS Papers (\n            id SERIAL PRIMARY KEY,\n            paper_title TEXT,\n            source_content TEXT,\n            links TEXT,\n            arxiv_link TEXT UNIQUE,\n            arxiv_title TEXT,\n            arxiv_abstract TEXT,\n            arxiv_metadata TEXT,\n            arxiv_filename TEXT,\n            arxiv_paper_markdown TEXT,\n            citations INTEGER,\n            versions INTEGER\n        );\n        \"\"\",\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS Query_Papers (\n            id SERIAL PRIMARY KEY,\n            query TEXT,\n            arxiv_link TEXT,\n            relevance_score REAL,\n            final_rank INTEGER,\n            relevant_answer TEXT,\n            paper_stats TEXT,\n            paper_metadata_filtered TEXT,\n            download_link TEXT,\n            relevant_snippets TEXT,\n            job_id INT,\n            CONSTRAINT unique_query_arxiv_link UNIQUE (query, arxiv_link)\n        );\n        \"\"\",\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS jobs (\n            job_id SERIAL PRIMARY KEY,\n            query TEXT,\n            job_status TEXT,\n            printed_ranks INTEGER DEFAULT 0,\n            terminal_output TEXT\n            gpt_response TEXT\n            perplexity_response TEXT\n            final_response TEXT\n            keyword_search_queries TEXT\n            paper_search_queries TEXT\n        );\n        \"\"\"\n    ]\n    try:\n        with conn.cursor() as cursor:\n            # Execute each SQL command separately\n            for sql_command in sql_commands:\n                cursor.execute(sql_command)\n            conn.commit()  # Commit the transaction\n            print(\"All tables are created successfully.\")\n\n    except (Exception, psycopg2.Error) as error:\n        print(\"Failed to create tables\", error)\n        conn.rollback()  # Rollback the transaction on error\n\n    finally:\n        if conn:\n            conn.close()\n            print(\"Database connection is closed.\")\n# Main script execution\ntry:\n    connection()\n    create_tables()\n\nexcept (Exception, psycopg2.Error) as error:\n    print(\"Error while connecting to database\", error)\n","block_group":"bfd069d9767d431abab154ac1cdd1f1b","execution_count":215,"outputs":[{"name":"stdout","text":"Failed to create tables syntax error at or near \"title\"\nLINE 8:             title TEXT,\n                    ^\n\nDatabase connection is closed.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/672c4bc1-6db3-4a7c-b23f-f036c4d1c27b"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"3ab3a80b14c04343a479fd26120a5984","deepnote_cell_type":"text-cell-p"},"source":"clean arxiv URLs","block_group":"a4d30cc6517c4fecb7f6c74b5ad87b1b"},{"cell_type":"code","metadata":{"source_hash":"9df5559a","execution_start":1710468014449,"execution_millis":10809,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"a53af3e2466848949d796bf937afafbe","deepnote_cell_type":"code"},"source":"import re\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import unquote, urlparse, parse_qs\n\ndef extract_arxiv_url_from_wrapped_url(wrapped_url):\n    # Unquote to handle deeply nested or doubly-encoded URLs\n    wrapped_url = unquote(wrapped_url)\n    # print('Wrapped URL:', wrapped_url)\n    parsed_url = urlparse(wrapped_url)\n    # print('Parsed URL:', parsed_url)\n    query_params = parse_qs(parsed_url.query)\n    # print('Query Parameters:', query_params)\n    # Explore all possible URLs found in the 'url' parameter and extract the arXiv link\n    if 'url' in query_params:\n        for possible_url in query_params['url']:\n            # Check if we have nested URLs and extract the innermost one\n            while 'url=' in possible_url:\n                inner_parsed = parse_qs(urlparse(possible_url).query)\n                if 'url' in inner_parsed:\n                    possible_url = inner_parsed['url'][0]\n                else:\n                    break  # Break if no inner 'url' parameter is found\n            possible_url = unquote(possible_url)  # Ensure the inner URL is fully decoded\n            # print('Possible URL after extraction:', possible_url)\n            \n            # Now check if the final extracted URL is an arXiv link\n            if 'arxiv.org' in possible_url:\n                # print('Extracted arXiv URL:', possible_url)\n                return possible_url  # Return the first arXiv URL found\n    return None  # Return None if no arXiv URL is found\n\ndef clean_arxiv_link(link):\n    # Clean the extracted arXiv link\n    link = unquote(link)  # Ensure the link is fully decoded\n    link_obj = urlparse(link)\n\n    # Handle typical arXiv link structures for abstract and PDF\n    if re.search(r'(/abs/|/pdf/)[0-9]+\\.[0-9]+', link_obj.path):\n        clean_path = re.sub(r'/pdf/', '/abs/', link_obj.path.split('.pdf')[0])\n        clean_path = re.sub(r'v\\d+$', '', clean_path)  # Remove versioning\n        return f\"https://arxiv.org{clean_path}\"\n    # Handle FTP PDF links\n    elif re.search(r'/ftp/arxiv/papers/[0-9]{4}/[0-9]{4}\\.[0-9]+\\.pdf', link_obj.path):\n        paper_id = re.findall(r'/ftp/arxiv/papers/[0-9]{4}/([0-9]{4}\\.[0-9]+)\\.pdf', link_obj.path)[0]\n        return f\"https://arxiv.org/abs/{paper_id}\"  # Convert to standard abstract link\n    return None","block_group":"f5215cc8fe704aad818cb23c35abbf88","execution_count":216,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"e39b56d742ba4ad9854751e45f6f1040","deepnote_cell_type":"text-cell-p"},"source":"insert arxiv URLs into db","block_group":"30b2bbc097554804a76a3ecf10a296b1"},{"cell_type":"code","metadata":{"source_hash":"e5e25825","execution_start":1710468014538,"execution_millis":10720,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"c6edc98d5963446393ebd3f2f109b5ed","deepnote_cell_type":"code"},"source":"def insert_arxiv_links_and_snippets_into_db(arxiv_links_dict, user_query):\n    cleaned_arxiv_links = []\n    snippets = []\n\n    # Clean the arXiv links and prepare snippets\n    for link, snippet in arxiv_links_dict.items():\n        cleaned_link = clean_arxiv_link(link)\n        if cleaned_link:\n            cleaned_arxiv_links.append(cleaned_link)\n            snippets.append(snippet)  # Keep the snippet aligned with its cleaned link\n\n    # Insert cleaned links and their corresponding snippets into the database\n    print(f\"Cleaned arXiv links[{len(cleaned_arxiv_links)}]: {cleaned_arxiv_links}\")\n    if cleaned_arxiv_links and snippets:\n        try:\n            conn = connection()  # Establish your database connection here\n            c = conn.cursor()\n\n            # Insert arxiv links into Papers table in bulk\n            arxiv_links_data = [(link,) for link in cleaned_arxiv_links]\n            psycopg2.extras.execute_batch(\n                c,\n                \"INSERT INTO Papers (arxiv_link) VALUES (%s) ON CONFLICT (arxiv_link) DO NOTHING\",\n                arxiv_links_data\n            )\n\n            # Insert records associated with user query and their snippets in Query_Papers table in bulk\n            query_papers_data = [(user_query, link, snippet) for link, snippet in zip(cleaned_arxiv_links, snippets)]\n            psycopg2.extras.execute_batch(\n                c,\n                \"INSERT INTO Query_Papers (query, arxiv_link, relevant_snippets) VALUES (%s, %s, %s) ON CONFLICT (query, arxiv_link) DO NOTHING\",\n                query_papers_data\n            )\n\n            # Commit the transaction\n            conn.commit()\n            print(f\"Successfully inserted records associated with the query '{user_query}' into the database. Links: {cleaned_arxiv_links}\")\n        except Exception as e:\n            # Rollback any changes if an error occurs\n            conn.rollback()\n            print(f\"Transaction rolled back. Error occurred: {e}\")\n        finally:\n            if conn:\n                conn.close()","block_group":"bbdaa10de320426db39fc5192420b41d","execution_count":217,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"allow_embed":false,"source_hash":"638911d6","execution_start":1710486670844,"execution_millis":43,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"db07a8f768c042eca66d7465dcdf7d08","deepnote_cell_type":"code"},"source":"def insert_first_arxiv_link_into_db(arxiv_links_dict, user_query):\n    # Extract the first (and only) cleaned arXiv link and its snippet from the dictionary\n    if arxiv_links_dict:\n        cleaned_arxiv_link, snippet = next(iter(arxiv_links_dict.items()))\n        try:\n            conn = connection()  # Establish your database connection here\n            c = conn.cursor()\n\n            # Insert arxiv link into Papers table\n            c.execute(\n                \"INSERT INTO Papers (arxiv_link) VALUES (%s) ON CONFLICT (arxiv_link) DO NOTHING\",\n                (cleaned_arxiv_link,)\n            )\n\n            # Insert record associated with user query and its snippet into Query_Papers table\n            c.execute(\n                \"INSERT INTO Query_Papers (query, arxiv_link, relevant_snippets) VALUES (%s, %s, %s) ON CONFLICT (query, arxiv_link) DO NOTHING\",\n                (user_query, cleaned_arxiv_link, snippet)\n            )\n\n            # Commit the transaction\n            conn.commit()\n            print(f\"Successfully inserted the record associated with the query '{user_query}' into the database. Link: {cleaned_arxiv_link}\")\n        except Exception as e:\n            # Rollback any changes if an error occurs\n            conn.rollback()\n            print(f\"Transaction rolled back. Error occurred: {e}\")\n        finally:\n            if conn:\n                conn.close()\n    else:\n        print(\"No valid arXiv link to insert into the database.\")","block_group":"27f804c792704c0999c3c1b3886cac58","execution_count":267,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"2cd211be","execution_start":1710483428284,"execution_millis":880,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"3f342ab2cfc84051bf55a1eca6442e18","deepnote_cell_type":"code"},"source":"def insert_arxiv_links_into_db(html_content, user_query, job_id):\n    soup = BeautifulSoup(html_content, 'html.parser')\n    links = soup.find_all('a', href=True)\n    \n    arxiv_links = []\n\n    for link in links:\n        href = unquote(link['href']).split(']')[0]  # Decode and preliminarily clean the URL\n        if 'arxiv.org/ct?url=' in href:\n            # Special handling for wrapped links\n            extracted_url = extract_arxiv_url_from_wrapped_url(href)\n            if extracted_url:\n                # Clean and validate the extracted arXiv URL\n                cleaned_link = clean_arxiv_link(extracted_url)\n                if cleaned_link:\n                    arxiv_links.append(cleaned_link)\n                    continue  # Move to the next link after handling a wrapped URL\n\n        # Regular processing for non-wrapped arXiv links\n        if 'arxiv.org' in href and not any(x in href for x in ['/login', '/search', '/about', '/help', '/status']):\n            cleaned_link = clean_arxiv_link(href)\n            if cleaned_link:\n                arxiv_links.append(cleaned_link)\n    \n    # Insert cleaned links into the database\n    # print(f\"Cleaned arXiv links[{len(arxiv_links)}]: {arxiv_links}\")\n    # print(f\"Cleaned arXiv links[{len(arxiv_links)}]:\")\n    # for i, link in enumerate(arxiv_links, start=1):\n    #     print(f\"{i}. {link}\")\n\n    # Insertion into the database would go here - omitted for brevity\n    if arxiv_links:\n        try:\n            conn = connection()\n            c = conn.cursor()\n            # Insert arxiv links into Papers table in bulk if there are any\n            arxiv_links_data = [(link,) for link in arxiv_links]  # Prepare data for bulk insert\n            psycopg2.extras.execute_batch(\n                c, \n                \"INSERT INTO Papers (arxiv_link) VALUES (%s) ON CONFLICT (arxiv_link) DO NOTHING\",\n                arxiv_links_data\n            )\n            \n            # Insert records associated with user query in Query_Papers table in bulk\n            query_papers_data = [(job_id,user_query, link) for link in arxiv_links]  # Prepare data\n            psycopg2.extras.execute_batch(\n                c, \n                \"INSERT INTO Query_Papers (job_id, query, arxiv_link) VALUES (%s, %s, %s) ON CONFLICT (query, arxiv_link) DO NOTHING\",\n                query_papers_data\n            )\n            # Commit the transaction\n            conn.commit()\n            # print(f\"Successfully inserted records associated with the query '{user_query}' into the database.\")\n\n        except Exception as e:\n            # Rollback any changes if an error occurs\n            conn.rollback()\n            print(f\"Transaction rolled back. Error occurred: {e}\")\n        if conn:\n            conn.close()\n# Connect to the database\n\n# Example \njob_id = 1\nuser_query = \"Example Query for Testing\"\n# Example HTML content\nhtml_content = \"\"\"\n<html>\n    <body>\n        <p>Here are some arXiv papers that might interest you:</p>\n        <a href=\"https://arxiv.org/abs/12457457234623434.56789\">Paper 1</a>\n        <a href=\"https://arxiv.org/abs/98724723463246234623466.54321\">Paper 2</a>\n        <a href=\"http://example.com\">Non-arXiv link</a>\n        <a href=\"https://arxiv.org/abs/11223472347234722.3344\">Paper 3</a>\n        <a href=\"https://info.arxiv.org/help/submit_latex_best_practices.html\">LaTeX Best Practices</a>\n        <a href=\"https://info.dev.arxiv.org/about/accessibility_html_error_messages.html\">Accessibility Info</a>\n        <a href=\"https://arxiv.org/abs/2112.08726v1\">Versioned Paper</a>\n        <a href=\"https://arxiv.org/pdf/2112.08726.pdf\">PDF Paper</a>\n        <a href=\"https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf\">FTP PDF Paper</a>\n        <a href=\"https://arxiv.org/pdf/2310.06825.pdf%5D%5D%3E\">Dirty PDF Link</a>\n        <a href=\"https://arxiv.org/abs/2308.09687v2\">Versioned Paper 2</a>\n        <a href=\"https://arxiv.org/ct?url=http://www.bibsonomy.org/BibtexHandler?requTask%3Dupload%26url%3Dhttps://arxiv.org/abs/2402.10200%26description%3DChain-of-Thought+Reasoning+Without+Prompting&v=50e87f9b\">Wrapped Link 1</a>\n        <a href=\"https://arxiv.org/ct?url=https://reddit.com/submit?url%3Dhttps://arxiv.org/abs/2402.99200%26title%3DChain-of-Thought+Reasoning+Without+Prompting&v=8392271f\">Wrapped Link 2</a>\n        <a href=\"https://arxiv.org/login\">Login Page</a>\n        <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a>\n        <a href=\"https://arxiv.org/search/cs?searchtype=author&query=Wang,+X\">Author Search 1</a>\n        <a href=\"https://arxiv.org/search/cs?searchtype=author&query=Zhou,+D\">Author Search 2</a>\n        <a href=\"https://info.arxiv.org/about\">About arXiv</a>\n        <a href=\"https://info.arxiv.org/about/donate.html\">Donate to arXiv</a>\n        <a href=\"https://info.arxiv.org/about/ourmembers.html\">Our Members</a>\n        <a href=\"https://info.arxiv.org/help\">Help Page</a>\n        <a href=\"https://info.arxiv.org/help/contact.html\">Contact Page</a>\n        <a href=\"https://status.arxiv.org\">Status Page</a>\n        <p>Here are some arXiv papers and other links that might interest you:</p>\n        <a href=\"https://arxiv.org/pdf/2010.02903.pdf\">Citations and Versions</a>\n        <a href=\"https://arxiv.org/ct?url=http://www.bibsonomy.org/BibtexHandler?requTask%3Dupload%26url%3Dhttps://arxiv.org/abs/2201.11903%26description%3DChain-of-Thought+Prompting+Elicits+Reasoning+in+Large+Language+Models&v=be299b0a\">Wrapped Link for Chain-of-Thought Prompting</a>\n        <a href=\"https://arxiv.org/login\">Login Page</a>\n        <a href=\"https://arxiv.org/search/cs?searchtype=author&query=Gao,+Y\">Author Search for Gao, Y</a>\n        <a href=\"https://arxiv.org/abs/2005.11401v4\">arXiv Paper Version 4</a>\n        <a href=\"https://arxiv.org/pdf/2310.06825.pdf%5D%5D%3E\">Dirty PDF Link</a>\n        <a href=\"https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf\">FTP PDF Paper</a>\n        <a href=\"https://llamahub.ai/l/llama-packs/llama-index-packs-node-parser-semantic-chunking?from=all\">LLamaHub AI Node Parser</a>\n        <a href=\"https://arxiv.org/abs/2310.06147\">arXiv Paper</a>\n    </body>\n</html>\n\"\"\"\ninsert_arxiv_links_into_db(html_content, user_query, job_id)","block_group":"1b61ac5205dc4d72a902767398261840","execution_count":260,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"7f5101fe3aa04b439cdc2e588d1bddc5","deepnote_cell_type":"text-cell-p"},"source":"google search","block_group":"eac20d8d249b4d5ca3fde64688b9c344"},{"cell_type":"code","metadata":{"id":"kQlg-GSNcDkc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a710e607-5778-4782-800c-0d18a9c60cde","source_hash":"ed224f34","execution_start":1710468015191,"execution_millis":10067,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"a1aca2dfefa54140b8c63124b5f6fd24","deepnote_cell_type":"code"},"source":"import requests\nimport json\n\n# Set up your SERP API key\n# It's better to use an environment variable for API keys\n\ndef search_google(query):\n    params = {\n        \"engine\": \"google\",\n        \"q\": query,\n        \"api_key\": serp_api_key,\n        \"location\": \"San Francisco Bay Area, United States\",\n        \"google_domain\": \"google.com\",\n        \"gl\": \"us\",\n        \"hl\": \"en\",\n        \"num\": \"10\"\n    }\n    response = requests.get(\"https://serpapi.com/search\", params=params)\n    response.raise_for_status()  # Raises an HTTPError if the HTTP request returned an unsuccessful status code\n    results = response.json()\n    # Extracting only the needed information\n    formatted_data = {\n        \"organic_results\": [\n            {\n                \"link\": result[\"link\"],\n                \"title\": result[\"title\"],\n                \"snippet\": result.get(\"snippet\", \"\")\n            } for result in results.get(\"organic_results\", [])\n        ]\n    }\n    # Assuming search_results is your JSON dictionary obtained from the search\n    organic_results = formatted_data.get('organic_results', [])\n\n    # Initialize an empty list to store all the links\n    all_links = []\n\n    # Loop through each result in the organic results\n    for result in results.get(\"organic_results\", []):\n        # Extract the link, title, and snippet if they exist and add them to the list\n        link = result.get(\"link\", \"\")\n        title = result.get(\"title\", \"\")\n        snippet = result.get(\"snippet\", \"\")\n        if link:  # Only add to list if link is present\n            all_links.append({\"link\": link, \"title\": title, \"snippet\": snippet})\n\n    # Process search results to extract arXiv links and snippets\n    arxiv_links_dict = {}\n    for result in results.get(\"organic_results\", []):\n        link = result.get(\"link\", \"\")\n        snippet = result.get(\"snippet\", \"\")\n        cleaned_link = clean_arxiv_link(link)\n        if cleaned_link:  # Only add to dict if link is an arXiv link\n            arxiv_links_dict[cleaned_link] = snippet\n\n    # Now call your function to insert arXiv links and snippets into the database\n    if arxiv_links_dict:  # Only attempt to insert if we have arXiv links\n        insert_arxiv_links_and_snippets_into_db(arxiv_links_dict, query)\n\n\n    return all_links\n\n# Example usage\nquery = \"how many experts are queries routed to in deepseek moe\"\n# query = \"Top academic papers on LLMs\"\nsearch_google(query)","block_group":"e455b5ec168e43f2b26a6e7d94fd7da3","execution_count":219,"outputs":[{"name":"stdout","text":"Cleaned arXiv links[1]: ['https://arxiv.org/abs/2401.06066']\nSuccessfully inserted records associated with the query 'how many experts are queries routed to in deepseek moe' into the database. Links: ['https://arxiv.org/abs/2401.06066']\n","output_type":"stream"},{"output_type":"execute_result","execution_count":219,"data":{"text/plain":"[{'link': 'https://arxiv.org/html/2401.06066v1',\n  'title': 'DeepSeekMoE: Towards Ultimate Expert Specialization in ...',\n  'snippet': 'DeepSeekMoE has 1 shared expert and 63 routed experts, where each expert is 0.25 times the size of a standard FFN. Including DeepSeekMoE, all compared models ...'},\n {'link': 'https://arxiv.org/pdf/2401.06066',\n  'title': 'DeepSeekMoE: Towards Ultimate Expert Specialization in ...',\n  'snippet': 'Each MoE layer consists of 2 shared experts and 64 routed experts, where each expert is 0.25 times the size of a standard FFN. Each token ...'},\n {'link': 'https://www.linkedin.com/posts/pramodith_deepseek-mixture-of-experts-moe-proposes-activity-7152253019256991744-97Xz',\n  'title': \"Pramodith B.'s Post\",\n  'snippet': 'Most MoE models route inputs to a handful of experts i.e. 1 or 2. 🎛️. Where the outputs of each expert can be fused based on a relevance score.'},\n {'link': 'https://medium.com/@bnjmn_marie/deepseekmoe-moe-with-segmented-and-shared-experts-dedf22e4a98c',\n  'title': 'DeepSeekMoE: MoE with Segmented and Shared Experts',\n  'snippet': \"In this new work, DeepSeek AI proposes two improvements to the mixture of experts: Expert segmentation: By splitting the expert's Feedforward ...\"},\n {'link': 'https://huggingface.co/deepseek-ai/deepseek-moe-16b-chat/commit/f229d1c9b75a3f6feeeb7fecfbd790187e864246',\n  'title': 'initial commit · deepseek-ai/deepseek-moe-16b-chat at ...',\n  'snippet': '+ Number of routed experts, None means dense model. ... Query Attention. ... + \"The bare Deepseek Model outputting raw hidden-states without any ...'},\n {'link': 'https://portal.opencsg.com/models/deepseek-ai/deepseek-moe-16b-base/blob/main/configuration_deepseek.py',\n  'title': 'r',\n  'snippet': 'deepseek-moe-16b-base. 公开. 下载模型. 介绍 ... Number of routed experts, None means dense model. ... Query Attention. If. `num_key_value_heads ...'},\n {'link': 'https://m.facebook.com/groups/3670562573177653/posts/3780682422165667/',\n  'title': 'Machine Learning Large Language Models | **DeepSeekMoE ...',\n  'snippet': 'DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models - Achieves comparable performance with LLaMA2 7B, with only...'},\n {'link': 'https://github.com/XueFuzhao/awesome-mixture-of-experts',\n  'title': 'GitHub - XueFuzhao/awesome-mixture-of-experts',\n  'snippet': 'I list my favorite MoE papers here. I think these papers can greatly help new MoErs to know about this topic. A Review of Sparse Expert Models in Deep Learning ...'},\n {'link': 'https://arxiv-sanity-lite.com/?rank=pid&pid=2204.08396',\n  'title': 'StableMoE: Stable Routing Strategy for Mixture of Experts',\n  'snippet': 'Mixture of Experts (MoE) has become a key ingredient for scaling large foundation models while keeping inference costs steady. We show that expert routing ...'},\n {'link': 'https://www.linkedin.com/posts/pascalbiese_moe-models-bigger-isnt-always-better-activity-7152605656418549760-XlWV',\n  'title': \"Pascal Biese's Post - MoE Models\",\n  'snippet': '... expert routing. DeepSeekMoE now proposes a new way of arranging experts, allowing for a richer mixture and better specialism, alongside ...'}]"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/02eadd44-9b43-4ed4-8995-69389869f609"},{"cell_type":"code","metadata":{"id":"kQlg-GSNcDkc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a710e607-5778-4782-800c-0d18a9c60cde","allow_embed":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"2816fc82d07b4d3ca3e3a8b058c993e4","deepnote_cell_type":"code"},"source":"def search_google_for_specific_papers(query):\n    params = {\n        \"engine\": \"google\",\n        \"q\": query,\n        \"api_key\": serp_api_key,\n        \"location\": \"San Francisco Bay Area, United States\",\n        \"google_domain\": \"google.com\",\n        \"gl\": \"us\",\n        \"hl\": \"en\",\n        \"num\": \"10\"\n    }\n    response = requests.get(\"https://serpapi.com/search\", params=params)\n    response.raise_for_status()  # Raises an HTTPError if the HTTP request returned an unsuccessful status code\n    results = response.json()\n    # Extracting only the needed information\n    formatted_data = {\n        \"organic_results\": [\n            {\n                \"link\": result[\"link\"],\n                \"title\": result[\"title\"],\n                \"snippet\": result.get(\"snippet\", \"\")\n            } for result in results.get(\"organic_results\", [])\n        ]\n    }\n    # Assuming search_results is your JSON dictionary obtained from the search\n    organic_results = formatted_data.get('organic_results', [])\n\n    # Initialize variables to store the first arXiv link and its snippet\n    first_arxiv_link = None\n    first_arxiv_snippet = \"\"\n\n    # Loop through each result in the organic results\n    for result in results.get(\"organic_results\", []):\n        link = result.get(\"link\", \"\")\n        snippet = result.get(\"snippet\", \"\")\n        cleaned_link = clean_arxiv_link(link)\n        if cleaned_link:  # Check if link is an arXiv link and store the first one\n            first_arxiv_link = cleaned_link\n            first_arxiv_snippet = snippet\n            break  # Stop searching once the first arXiv link is found\n\n    # Insert the first arXiv link and its snippet into the database\n    if first_arxiv_link:  # Only attempt to insert if an arXiv link was found\n        arxiv_links_dict = {first_arxiv_link: first_arxiv_snippet}\n        insert_first_arxiv_link_into_db(arxiv_links_dict, query)\n\n    # Return the first arXiv link if found, otherwise return None\n    return first_arxiv_link if first_arxiv_link else None\n\n\n    return all_links\n\n# Example usage\nquery = \"how many experts are queries routed to in deepseek moe\"\n# query = \"Top academic papers on LLMs\"\nsearch_google_for_specific_papers(query)","block_group":"cbdbb33e2f664169a71717ccf77d6e82","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"source_hash":"512e8d6","execution_start":1709515167081,"execution_millis":9,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"8707b09ca5a24cfd814ed8656ea7fee8","deepnote_cell_type":"markdown"},"source":"# Main logic","block_group":"2ee1adc93b4e4ed8884aa912fa071dd0"},{"cell_type":"markdown","metadata":{"id":"PGhfirytcDkd","deepnote_app_block_visible":true,"cell_id":"2e779478d9f547b2a9682a8163537b97","deepnote_cell_type":"markdown"},"source":"### Scrape the content of the page displayed in the search results","block_group":"ee6c7fb7d3ad4320ae7b3997ed1e253d"},{"cell_type":"code","metadata":{"id":"KmYe0sLncDke","colab":{"base_uri":"https://localhost:8080/"},"outputId":"180473ad-84fd-4623-b3bc-500e7b60e2a8","source_hash":"d92a16f6","execution_start":1710468021862,"execution_millis":3396,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"e0bb61fb8e5e45169f873d360edf04fd","deepnote_cell_type":"code"},"source":"import requests\nfrom bs4 import BeautifulSoup\n\ndef fetch_url_content(url):\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n    }\n    response = requests.get(url, headers=headers)\n\n    # Initialize the default response structure\n    result = {\n        \"status\": response.status_code,\n        \"soup\": None\n    }\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        # Adjusts encoding to match what the response seems to use\n        response.encoding = response.apparent_encoding\n        \n        # Now using response.text to utilize the corrected encoding rather than response.content\n        result['soup'] = BeautifulSoup(response.text, 'html.parser')\n    else:\n        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n\n    return result\n\n# Test the function with a URL\nurl = 'https://medium.com/@thedatabeast/top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f'\nresponse = fetch_url_content(url)\nprint(\"Status Code:\", response['status'])\nprint(\"Soup:\", response['soup'])","block_group":"a0e9a61cee0f4dbf8c675a5f13096b69","execution_count":220,"outputs":[{"name":"stdout","text":"Status Code: 200\nSoup: <!DOCTYPE html>\n<html lang=\"en\"><head><title data-rh=\"true\">Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications | by The Data Beast | Medium</title><meta charset=\"utf-8\" data-rh=\"true\"/><meta content=\"width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1\" data-rh=\"true\" name=\"viewport\"/><meta content=\"#000000\" data-rh=\"true\" name=\"theme-color\"/><meta content=\"Medium\" data-rh=\"true\" name=\"twitter:app:name:iphone\"/><meta content=\"828256236\" data-rh=\"true\" name=\"twitter:app:id:iphone\"/><meta content=\"Medium\" data-rh=\"true\" property=\"al:ios:app_name\"/><meta content=\"828256236\" data-rh=\"true\" property=\"al:ios:app_store_id\"/><meta content=\"com.medium.reader\" data-rh=\"true\" property=\"al:android:package\"/><meta content=\"542599432471018\" data-rh=\"true\" property=\"fb:app_id\"/><meta content=\"Medium\" data-rh=\"true\" property=\"og:site_name\"/><meta content=\"article\" data-rh=\"true\" property=\"og:type\"/><meta content=\"2023-11-13T09:09:19.152Z\" data-rh=\"true\" property=\"article:published_time\"/><meta content=\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications | by The Data Beast | Medium\" data-rh=\"true\" name=\"title\"/><meta content=\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering…\" data-rh=\"true\" property=\"og:title\"/><meta content=\"medium://p/7abfcb69da7f\" data-rh=\"true\" property=\"al:android:url\"/><meta content=\"medium://p/7abfcb69da7f\" data-rh=\"true\" property=\"al:ios:url\"/><meta content=\"Medium\" data-rh=\"true\" property=\"al:android:app_name\"/><meta content=\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering…. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" data-rh=\"true\" name=\"description\"/><meta content=\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" data-rh=\"true\" property=\"og:description\"/><meta content=\"https://medium.com/@thedatabeast/top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\" data-rh=\"true\" property=\"og:url\"/><meta content=\"https://medium.com/@thedatabeast/top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\" data-rh=\"true\" property=\"al:web:url\"/><meta content=\"https://miro.medium.com/v2/resize:fit:1024/1*DJCRL6_IaSlWcebV_rDzjw.png\" data-rh=\"true\" property=\"og:image\"/><meta content=\"https://medium.com/@thedatabeast\" data-rh=\"true\" property=\"article:author\"/><meta content=\"The Data Beast\" data-rh=\"true\" name=\"author\"/><meta content=\"index,follow,max-image-preview:large\" data-rh=\"true\" name=\"robots\"/><meta content=\"unsafe-url\" data-rh=\"true\" name=\"referrer\"/><meta content=\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering…\" data-rh=\"true\" property=\"twitter:title\"/><meta content=\"@Medium\" data-rh=\"true\" name=\"twitter:site\"/><meta content=\"medium://p/7abfcb69da7f\" data-rh=\"true\" name=\"twitter:app:url:iphone\"/><meta content=\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" data-rh=\"true\" property=\"twitter:description\"/><meta content=\"https://miro.medium.com/v2/resize:fit:1024/1*DJCRL6_IaSlWcebV_rDzjw.png\" data-rh=\"true\" name=\"twitter:image:src\"/><meta content=\"summary_large_image\" data-rh=\"true\" name=\"twitter:card\"/><meta content=\"@the_data_beast\" data-rh=\"true\" name=\"twitter:creator\"/><meta content=\"Reading time\" data-rh=\"true\" name=\"twitter:label1\"/><meta content=\"2 min read\" data-rh=\"true\" name=\"twitter:data1\"/><meta content=\"2\" data-rh=\"true\" name=\"twitter:tile:template:testing\"/><meta content=\"https://miro.medium.com/v2/resize:fit:1024/1*DJCRL6_IaSlWcebV_rDzjw.png\" data-rh=\"true\" name=\"twitter:tile:image\"/><meta content=\"Person\" data-rh=\"true\" name=\"twitter:tile:info1:icon\"/><meta content=\"The Data Beast\" data-rh=\"true\" name=\"twitter:tile:info1:text\"/><meta content=\"Calendar\" data-rh=\"true\" name=\"twitter:tile:info2:icon\"/><meta content=\"Nov 13, 2023\" data-rh=\"true\" name=\"twitter:tile:info2:text\"/><meta content=\"Read on Medium\" data-rh=\"true\" name=\"twitter:cta\"/><link data-rh=\"true\" href=\"https://miro.medium.com/v2/1*m-R_BkNf1Qjr1YbyOIJY2w.png\" rel=\"icon\"/><link data-rh=\"true\" href=\"/osd.xml\" rel=\"search\" title=\"Medium\" type=\"application/opensearchdescription+xml\"/><link data-rh=\"true\" href=\"https://miro.medium.com/v2/resize:fill:152:152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png\" rel=\"apple-touch-icon\" sizes=\"152x152\"/><link data-rh=\"true\" href=\"https://miro.medium.com/v2/resize:fill:120:120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png\" rel=\"apple-touch-icon\" sizes=\"120x120\"/><link data-rh=\"true\" href=\"https://miro.medium.com/v2/resize:fill:76:76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png\" rel=\"apple-touch-icon\" sizes=\"76x76\"/><link data-rh=\"true\" href=\"https://miro.medium.com/v2/resize:fill:60:60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png\" rel=\"apple-touch-icon\" sizes=\"60x60\"/><link color=\"#171717\" data-rh=\"true\" href=\"https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg\" rel=\"mask-icon\"/><link crossorigin=\"\" data-rh=\"true\" href=\"https://glyph.medium.com\" rel=\"preconnect\"/><link as=\"style\" data-rh=\"true\" href=\"https://glyph.medium.com/css/unbound.css\" id=\"glyph_preload_link\" rel=\"preload\" type=\"text/css\"/><link data-rh=\"true\" href=\"https://glyph.medium.com/css/unbound.css\" id=\"glyph_link\" rel=\"stylesheet\" type=\"text/css\"/><link data-rh=\"true\" href=\"https://medium.com/@thedatabeast\" rel=\"author\"/><link data-rh=\"true\" href=\"https://medium.com/@thedatabeast/top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\" rel=\"canonical\"/><link data-rh=\"true\" href=\"android-app://com.medium.reader/https/medium.com/p/7abfcb69da7f\" rel=\"alternate\"/><script data-rh=\"true\" type=\"application/ld+json\">{\"@context\":\"http:\\u002F\\u002Fschema.org\",\"@type\":\"NewsArticle\",\"image\":[\"https:\\u002F\\u002Fmiro.medium.com\\u002Fv2\\u002Fresize:fit:1200\\u002F1*DJCRL6_IaSlWcebV_rDzjw.png\"],\"url\":\"https:\\u002F\\u002Fmedium.com\\u002F@thedatabeast\\u002Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\",\"dateCreated\":\"2023-11-13T09:09:19.152Z\",\"datePublished\":\"2023-11-13T09:09:19.152Z\",\"dateModified\":\"2023-11-14T05:31:34.681Z\",\"headline\":\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications\",\"name\":\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications\",\"description\":\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering…. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\",\"identifier\":\"7abfcb69da7f\",\"author\":{\"@type\":\"Person\",\"name\":\"The Data Beast\",\"url\":\"https:\\u002F\\u002Fmedium.com\\u002F@thedatabeast\"},\"creator\":[\"The Data Beast\"],\"publisher\":{\"@type\":\"Organization\",\"name\":\"Medium\",\"url\":\"https:\\u002F\\u002Fmedium.com\\u002F\",\"logo\":{\"@type\":\"ImageObject\",\"width\":308,\"height\":60,\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fv2\\u002Fresize:fit:616\\u002F1*OMF3fSqH8t4xBJ9-6oZDZw.png\"}},\"mainEntityOfPage\":\"https:\\u002F\\u002Fmedium.com\\u002F@thedatabeast\\u002Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\",\"isAccessibleForFree\":\"False\",\"hasPart\":{\"@type\":\"WebPageElement\",\"isAccessibleForFree\":\"False\",\"cssSelector\":\".meteredContent\"}}</script><style data-fela-rehydration=\"447\" data-fela-type=\"STATIC\" type=\"text/css\">html{box-sizing:border-box;-webkit-text-size-adjust:100%}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}#speechify-root{font-family:Sohne, sans-serif}div[data-popper-reference-hidden=\"true\"]{visibility:hidden;pointer-events:none}\n/*XCode style (c) Angel Garcia <angelgarcia.mail@gmail.com>*/.hljs {background: #fff;color: black;\n}/* Gray DOCTYPE selectors like WebKit */\n.xml .hljs-meta {color: #c0c0c0;\n}.hljs-comment,\n.hljs-quote {color: #007400;\n}.hljs-tag,\n.hljs-attribute,\n.hljs-keyword,\n.hljs-selector-tag,\n.hljs-literal,\n.hljs-name {color: #aa0d91;\n}.hljs-variable,\n.hljs-template-variable {color: #3F6E74;\n}.hljs-code,\n.hljs-string,\n.hljs-meta .hljs-string {color: #c41a16;\n}.hljs-regexp,\n.hljs-link {color: #0E0EFF;\n}.hljs-title,\n.hljs-symbol,\n.hljs-bullet,\n.hljs-number {color: #1c00cf;\n}.hljs-section,\n.hljs-meta {color: #643820;\n}.hljs-title.class_,\n.hljs-class .hljs-title,\n.hljs-type,\n.hljs-built_in,\n.hljs-params {color: #5c2699;\n}.hljs-attr {color: #836C28;\n}.hljs-subst {color: #000;\n}.hljs-formula {background-color: #eee;font-style: italic;\n}.hljs-addition {background-color: #baeeba;\n}.hljs-deletion {background-color: #ffc8bd;\n}.hljs-selector-id,\n.hljs-selector-class {color: #9b703f;\n}.hljs-doctag,\n.hljs-strong {font-weight: bold;\n}.hljs-emphasis {font-style: italic;\n}\n</style><style data-fela-rehydration=\"447\" data-fela-type=\"KEYFRAME\" type=\"text/css\">@-webkit-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" type=\"text/css\">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Oxygen, Ubuntu, Cantarell, \"Open Sans\", \"Helvetica Neue\", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{display:block}.m{position:sticky}.n{top:0}.o{z-index:500}.p{padding:0 24px}.q{align-items:center}.r{border-bottom:solid 1px #F2F2F2}.y{height:41px}.z{line-height:20px}.ab{display:flex}.ac{height:57px}.ae{flex:1 0 auto}.af{color:inherit}.ag{fill:inherit}.ah{font-size:inherit}.ai{border:inherit}.aj{font-family:inherit}.ak{letter-spacing:inherit}.al{font-weight:inherit}.am{padding:0}.an{margin:0}.ao{cursor:pointer}.ap:disabled{cursor:not-allowed}.aq:disabled{color:#6B6B6B}.ar:disabled{fill:#6B6B6B}.au{fill:rgba(0, 0, 0, 1)}.av{height:22px}.aw{margin-left:16px}.ax{border:none}.ay{border-radius:20px}.az{width:240px}.ba{background:#F9F9F9}.bb path{fill:#6B6B6B}.bd{outline:none}.be{font-family:sohne, \"Helvetica Neue\", Helvetica, Arial, sans-serif}.bf{font-size:14px}.bg{width:100%}.bh{padding:10px 20px 10px 0}.bi{background-color:transparent}.bj{color:#242424}.bk::placeholder{color:#6B6B6B}.bl{display:inline-block}.bm{margin-left:12px}.bn{margin-right:12px}.bo{border-radius:4px}.bp{margin-left:24px}.bq{height:24px}.bw{background-color:#F9F9F9}.bx{border-radius:50%}.by{height:32px}.bz{width:32px}.ca{justify-content:center}.cg{max-width:680px}.ch{min-width:0}.ci{animation:k1 1.2s ease-in-out infinite}.cj{height:100vh}.ck{margin-bottom:16px}.cl{margin-top:48px}.cm{align-items:flex-start}.cn{flex-direction:column}.co{justify-content:space-between}.cp{margin-bottom:24px}.cv{width:80%}.cw{background-color:#F2F2F2}.dc{height:44px}.dd{width:44px}.de{margin:auto 0}.df{margin-bottom:4px}.dg{height:16px}.dh{width:120px}.di{width:80px}.do{margin-bottom:8px}.dp{width:96%}.dq{width:98%}.dr{width:81%}.ds{margin-left:8px}.dt{color:#6B6B6B}.du{font-size:13px}.dv{height:100%}.eo{color:#FFFFFF}.ep{fill:#FFFFFF}.eq{background:#1A8917}.er{border-color:#1A8917}.ev:disabled{cursor:inherit !important}.ew:disabled{opacity:0.3}.ex:disabled:hover{background:#1A8917}.ey:disabled:hover{border-color:#1A8917}.ez{border-radius:99em}.fa{border-width:1px}.fb{border-style:solid}.fc{box-sizing:border-box}.fd{text-decoration:none}.fe{text-align:center}.fh{margin-right:32px}.fi{position:relative}.fj{fill:#6B6B6B}.fm{background:transparent}.fn svg{margin-left:4px}.fo svg{fill:#6B6B6B}.fq{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.05)}.fr{position:absolute}.fy{margin:0 24px}.gc{background:rgba(255, 255, 255, 1)}.gd{border:1px solid #F2F2F2}.ge{box-shadow:0 1px 4px #F2F2F2}.gf{max-height:100vh}.gg{overflow-y:auto}.gh{left:0}.gi{top:calc(100vh + 100px)}.gj{bottom:calc(100vh + 100px)}.gk{width:10px}.gl{pointer-events:none}.gr{margin-right:4px}.gs{margin-top:2px}.gt{box-sizing:content-box}.gu{word-break:break-word}.gv{word-wrap:break-word}.gw:after{display:block}.gx:after{content:\"\"}.gy:after{clear:both}.gz{line-height:1.23}.ha{letter-spacing:0}.hb{font-style:normal}.hc{font-weight:700}.hx{margin-top:0px}.hy{@media all and (max-width: 551.98px):8px}.hz{@media all and (min-width: 552px) and (max-width: 727.98px):8px}.ia{@media all and (min-width: 728px) and (max-width: 903.98px):16px}.ib{@media all and (min-width: 904px) and (max-width: 1079.98px):16px}.ic{@media all and (min-width: 1080px):16px}.ii{align-items:baseline}.ij{width:48px}.ik{height:48px}.il{border:2px solid rgba(255, 255, 255, 1)}.im{z-index:0}.in{box-shadow:none}.io{border:1px solid rgba(0, 0, 0, 0.05)}.ip{margin-bottom:2px}.iq{flex-wrap:nowrap}.ir{font-size:16px}.is{line-height:24px}.iu{margin:0 8px}.iv{display:inline}.iw{color:#1A8917}.ix{fill:#1A8917}.ja{flex:0 0 auto}.jd{flex-wrap:wrap}.je{padding-left:8px}.jf{padding-right:8px}.kg> *{flex-shrink:0}.kh{overflow-x:scroll}.ki::-webkit-scrollbar{display:none}.kj{scrollbar-width:none}.kk{-ms-overflow-style:none}.kl{width:74px}.km{flex-direction:row}.kp{-webkit-user-select:none}.kq{border:0}.kr{fill:rgba(117, 117, 117, 1)}.ku{outline:0}.kv{user-select:none}.kw> svg{pointer-events:none}.lf{cursor:progress}.lg{margin-left:4px}.lh{opacity:1}.li{padding:4px 0}.ll{width:16px}.ln{display:inline-flex}.lt{max-width:100%}.lu{padding:8px 2px}.lv svg{color:#6B6B6B}.mm{margin-left:auto}.mn{margin-right:auto}.mo{max-width:1024px}.mp{clear:both}.mr{cursor:zoom-in}.ms{z-index:auto}.mu{height:auto}.mv{line-height:1.58}.mw{letter-spacing:-0.004em}.mx{font-family:source-serif-pro, Georgia, Cambria, \"Times New Roman\", Times, serif}.ns{margin-bottom:-0.46em}.nt{list-style-type:decimal}.nu{margin-left:30px}.nv{padding-left:0px}.nw{list-style-type:disc}.nx{text-decoration:underline}.od{border-top:none}.oj{height:52px}.ok{max-height:52px}.ol{position:static}.om{z-index:1}.oo{max-width:155px}.ou{margin-right:20px}.pa{align-items:flex-end}.pb{width:76px}.pc{height:76px}.pd{border:2px solid #F9F9F9}.pe{height:72px}.pf{width:72px}.pg{padding:8px 16px}.ph{width:auto}.pi{stroke:#F2F2F2}.pj{height:36px}.pk{width:36px}.pl{color:#F2F2F2}.pm{fill:#F2F2F2}.pn{background:#F2F2F2}.po{border-color:#F2F2F2}.pu{font-weight:500}.pv{font-size:24px}.pw{line-height:30px}.px{letter-spacing:-0.016em}.py{margin-top:8px}.pz{margin-top:16px}.qa{height:0px}.qb{border-bottom:solid 1px #E5E5E5}.qc{margin-top:72px}.qd{padding:24px 0}.qe{margin-bottom:0px}.qf{margin-right:16px}.as:hover:not(:disabled){color:rgba(25, 25, 25, 1)}.at:hover:not(:disabled){fill:rgba(25, 25, 25, 1)}.es:hover{background:#156D12}.et:hover{border-color:#156D12}.eu:hover{cursor:pointer}.fk:hover{color:#242424}.fl:hover{fill:#242424}.fp:hover svg{fill:#242424}.fs:hover{background-color:rgba(0, 0, 0, 0.1)}.it:hover{text-decoration:underline}.iy:hover:not(:disabled){color:#156D12}.iz:hover:not(:disabled){fill:#156D12}.kt:hover{fill:rgba(8, 8, 8, 1)}.lj:hover{fill:#000000}.lk:hover p{color:#000000}.lm:hover{color:#000000}.lw:hover svg{color:#000000}.pp:hover{background:#F2F2F2}.pq:hover{border-color:#F2F2F2}.pr:hover{cursor:wait}.ps:hover{color:#F2F2F2}.pt:hover{fill:#F2F2F2}.bc:focus-within path{fill:#242424}.ks:focus{fill:rgba(8, 8, 8, 1)}.lx:focus svg{color:#000000}.mt:focus{transform:scale(1.01)}.kx:active{border-style:none}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"all and (min-width: 1080px)\" type=\"text/css\">.d{display:none}.bv{width:64px}.cf{margin:0 64px}.cu{height:48px}.db{margin-bottom:52px}.dn{margin-bottom:48px}.ee{font-size:14px}.ef{line-height:20px}.el{font-size:13px}.em{padding:5px 12px}.fg{display:flex}.fx{margin-bottom:68px}.gb{max-width:680px}.gq{margin-top:40px}.ht{font-size:42px}.hu{margin-bottom:32px}.hv{line-height:52px}.hw{letter-spacing:-0.011em}.ih{align-items:center}.js{border-top:solid 1px #F2F2F2}.jt{border-bottom:solid 1px #F2F2F2}.ju{margin:32px 0 0}.jv{padding:3px 8px}.ke> *{margin-right:24px}.kf> :last-child{margin-right:0}.le{margin-top:0px}.ls{margin:0}.no{font-size:20px}.np{margin-top:2.14em}.nq{line-height:32px}.nr{letter-spacing:-0.003em}.oc{margin-top:1.14em}.oi{margin-bottom:88px}.ot{display:inline-block}.oz{padding-top:72px}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"all and (max-width: 1079.98px)\" type=\"text/css\">.e{display:none}.ld{margin-top:0px}.os{display:inline-block}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"all and (max-width: 903.98px)\" type=\"text/css\">.f{display:none}.lc{margin-top:0px}.or{display:inline-block}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"all and (max-width: 727.98px)\" type=\"text/css\">.g{display:none}.la{margin-top:0px}.lb{margin-right:0px}.oq{display:inline-block}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"all and (max-width: 551.98px)\" type=\"text/css\">.h{display:none}.s{display:flex}.t{justify-content:space-between}.br{width:24px}.cb{margin:0 24px}.cq{height:40px}.cx{margin-bottom:44px}.dj{margin-bottom:32px}.dw{font-size:13px}.dx{line-height:20px}.eg{padding:0px 8px 1px}.ft{margin-bottom:4px}.gm{margin-top:32px}.hd{font-size:32px}.he{margin-bottom:24px}.hf{line-height:38px}.hg{letter-spacing:-0.014em}.id{align-items:flex-start}.jb{flex-direction:column}.jg{margin:24px -24px 0}.jh{padding:0}.jw> *{margin-right:8px}.jx> :last-child{margin-right:24px}.kn{margin-left:0px}.ky{margin-top:0px}.kz{margin-right:0px}.lo{margin:0}.ly{border:1px solid #F2F2F2}.lz{border-radius:99em}.ma{padding:0px 16px 0px 12px}.mb{height:38px}.mc{align-items:center}.me svg{margin-right:8px}.my{font-size:18px}.mz{margin-top:1.56em}.na{line-height:28px}.nb{letter-spacing:-0.003em}.ny{margin-top:1.34em}.oe{margin-bottom:80px}.op{display:inline-block}.ov{padding-top:48px}.md:hover{border-color:#E5E5E5}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"all and (min-width: 904px) and (max-width: 1079.98px)\" type=\"text/css\">.i{display:none}.bu{width:64px}.ce{margin:0 64px}.ct{height:48px}.da{margin-bottom:52px}.dm{margin-bottom:48px}.ec{font-size:14px}.ed{line-height:20px}.ej{font-size:13px}.ek{padding:5px 12px}.ff{display:flex}.fw{margin-bottom:68px}.ga{max-width:680px}.gp{margin-top:40px}.hp{font-size:42px}.hq{margin-bottom:32px}.hr{line-height:52px}.hs{letter-spacing:-0.011em}.ig{align-items:center}.jo{border-top:solid 1px #F2F2F2}.jp{border-bottom:solid 1px #F2F2F2}.jq{margin:32px 0 0}.jr{padding:3px 8px}.kc> *{margin-right:24px}.kd> :last-child{margin-right:0}.lr{margin:0}.nk{font-size:20px}.nl{margin-top:2.14em}.nm{line-height:32px}.nn{letter-spacing:-0.003em}.ob{margin-top:1.14em}.oh{margin-bottom:88px}.oy{padding-top:72px}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"all and (min-width: 728px) and (max-width: 903.98px)\" type=\"text/css\">.j{display:none}.w{display:flex}.x{justify-content:space-between}.bt{width:64px}.cd{margin:0 48px}.cs{height:48px}.cz{margin-bottom:52px}.dl{margin-bottom:48px}.ea{font-size:13px}.eb{line-height:20px}.ei{padding:0px 8px 1px}.fv{margin-bottom:68px}.fz{max-width:680px}.go{margin-top:40px}.hl{font-size:42px}.hm{margin-bottom:32px}.hn{line-height:52px}.ho{letter-spacing:-0.011em}.if{align-items:center}.jk{border-top:solid 1px #F2F2F2}.jl{border-bottom:solid 1px #F2F2F2}.jm{margin:32px 0 0}.jn{padding:3px 8px}.ka> *{margin-right:24px}.kb> :last-child{margin-right:0}.lq{margin:0}.ng{font-size:20px}.nh{margin-top:2.14em}.ni{line-height:32px}.nj{letter-spacing:-0.003em}.oa{margin-top:1.14em}.og{margin-bottom:88px}.ox{padding-top:72px}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"all and (min-width: 552px) and (max-width: 727.98px)\" type=\"text/css\">.k{display:none}.u{display:flex}.v{justify-content:space-between}.bs{width:24px}.cc{margin:0 24px}.cr{height:40px}.cy{margin-bottom:44px}.dk{margin-bottom:32px}.dy{font-size:13px}.dz{line-height:20px}.eh{padding:0px 8px 1px}.fu{margin-bottom:4px}.gn{margin-top:32px}.hh{font-size:32px}.hi{margin-bottom:24px}.hj{line-height:38px}.hk{letter-spacing:-0.014em}.ie{align-items:flex-start}.jc{flex-direction:column}.ji{margin:24px 0 0}.jj{padding:0}.jy> *{margin-right:8px}.jz> :last-child{margin-right:8px}.ko{margin-left:0px}.lp{margin:0}.mf{border:1px solid #F2F2F2}.mg{border-radius:99em}.mh{padding:0px 16px 0px 12px}.mi{height:38px}.mj{align-items:center}.ml svg{margin-right:8px}.nc{font-size:18px}.nd{margin-top:1.56em}.ne{line-height:28px}.nf{letter-spacing:-0.003em}.nz{margin-top:1.34em}.of{margin-bottom:80px}.ow{padding-top:48px}.mk:hover{border-color:#E5E5E5}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"print\" type=\"text/css\">.on{display:none}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"(prefers-reduced-motion: no-preference)\" type=\"text/css\">.mq{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}</style></head><body><div id=\"root\"><div class=\"a b c\"><div class=\"d e f g h i j k\"></div><script>document.domain = document.domain;</script><div class=\"l c\"><div class=\"l m n o c\"><div class=\"p q r s t u v w x i d y z\"><a class=\"dt ag du be ak b am an ao ap aq ar as at s u w i d q dv z\" href=\"https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7abfcb69da7f&amp;%7Efeature=LoOpenInAppButton&amp;%7Echannel=ShowPostUnderUser&amp;source=---two_column_layout_nav----------------------------------\" rel=\"noopener follow\">Open in app<svg class=\"ds\" fill=\"none\" height=\"10\" viewbox=\"0 0 10 10\" width=\"10\"><path d=\"M.98 8.48a.37.37 0 1 0 .54.54l-.54-.54zm7.77-7.23h.38c0-.2-.17-.38-.38-.38v.38zM8.37 6.5a.37.37 0 1 0 .76 0h-.76zM3.5.87a.37.37 0 1 0 0 .76V.88zM1.52 9.03l7.5-7.5-.54-.54-7.5 7.5.54.54zm6.86-7.77V6.5h.74V1.25h-.74zm-4.88.38h5.25V.88H3.5v.74z\" fill=\"currentColor\"></path></svg></a><div class=\"ab q\"><p class=\"be b dw dx dy dz ea eb ec ed ee ef dt\"><span><button class=\"be b dw dx eg dy dz eh ea eb ei ej ed ek el ef em eo ep eq er es et eu ev ew ex ey ez fa fb fc bl fd fe\" data-testid=\"headerSignUpButton\">Sign up</button></span></p><div class=\"aw l\"><p class=\"be b dw dx dy dz ea eb ec ed ee ef dt\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerSignInButton\" href=\"/m/signin?operation=login&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------\" rel=\"noopener follow\">Sign in</a></span></p></div></div></div><div class=\"p q r ab ac\"><div class=\"ab q ae\"><a aria-label=\"Homepage\" class=\"af ag ah ai aj ak al am an ao ap aq ar as at ab\" data-testid=\"headerMediumLogo\" href=\"/?source=---two_column_layout_nav----------------------------------\" rel=\"noopener follow\"><svg class=\"au av\" viewbox=\"0 0 3940 610\"><path d=\"M594.79 308.2c0 163.76-131.85 296.52-294.5 296.52S5.8 472 5.8 308.2 137.65 11.69 300.29 11.69s294.5 132.75 294.5 296.51M917.86 308.2c0 154.16-65.93 279.12-147.25 279.12s-147.25-125-147.25-279.12S689.29 29.08 770.61 29.08s147.25 125 147.25 279.12M1050 308.2c0 138.12-23.19 250.08-51.79 250.08s-51.79-112-51.79-250.08 23.19-250.08 51.8-250.08S1050 170.09 1050 308.2M1862.77 37.4l.82-.18v-6.35h-167.48l-155.51 365.5-155.51-365.5h-180.48v6.35l.81.18c30.57 6.9 46.09 17.19 46.09 54.3v434.45c0 37.11-15.58 47.4-46.15 54.3l-.81.18V587H1327v-6.35l-.81-.18c-30.57-6.9-46.09-17.19-46.09-54.3V116.9L1479.87 587h11.33l205.59-483.21V536.9c-2.62 29.31-18 38.36-45.68 44.61l-.82.19v6.3h213.3v-6.3l-.82-.19c-27.71-6.25-43.46-15.3-46.08-44.61l-.14-445.2h.14c0-37.11 15.52-47.4 46.08-54.3m97.43 287.8c3.49-78.06 31.52-134.4 78.56-135.37 14.51.24 26.68 5 36.14 14.16 20.1 19.51 29.55 60.28 28.09 121.21zm-2.11 22h250v-1.05c-.71-59.69-18-106.12-51.34-138-28.82-27.55-71.49-42.71-116.31-42.71h-1c-23.26 0-51.79 5.64-72.09 15.86-23.11 10.7-43.49 26.7-60.45 47.7-27.3 33.83-43.84 79.55-47.86 130.93-.13 1.54-.24 3.08-.35 4.62s-.18 2.92-.25 4.39a332.64 332.64 0 0 0-.36 21.69C1860.79 507 1923.65 600 2035.3 600c98 0 155.07-71.64 169.3-167.8l-7.19-2.53c-25 51.68-69.9 83-121 79.18-69.76-5.22-123.2-75.95-118.35-161.63m532.69 157.68c-8.2 19.45-25.31 30.15-48.24 30.15s-43.89-15.74-58.78-44.34c-16-30.7-24.42-74.1-24.42-125.51 0-107 33.28-176.21 84.79-176.21 21.57 0 38.55 10.7 46.65 29.37zm165.84 76.28c-30.57-7.23-46.09-18-46.09-57V5.28L2424.77 60v6.7l1.14-.09c25.62-2.07 43 1.47 53.09 10.79 7.9 7.3 11.75 18.5 11.75 34.26v71.14c-18.31-11.69-40.09-17.38-66.52-17.38-53.6 0-102.59 22.57-137.92 63.56-36.83 42.72-56.3 101.1-56.3 168.81C2230 518.72 2289.53 600 2378.13 600c51.83 0 93.53-28.4 112.62-76.3V588h166.65v-6.66zm159.29-505.33c0-37.76-28.47-66.24-66.24-66.24-37.59 0-67 29.1-67 66.24s29.44 66.24 67 66.24c37.77 0 66.24-28.48 66.24-66.24m43.84 505.33c-30.57-7.23-46.09-18-46.09-57h-.13V166.65l-166.66 47.85v6.5l1 .09c36.06 3.21 45.93 15.63 45.93 57.77V588h166.8v-6.66zm427.05 0c-30.57-7.23-46.09-18-46.09-57V166.65L3082 212.92v6.52l.94.1c29.48 3.1 38 16.23 38 58.56v226c-9.83 19.45-28.27 31-50.61 31.78-36.23 0-56.18-24.47-56.18-68.9V166.66l-166.66 47.85V221l1 .09c36.06 3.2 45.94 15.62 45.94 57.77v191.27a214.48 214.48 0 0 0 3.47 39.82l3 13.05c14.11 50.56 51.08 77 109 77 49.06 0 92.06-30.37 111-77.89v66h166.66v-6.66zM3934.2 588v-6.67l-.81-.19c-33.17-7.65-46.09-22.07-46.09-51.43v-243.2c0-75.83-42.59-121.09-113.93-121.09-52 0-95.85 30.05-112.73 76.86-13.41-49.6-52-76.86-109.06-76.86-50.12 0-89.4 26.45-106.25 71.13v-69.87l-166.66 45.89v6.54l1 .09c35.63 3.16 45.93 15.94 45.93 57V588h155.5v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66V255.72c7-16.35 21.11-35.72 49-35.72 34.64 0 52.2 24 52.2 71.28V588h155.54v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66v-248a160.45 160.45 0 0 0-2.2-27.68c7.42-17.77 22.34-38.8 51.37-38.8 35.13 0 52.2 23.31 52.2 71.28V588z\"></path></svg></a><div class=\"aw h\"><div class=\"ab ax ay az ba q bb bc\"><div aria-describedby=\"searchResults\" aria-hidden=\"false\" aria-labelledby=\"searchResults\" class=\"bl\"></div><div class=\"bm bn ab\"><svg fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z\" fill=\"currentColor\" fill-rule=\"evenodd\"></path></svg></div><input aria-controls=\"searchResults\" aria-expanded=\"false\" aria-label=\"search\" class=\"ax bd be bf z bg bh bi bj bk\" data-testid=\"headerSearchInput\" placeholder=\"Search\" role=\"combobox\" tabindex=\"0\" value=\"\"/></div></div></div><div class=\"h k w ff fg\"><div class=\"fh ab\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerWriteButton\" href=\"/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fnew-story&amp;source=---two_column_layout_nav-----------------------new_post_topnav-----------\" rel=\"noopener follow\"><div class=\"be b bf z dt fi fj ab q fk fl\"><svg aria-label=\"Write\" fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M14 4a.5.5 0 0 0 0-1v1zm7 6a.5.5 0 0 0-1 0h1zm-7-7H4v1h10V3zM3 4v16h1V4H3zm1 17h16v-1H4v1zm17-1V10h-1v10h1zm-1 1a1 1 0 0 0 1-1h-1v1zM3 20a1 1 0 0 0 1 1v-1H3zM4 3a1 1 0 0 0-1 1h1V3z\" fill=\"currentColor\"></path><path d=\"M17.5 4.5l-8.46 8.46a.25.25 0 0 0-.06.1l-.82 2.47c-.07.2.12.38.31.31l2.47-.82a.25.25 0 0 0 .1-.06L19.5 6.5m-2-2l2.32-2.32c.1-.1.26-.1.36 0l1.64 1.64c.1.1.1.26 0 .36L19.5 6.5m-2-2l2 2\" stroke=\"currentColor\"></path></svg><div class=\"ds l\">Write</div></div></a></span></div></div><div class=\"k j i d\"><div class=\"fh ab\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerSearchButton\" href=\"/search?source=---two_column_layout_nav----------------------------------\" rel=\"noopener follow\"><div class=\"be b bf z dt fi fj ab q fk fl\"><svg aria-label=\"Search\" fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z\" fill=\"currentColor\" fill-rule=\"evenodd\"></path></svg></div></a></div></div><div class=\"fh h k j\"><div class=\"ab q\"><p class=\"be b dw dx dy dz ea eb ec ed ee ef dt\"><span><button class=\"be b dw dx eg dy dz eh ea eb ei ej ed ek el ef em eo ep eq er es et eu ev ew ex ey ez fa fb fc bl fd fe\" data-testid=\"headerSignUpButton\">Sign up</button></span></p><div class=\"aw l\"><p class=\"be b dw dx dy dz ea eb ec ed ee ef dt\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerSignInButton\" href=\"/m/signin?operation=login&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------\" rel=\"noopener follow\">Sign in</a></span></p></div></div></div><div aria-hidden=\"false\" class=\"l\"><button aria-label=\"user options menu\" class=\"ax fm am ab q ao fn fo fp\" data-testid=\"headerUserIcon\"><div class=\"l fi\"><img alt=\"\" class=\"l fc bx by bz cw\" height=\"32\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png\" width=\"32\"/><div class=\"fq bx l by bz fr n ax fs\"></div></div></button></div></div></div><div class=\"l\"><div class=\"ft fu fv fw fx l\"><div class=\"ab ca\"><div class=\"ch bg fy fz ga gb\"></div></div><article class=\"meteredContent\"><div class=\"l\"><div class=\"l\"><span class=\"l\"></span><section><div><div class=\"fr gh gi gj gk gl\"></div><div><div class=\"speechify-ignore l\"><div class=\"gm gn go gp gq l\"></div><div class=\"ab ca\"><div class=\"ch bg fy fz ga gb\"><div class=\"ck l\"><div class=\"gt ab\"><div aria-hidden=\"false\" class=\"bl\"><button aria-label=\"Member-only story\" class=\"l ax ao am\"><div class=\"h k j i d\"><div><div aria-hidden=\"false\" class=\"bl\"><svg fill=\"none\" height=\"16\" viewbox=\"0 0 64 64\" width=\"16\"><path d=\"M39.64 40.83L33.87 56.7a1.99 1.99 0 0 1-3.74 0l-5.77-15.87a2.02 2.02 0 0 0-1.2-1.2L7.3 33.88a1.99 1.99 0 0 1 0-3.74l15.87-5.77a2.02 2.02 0 0 0 1.2-1.2L30.12 7.3a1.99 1.99 0 0 1 3.74 0l5.77 15.87a2.02 2.02 0 0 0 1.2 1.2l15.86 5.76a1.99 1.99 0 0 1 0 3.74l-15.87 5.77a2.02 2.02 0 0 0-1.2 1.2z\" fill=\"#FFC017\"></path></svg></div></div></div><div class=\"s u w ff fg q\"><svg class=\"gr gs\" fill=\"none\" height=\"16\" viewbox=\"0 0 64 64\" width=\"16\"><path d=\"M39.64 40.83L33.87 56.7a1.99 1.99 0 0 1-3.74 0l-5.77-15.87a2.02 2.02 0 0 0-1.2-1.2L7.3 33.88a1.99 1.99 0 0 1 0-3.74l15.87-5.77a2.02 2.02 0 0 0 1.2-1.2L30.12 7.3a1.99 1.99 0 0 1 3.74 0l5.77 15.87a2.02 2.02 0 0 0 1.2 1.2l15.86 5.76a1.99 1.99 0 0 1 0 3.74l-15.87 5.77a2.02 2.02 0 0 0-1.2 1.2z\" fill=\"#FFC017\"></path></svg><p class=\"be b bf z dt\">Member-only story</p></div></button></div></div></div></div></div></div></div><div class=\"gu gv gw gx gy\"><div class=\"ab ca\"><div class=\"ch bg fy fz ga gb\"><div><h1 class=\"pw-post-title gz ha hb be hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx bj\" data-testid=\"storyTitle\" id=\"1be4\">Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications</h1><div class=\"hy hz ia ib ic\"><div class=\"speechify-ignore ab co\"><div class=\"speechify-ignore bg l\"><div class=\"id ie if ig ih ab\"><div><div class=\"ab ii\"><a href=\"/@thedatabeast?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><div><div aria-hidden=\"false\" class=\"bl\"><div class=\"l ij ik bx il im\"><div class=\"l fi\"><img alt=\"The Data Beast\" class=\"l fc bx dc dd cw\" data-testid=\"authorPhoto\" height=\"44\" loading=\"lazy\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*A7eSHxmpHV-LOcc4-DxVnw.png\" width=\"44\"/><div class=\"in bx l dc dd fr n io fs\"></div></div></div></div></div></a></div></div><div class=\"bm bg l\"><div class=\"ab\"><div style=\"flex:1\"><span class=\"be b bf z bj\"><div class=\"ip ab q\"><div class=\"ab q iq\"><div class=\"ab q\"><div><div aria-hidden=\"false\" class=\"bl\"><p class=\"be b ir is bj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar it\" data-testid=\"authorName\" href=\"/@thedatabeast?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\">The Data Beast</a></p></div></div></div><span aria-hidden=\"true\" class=\"iu iv\"><span class=\"be b bf z dt\">·</span></span><p class=\"be b ir is dt\"><span><a class=\"iw ix ah ai aj ak al am an ao ap aq ar ew iy iz\" href=\"/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6bbc6865a96&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;user=The+Data+Beast&amp;userId=f6bbc6865a96&amp;source=post_page-f6bbc6865a96----7abfcb69da7f---------------------post_header-----------\" rel=\"noopener follow\">Follow</a></span></p></div></div></span></div></div><div class=\"l ja\"><span class=\"be b bf z dt\"><div class=\"ab cm jb jc jd\"><span class=\"be b bf z dt\"><div class=\"ab ae\"><span data-testid=\"storyReadTime\">2 min read</span><div aria-hidden=\"true\" class=\"je jf l\"><span aria-hidden=\"true\" class=\"l\"><span class=\"be b bf z dt\">·</span></span></div><span data-testid=\"storyPublishDate\">Nov 13, 2023</span></div></span></div></span></div></div></div><div class=\"ab co jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv\"><div class=\"h k w ff fg q\"><div class=\"kl l\"><div class=\"ab q km\"><div class=\"pw-multi-vote-icon fi gr kn ko kp\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerClapButton\" href=\"/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F7abfcb69da7f&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;user=The+Data+Beast&amp;userId=f6bbc6865a96&amp;source=-----7abfcb69da7f---------------------clap_footer-----------\" rel=\"noopener follow\"><div><div aria-hidden=\"false\" class=\"bl\"><div class=\"kq ao kr ks kt ku am kv kw kx kp\"><svg aria-label=\"clap\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\" fill-rule=\"evenodd\"></path></svg></div></div></div></a></span></div><div class=\"pw-multi-vote-count l ky kz la lb lc ld le\"><p class=\"be b du z dt\"><span class=\"lf\">--</span></p></div></div></div><div><div aria-hidden=\"false\" class=\"bl\"><button aria-label=\"responses\" class=\"ao kq lh li ab q fj lj lk\"><svg class=\"hx\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg><p class=\"be b du z dt\"><span class=\"pw-responses-count lg hx\">1</span></p></button></div></div></div><div class=\"ab q jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk\"><div class=\"ll k j i d\"></div><div class=\"h k\"><div><div aria-hidden=\"false\" class=\"bl\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerBookmarkButton\" href=\"/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7abfcb69da7f&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;source=-----7abfcb69da7f---------------------bookmark_footer-----------\" rel=\"noopener follow\"><svg aria-label=\"Add to list bookmark button\" class=\"dt lm\" fill=\"none\" height=\"25\" viewbox=\"0 0 25 25\" width=\"25\"><path d=\"M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z\" fill=\"currentColor\"></path></svg></a></span></div></div></div><div class=\"fc ln cm\"><div class=\"l ae\"><div class=\"ab ca\"><div class=\"lo lp lq lr ls lt ch bg\"><div class=\"ab\"></div></div></div></div></div><div aria-describedby=\"postFooterSocialMenu\" aria-hidden=\"false\" aria-labelledby=\"postFooterSocialMenu\" class=\"bl\"><div><div aria-hidden=\"false\" class=\"bl\"><button aria-controls=\"postFooterSocialMenu\" aria-expanded=\"false\" aria-label=\"Share Post\" class=\"af fj ah ai aj ak al lu an ao ap ew lv lw lk lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml\" data-testid=\"headerSocialShareButton\"><svg fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z\" fill=\"currentColor\" fill-rule=\"evenodd\"></path></svg><div class=\"j i d\"><p class=\"be b bf z dt\">Share</p></div></button></div></div></div></div></div></div></div></div></div><figure class=\"gm gn go gp gq mp mm mn paragraph-image\"><div class=\"mq mr fi ms bg mt\" role=\"button\" tabindex=\"0\"><div class=\"mm mn mo\"><picture><source sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*DJCRL6_IaSlWcebV_rDzjw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*DJCRL6_IaSlWcebV_rDzjw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*DJCRL6_IaSlWcebV_rDzjw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*DJCRL6_IaSlWcebV_rDzjw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*DJCRL6_IaSlWcebV_rDzjw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*DJCRL6_IaSlWcebV_rDzjw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DJCRL6_IaSlWcebV_rDzjw.png 1400w\" type=\"image/webp\"/><source data-testid=\"og\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*DJCRL6_IaSlWcebV_rDzjw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*DJCRL6_IaSlWcebV_rDzjw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*DJCRL6_IaSlWcebV_rDzjw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*DJCRL6_IaSlWcebV_rDzjw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*DJCRL6_IaSlWcebV_rDzjw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*DJCRL6_IaSlWcebV_rDzjw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*DJCRL6_IaSlWcebV_rDzjw.png 1400w\"/><img alt=\"\" class=\"bg lt mu c\" height=\"700\" loading=\"eager\" role=\"presentation\" width=\"700\"/></picture></div></div></figure><ol class=\"\"><li class=\"mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv bj\" id=\"62b7\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</li></ol><ul class=\"\"><li class=\"mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nw nu nv bj\" id=\"b27d\">Link: <a class=\"af nx\" href=\"https://arxiv.org/abs/1810.04805\" rel=\"noopener ugc nofollow\" target=\"_blank\">BERT Paper</a></li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"6118\">Details: Released by Google AI Language team, BERT introduced a deep bidirectional architecture, which enhanced transfer learning demonstrated by unsupervised pre-training.</li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"d23e\">Practical Implementation: Used for state-of-the-art (SOTA) models in language inference and simple question-answer tasks​​.</li></ul><p class=\"pw-post-body-paragraph mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns gu bj\" id=\"2349\">2. BlenderBot 3: A deployed conversational agent that continually learns to responsibly engage</p><ul class=\"\"><li class=\"mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nw nu nv bj\" id=\"685c\">Link: <a class=\"af nx\" href=\"https://arxiv.org/abs/2208.03188\" rel=\"noopener ugc nofollow\" target=\"_blank\">BlenderBot 3 Paper</a></li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"d33f\">Details: From Meta AI, BlenderBot 3, with its 175 billion parameters, can scour the internet, setting it apart from other conversational bots.</li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"7127\">Practical Implementation: Continually learns from deployment data, enhancing engagement and response quality​​.</li></ul><p class=\"pw-post-body-paragraph mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns gu bj\" id=\"3dc8\">3. Improving alignment of dialogue agents via targeted human judgements</p><ul class=\"\"><li class=\"mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nw nu nv bj\" id=\"0d47\">Link: <a class=\"af nx\" href=\"https://arxiv.org/abs/2209.14375\" rel=\"noopener ugc nofollow\" target=\"_blank\">Sparrow Paper</a></li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"3ef2\">Details: DeepMind’s Sparrow employs the RLHF method for training, focusing on using human feedback for generative models.</li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"3e22\">Practical Implementation: Helps in building complex goals in chatbots by integrating human feedback effectively​​.</li></ul><p class=\"pw-post-body-paragraph mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns gu bj\" id=\"2012\">4. Improving Language Understanding by Generative Pre-Training</p><ul class=\"\"><li class=\"mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nw nu nv bj\" id=\"51bc\">Link: <a class=\"af nx\" href=\"https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035\" rel=\"noopener ugc nofollow\" target=\"_blank\">GPT Paper</a></li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"1e7c\">Details: OpenAI’s paper on GPT, demonstrating the use of a decoder-style LLM for generative modeling.</li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"979b\">Practical Implementation: Pioneered NLP tasks by generative pre-training of a language model​​.</li></ul><p class=\"pw-post-body-paragraph mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns gu bj\" id=\"5157\">5. Scaling Laws for Neural Language Models</p><ul class=\"\"><li class=\"mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nw nu nv bj\" id=\"0dfe\">Link: <a class=\"af nx\" href=\"https://arxiv.org/abs/2001.08361\" rel=\"noopener ugc nofollow\" target=\"_blank\">Scaling Laws Paper</a></li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"52b8\">Details: OpenAI’s theoretical investigation into the relationship between the size of a language model and its performance.</li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"292b\">Practical Implementation: Provides empirical evidence for the scaling laws that govern model performance​​.</li></ul><p class=\"pw-post-body-paragraph mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns gu bj\" id=\"2f6f\">6. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation…</p></div></div></div></div></section></div></div></article><div class=\"ab ca\"><div class=\"ch bg fy fz ga gb\"></div></div></div><div class=\"l\"></div><footer class=\"od oe of og oh oi oj ok gt ab q ol om c\"><div class=\"l ae\"><div class=\"ab ca\"><div class=\"ch bg fy fz ga gb\"><div class=\"ab co on\"><div class=\"ab q km\"><div class=\"oo l\"><span class=\"l op oq or e d\"><div class=\"ab q km\"><div class=\"pw-multi-vote-icon fi gr kn ko kp\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"footerClapButton\" href=\"/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F7abfcb69da7f&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;user=The+Data+Beast&amp;userId=f6bbc6865a96&amp;source=-----7abfcb69da7f---------------------clap_footer-----------\" rel=\"noopener follow\"><div><div aria-hidden=\"false\" class=\"bl\"><div class=\"kq ao kr ks kt ku am kv kw kx kp\"><svg aria-label=\"clap\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\" fill-rule=\"evenodd\"></path></svg></div></div></div></a></span></div><div class=\"pw-multi-vote-count l ky kz la lb lc ld le\"><p class=\"be b du z dt\"><span class=\"lf\">--</span></p></div></div></span><span class=\"l h g f os ot\"><div class=\"ab q km\"><div class=\"pw-multi-vote-icon fi gr kn ko kp\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"footerClapButton\" href=\"/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F7abfcb69da7f&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;user=The+Data+Beast&amp;userId=f6bbc6865a96&amp;source=-----7abfcb69da7f---------------------clap_footer-----------\" rel=\"noopener follow\"><div><div aria-hidden=\"false\" class=\"bl\"><div class=\"kq ao kr ks kt ku am kv kw kx kp\"><svg aria-label=\"clap\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\" fill-rule=\"evenodd\"></path></svg></div></div></div></a></span></div><div class=\"pw-multi-vote-count l ky kz la lb lc ld le\"><p class=\"be b du z dt\"><span class=\"lf\">--</span></p></div></div></span></div><div class=\"bp ab\"><div><div aria-hidden=\"false\" class=\"bl\"><button aria-label=\"responses\" class=\"ao kq lh li ab q fj lj lk\"><svg class=\"hx\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg><p class=\"be b bf z dt\"><span class=\"pw-responses-count lg hx\">1</span></p></button></div></div></div></div><div class=\"ab q\"><div class=\"ou l ja\"><div><div aria-hidden=\"false\" class=\"bl\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"footerBookmarkButton\" href=\"/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7abfcb69da7f&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;source=--------------------------bookmark_footer-----------\" rel=\"noopener follow\"><svg aria-label=\"Add to list bookmark button\" class=\"dt lm\" fill=\"none\" height=\"25\" viewbox=\"0 0 25 25\" width=\"25\"><path d=\"M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z\" fill=\"currentColor\"></path></svg></a></span></div></div></div><div class=\"ou l ja\"><div aria-describedby=\"postFooterSocialMenu\" aria-hidden=\"false\" aria-labelledby=\"postFooterSocialMenu\" class=\"bl\"><div><div aria-hidden=\"false\" class=\"bl\"><button aria-controls=\"postFooterSocialMenu\" aria-expanded=\"false\" aria-label=\"Share Post\" class=\"af fj ah ai aj ak al lu an ao ap ew lv lw lk lx\" data-testid=\"footerSocialShareButton\"><svg fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z\" fill=\"currentColor\" fill-rule=\"evenodd\"></path></svg></button></div></div></div></div></div></div></div></div></div></footer><div class=\"ov ow ox oy oz l bw\"><div class=\"ab ca\"><div class=\"ch bg fy fz ga gb\"><div class=\"ck ab pa co\"><div class=\"ab ii\"><a href=\"/@thedatabeast?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><div class=\"l pb pc bx pd im\"><div class=\"l fi\"><img alt=\"The Data Beast\" class=\"l fc bx pe pf cw\" height=\"72\" loading=\"lazy\" src=\"https://miro.medium.com/v2/resize:fill:144:144/1*A7eSHxmpHV-LOcc4-DxVnw.png\" width=\"72\"/><div class=\"in bx l pe pf fr n io fs\"></div></div></div></a></div><div class=\"j i d\"><div class=\"ab\"><span><button class=\"be b bf z eo pg ep eq er es et eu ev ew ex ey ez ph fa fb fc bl fd fe\">Follow</button></span><div class=\"ds l\"><div><div><div aria-hidden=\"false\" class=\"bl\"><div class=\"l\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F920e325a463a&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;newsletterV3=f6bbc6865a96&amp;newsletterV3Id=920e325a463a&amp;user=The+Data+Beast&amp;userId=f6bbc6865a96&amp;source=-----7abfcb69da7f---------------------subscribe_user-----------\" rel=\"noopener follow\"><button aria-label=\"Subscribe\" class=\"be b bf z pl am pm pn po pp pq pr ps pt ev ew ex ey ez fa fb fc bl fd fe\"><svg class=\"pi pj pk\" fill=\"none\" height=\"38\" viewbox=\"0 0 38 38\" width=\"38\"><rect height=\"6.5\" rx=\"0.25\" width=\"0.5\" x=\"26.25\" y=\"9.25\"></rect><rect height=\"6.5\" rx=\"0.25\" transform=\"rotate(90 29.75 12.25)\" width=\"0.5\" x=\"29.75\" y=\"12.25\"></rect><path d=\"M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5\"></path><path d=\"M11.5 14.5L19 20l4-3\"></path></svg></button></a></span></div></div></div></div></div></div></div></div><div class=\"ab cm co\"><div class=\"l\"><div class=\"ab q\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at ab q\" href=\"/@thedatabeast?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><h2 class=\"pw-author-name be pu pv pw px bj\"><span class=\"gu\">Written by <!-- -->The Data Beast</span></h2></a></div><div class=\"py ab\"><div class=\"l ja\"><span class=\"pw-follower-count be b bf z bj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar it\" href=\"/@thedatabeast/followers?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\">327 Followers</a></span></div></div><div class=\"pz l\"></div></div><div class=\"h k\"><div class=\"ab\"><span><button class=\"be b bf z eo pg ep eq er es et eu ev ew ex ey ez ph fa fb fc bl fd fe\">Follow</button></span><div class=\"ds l\"><div><div><div aria-hidden=\"false\" class=\"bl\"><div class=\"l\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F920e325a463a&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;newsletterV3=f6bbc6865a96&amp;newsletterV3Id=920e325a463a&amp;user=The+Data+Beast&amp;userId=f6bbc6865a96&amp;source=-----7abfcb69da7f---------------------subscribe_user-----------\" rel=\"noopener follow\"><button aria-label=\"Subscribe\" class=\"be b bf z pl am pm pn po pp pq pr ps pt ev ew ex ey ez fa fb fc bl fd fe\"><svg class=\"pi pj pk\" fill=\"none\" height=\"38\" viewbox=\"0 0 38 38\" width=\"38\"><rect height=\"6.5\" rx=\"0.25\" width=\"0.5\" x=\"26.25\" y=\"9.25\"></rect><rect height=\"6.5\" rx=\"0.25\" transform=\"rotate(90 29.75 12.25)\" width=\"0.5\" x=\"29.75\" y=\"12.25\"></rect><path d=\"M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5\"></path><path d=\"M11.5 14.5L19 20l4-3\"></path></svg></button></a></span></div></div></div></div></div></div></div></div><div class=\"qa bg qb gm gn go gp gq\"></div></div></div><div class=\"h k j\"><div class=\"qa bg qb qc\"></div><div class=\"ab ca\"><div class=\"ch bg fy fz ga gb\"><div class=\"qd ab km jd\"><div class=\"qe qf l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://help.medium.com/hc/en-us?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Help</p></a></div><div class=\"qe qf l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.statuspage.io/?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Status</p></a></div><div class=\"qe qf l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"/about?autoplay=1&amp;source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">About</p></a></div><div class=\"qe qf l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Careers</p></a></div><div class=\"qe qf l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://blog.medium.com/?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Blog</p></a></div><div class=\"qe qf l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Privacy</p></a></div><div class=\"qe qf l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Terms</p></a></div><div class=\"qe qf l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://speechify.com/medium?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Text to speech</p></a></div><div class=\"qe l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"/business?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Teams</p></a></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__=\"main-20240314-165433-2ecaa51246\"</script><script>window.__GRAPHQL_URI__ = \"https://medium.com/_/graphql\"</script><script>window.__PRELOADED_STATE__ = {\"algolia\":{\"queries\":{}},\"cache\":{\"experimentGroupSet\":true,\"reason\":\"\",\"group\":\"enabled\",\"tags\":[\"group-edgeCachePosts\",\"post-7abfcb69da7f\",\"user-f6bbc6865a96\"],\"serverVariantState\":\"44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\",\"middlewareEnabled\":true,\"cacheStatus\":\"DYNAMIC\",\"shouldUseCache\":true,\"vary\":[],\"loHomepageEnabled\":false,\"updatedPostPreviewsEnabled\":false,\"customMocPreviewWeightThreshold\":\"control\"},\"client\":{\"hydrated\":false,\"isUs\":false,\"isNativeMedium\":false,\"isSafariMobile\":true,\"isSafari\":true,\"isFirefox\":false,\"routingEntity\":{\"type\":\"DEFAULT\",\"explicit\":false},\"viewerIsBot\":false},\"debug\":{\"requestId\":\"b82fe29d-ddea-4d40-90a0-463a9c6fd655\",\"hybridDevServices\":[],\"originalSpanCarrier\":{\"ot-tracer-spanid\":\"63861e807c52cfe0\",\"ot-tracer-traceid\":\"2ca7b9327aa4d1c7\",\"ot-tracer-sampled\":\"true\"}},\"multiVote\":{\"clapsPerPost\":{}},\"navigation\":{\"branch\":{\"show\":null,\"hasRendered\":null,\"blockedByCTA\":false},\"hideGoogleOneTap\":false,\"hasRenderedAlternateUserBanner\":null,\"currentLocation\":\"https:\\u002F\\u002Fmedium.com\\u002F@thedatabeast\\u002Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\",\"host\":\"medium.com\",\"hostname\":\"medium.com\",\"referrer\":\"\",\"hasSetReferrer\":false,\"susiModal\":{\"step\":null,\"operation\":\"register\"},\"postRead\":false,\"queryString\":\"\",\"currentHash\":\"\"},\"config\":{\"nodeEnv\":\"production\",\"version\":\"main-20240314-165433-2ecaa51246\",\"target\":\"production\",\"productName\":\"Medium\",\"publicUrl\":\"https:\\u002F\\u002Fcdn-client.medium.com\\u002Flite\",\"authDomain\":\"medium.com\",\"authGoogleClientId\":\"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com\",\"favicon\":\"production\",\"glyphUrl\":\"https:\\u002F\\u002Fglyph.medium.com\",\"branchKey\":\"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm\",\"algolia\":{\"appId\":\"MQ57UUUQZ2\",\"apiKeySearch\":\"394474ced050e3911ae2249ecc774921\",\"indexPrefix\":\"medium_\",\"host\":\"-dsn.algolia.net\"},\"recaptchaKey\":\"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk\",\"recaptcha3Key\":\"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5\",\"recaptchaEnterpriseKeyId\":\"6Le-uGgpAAAAAPprRaokM8AKthQ9KNGdoxaGUvVp\",\"datadog\":{\"applicationId\":\"6702d87d-a7e0-42fe-bbcb-95b469547ea0\",\"clientToken\":\"pub853ea8d17ad6821d9f8f11861d23dfed\",\"rumToken\":\"pubf9cc52896502b9413b68ba36fc0c7162\",\"context\":{\"deployment\":{\"target\":\"production\",\"tag\":\"main-20240314-165433-2ecaa51246\",\"commit\":\"2ecaa51246b5b6b06c3ecf42bfab62df66e454a7\"}},\"datacenter\":\"us\"},\"googleAnalyticsCode\":\"G-7JY7T788PK\",\"googlePay\":{\"apiVersion\":\"2\",\"apiVersionMinor\":\"0\",\"merchantId\":\"BCR2DN6TV7EMTGBM\",\"merchantName\":\"Medium\",\"instanceMerchantId\":\"13685562959212738550\"},\"applePay\":{\"version\":3},\"signInWallCustomDomainCollectionIds\":[\"3a8144eabfe3\",\"336d898217ee\",\"61061eb0c96b\",\"138adf9c44c\",\"819cc2aaeee0\"],\"mediumMastodonDomainName\":\"me.dm\",\"mediumOwnedAndOperatedCollectionIds\":[\"8a9336e5bb4\",\"b7e45b22fec3\",\"193b68bd4fba\",\"8d6b8a439e32\",\"54c98c43354d\",\"3f6ecf56618\",\"d944778ce714\",\"92d2092dc598\",\"ae2a65f35510\",\"1285ba81cada\",\"544c7006046e\",\"fc8964313712\",\"40187e704f1c\",\"88d9857e584e\",\"7b6769f2748b\",\"bcc38c8f6edf\",\"cef6983b292\",\"cb8577c9149e\",\"444d13b52878\",\"713d7dbc99b0\",\"ef8e90590e66\",\"191186aaafa0\",\"55760f21cdc5\",\"9dc80918cc93\",\"bdc4052bbdba\",\"8ccfed20cbb2\"],\"tierOneDomains\":[\"medium.com\",\"thebolditalic.com\",\"arcdigital.media\",\"towardsdatascience.com\",\"uxdesign.cc\",\"codeburst.io\",\"psiloveyou.xyz\",\"writingcooperative.com\",\"entrepreneurshandbook.co\",\"prototypr.io\",\"betterhumans.coach.me\",\"theascent.pub\"],\"topicsToFollow\":[\"d61cf867d93f\",\"8a146bc21b28\",\"1eca0103fff3\",\"4d562ee63426\",\"aef1078a3ef5\",\"e15e46793f8d\",\"6158eb913466\",\"55f1c20aba7a\",\"3d18b94f6858\",\"4861fee224fd\",\"63c6f1f93ee\",\"1d98b3a9a871\",\"decb52b64abf\",\"ae5d4995e225\",\"830cded25262\"],\"topicToTagMappings\":{\"accessibility\":\"accessibility\",\"addiction\":\"addiction\",\"android-development\":\"android-development\",\"art\":\"art\",\"artificial-intelligence\":\"artificial-intelligence\",\"astrology\":\"astrology\",\"basic-income\":\"basic-income\",\"beauty\":\"beauty\",\"biotech\":\"biotech\",\"blockchain\":\"blockchain\",\"books\":\"books\",\"business\":\"business\",\"cannabis\":\"cannabis\",\"cities\":\"cities\",\"climate-change\":\"climate-change\",\"comics\":\"comics\",\"coronavirus\":\"coronavirus\",\"creativity\":\"creativity\",\"cryptocurrency\":\"cryptocurrency\",\"culture\":\"culture\",\"cybersecurity\":\"cybersecurity\",\"data-science\":\"data-science\",\"design\":\"design\",\"digital-life\":\"digital-life\",\"disability\":\"disability\",\"economy\":\"economy\",\"education\":\"education\",\"equality\":\"equality\",\"family\":\"family\",\"feminism\":\"feminism\",\"fiction\":\"fiction\",\"film\":\"film\",\"fitness\":\"fitness\",\"food\":\"food\",\"freelancing\":\"freelancing\",\"future\":\"future\",\"gadgets\":\"gadgets\",\"gaming\":\"gaming\",\"gun-control\":\"gun-control\",\"health\":\"health\",\"history\":\"history\",\"humor\":\"humor\",\"immigration\":\"immigration\",\"ios-development\":\"ios-development\",\"javascript\":\"javascript\",\"justice\":\"justice\",\"language\":\"language\",\"leadership\":\"leadership\",\"lgbtqia\":\"lgbtqia\",\"lifestyle\":\"lifestyle\",\"machine-learning\":\"machine-learning\",\"makers\":\"makers\",\"marketing\":\"marketing\",\"math\":\"math\",\"media\":\"media\",\"mental-health\":\"mental-health\",\"mindfulness\":\"mindfulness\",\"money\":\"money\",\"music\":\"music\",\"neuroscience\":\"neuroscience\",\"nonfiction\":\"nonfiction\",\"outdoors\":\"outdoors\",\"parenting\":\"parenting\",\"pets\":\"pets\",\"philosophy\":\"philosophy\",\"photography\":\"photography\",\"podcasts\":\"podcast\",\"poetry\":\"poetry\",\"politics\":\"politics\",\"privacy\":\"privacy\",\"product-management\":\"product-management\",\"productivity\":\"productivity\",\"programming\":\"programming\",\"psychedelics\":\"psychedelics\",\"psychology\":\"psychology\",\"race\":\"race\",\"relationships\":\"relationships\",\"religion\":\"religion\",\"remote-work\":\"remote-work\",\"san-francisco\":\"san-francisco\",\"science\":\"science\",\"self\":\"self\",\"self-driving-cars\":\"self-driving-cars\",\"sexuality\":\"sexuality\",\"social-media\":\"social-media\",\"society\":\"society\",\"software-engineering\":\"software-engineering\",\"space\":\"space\",\"spirituality\":\"spirituality\",\"sports\":\"sports\",\"startups\":\"startup\",\"style\":\"style\",\"technology\":\"technology\",\"transportation\":\"transportation\",\"travel\":\"travel\",\"true-crime\":\"true-crime\",\"tv\":\"tv\",\"ux\":\"ux\",\"venture-capital\":\"venture-capital\",\"visual-design\":\"visual-design\",\"work\":\"work\",\"world\":\"world\",\"writing\":\"writing\"},\"defaultImages\":{\"avatar\":{\"imageId\":\"1*dmbNkD5D-u45r44go_cf0g.png\",\"height\":150,\"width\":150},\"orgLogo\":{\"imageId\":\"1*OMF3fSqH8t4xBJ9-6oZDZw.png\",\"height\":106,\"width\":545},\"postLogo\":{\"imageId\":\"1*kFrc4tBFM_tCis-2Ic87WA.png\",\"height\":810,\"width\":1440},\"postPreviewImage\":{\"imageId\":\"1*hn4v1tCaJy7cWMyb0bpNpQ.png\",\"height\":386,\"width\":579}},\"collectionStructuredData\":{\"8d6b8a439e32\":{\"name\":\"Elemental\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fcdn-images-1.medium.com\\u002Fmax\\u002F980\\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png\",\"width\":980,\"height\":159}}},\"3f6ecf56618\":{\"name\":\"Forge\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fcdn-images-1.medium.com\\u002Fmax\\u002F596\\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png\",\"width\":596,\"height\":183}}},\"ae2a65f35510\":{\"name\":\"GEN\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F264\\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png\",\"width\":264,\"height\":140}}},\"88d9857e584e\":{\"name\":\"LEVEL\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F540\\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png\",\"width\":540,\"height\":108}}},\"7b6769f2748b\":{\"name\":\"Marker\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fcdn-images-1.medium.com\\u002Fmax\\u002F383\\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png\",\"width\":383,\"height\":92}}},\"444d13b52878\":{\"name\":\"OneZero\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F540\\u002F1*cw32fIqCbRWzwJaoQw6BUg.png\",\"width\":540,\"height\":123}}},\"8ccfed20cbb2\":{\"name\":\"Zora\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F540\\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png\",\"width\":540,\"height\":106}}}},\"embeddedPostIds\":{\"coronavirus\":\"cd3010f9d81f\"},\"sharedCdcMessaging\":{\"COVID_APPLICABLE_TAG_SLUGS\":[],\"COVID_APPLICABLE_TOPIC_NAMES\":[],\"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE\":[],\"COVID_MESSAGES\":{\"tierA\":{\"text\":\"For more information on the novel coronavirus and Covid-19, visit cdc.gov.\",\"markups\":[{\"start\":66,\"end\":73,\"href\":\"https:\\u002F\\u002Fwww.cdc.gov\\u002Fcoronavirus\\u002F2019-nCoV\"}]},\"tierB\":{\"text\":\"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.\",\"markups\":[{\"start\":37,\"end\":45,\"href\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Fcategories\\u002F201931128-Policies-Safety\"},{\"start\":125,\"end\":132,\"href\":\"https:\\u002F\\u002Fwww.cdc.gov\\u002Fcoronavirus\\u002F2019-nCoV\"}]},\"paywall\":{\"text\":\"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.\",\"markups\":[{\"start\":56,\"end\":70,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fmembership\"},{\"start\":138,\"end\":145,\"href\":\"https:\\u002F\\u002Fwww.cdc.gov\\u002Fcoronavirus\\u002F2019-nCoV\"}]},\"unbound\":{\"text\":\"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.\",\"markups\":[{\"start\":45,\"end\":59,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fmembership\"},{\"start\":127,\"end\":134,\"href\":\"https:\\u002F\\u002Fwww.cdc.gov\\u002Fcoronavirus\\u002F2019-nCoV\"}]}},\"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST\":[\"3b31a67bff4a\"]},\"sharedVoteMessaging\":{\"TAGS\":[\"politics\",\"election-2020\",\"government\",\"us-politics\",\"election\",\"2020-presidential-race\",\"trump\",\"donald-trump\",\"democrats\",\"republicans\",\"congress\",\"republican-party\",\"democratic-party\",\"biden\",\"joe-biden\",\"maga\"],\"TOPICS\":[\"politics\",\"election\"],\"MESSAGE\":{\"text\":\"Find out more about the U.S. election results here.\",\"markups\":[{\"start\":46,\"end\":50,\"href\":\"https:\\u002F\\u002Fcookpolitical.com\\u002F2020-national-popular-vote-tracker\"}]},\"EXCLUDE_POSTS\":[\"397ef29e3ca5\"]},\"embedPostRules\":[],\"recircOptions\":{\"v1\":{\"limit\":3},\"v2\":{\"limit\":8}},\"braintreeClientKey\":\"production_zjkj96jm_m56f8fqpf7ngnrd4\",\"braintree\":{\"enabled\":true,\"merchantId\":\"m56f8fqpf7ngnrd4\",\"merchantAccountId\":{\"usd\":\"AMediumCorporation_instant\",\"eur\":\"amediumcorporation_EUR\",\"cad\":\"amediumcorporation_CAD\"},\"publicKey\":\"ds2nn34bg2z7j5gd\",\"braintreeEnvironment\":\"production\",\"dashboardUrl\":\"https:\\u002F\\u002Fwww.braintreegateway.com\\u002Fmerchants\",\"gracePeriodDurationInDays\":14,\"mediumMembershipPlanId\":{\"monthly\":\"ce105f8c57a3\",\"monthlyWithTrial\":\"d5ee3dbe3db8\",\"monthlyPremium\":\"fa741a9b47a2\",\"yearly\":\"a40ad4a43185\",\"yearlyStaff\":\"d74fb811198a\",\"yearlyWithTrial\":\"b3bc7350e5c7\",\"yearlyPremium\":\"e21bd2c12166\",\"monthlyCad\":\"p52orjkaceei\",\"yearlyCad\":\"h4q9g2up9ktt\"},\"braintreeDiscountId\":{\"oneMonthFree\":\"MONTHS_FREE_01\",\"threeMonthsFree\":\"MONTHS_FREE_03\",\"sixMonthsFree\":\"MONTHS_FREE_06\",\"fiftyPercentOffOneYear\":\"FIFTY_PERCENT_OFF_ONE_YEAR\"},\"3DSecureVersion\":\"2\",\"defaultCurrency\":\"usd\",\"providerPlanIdCurrency\":{\"4ycw\":\"usd\",\"rz3b\":\"usd\",\"3kqm\":\"usd\",\"jzw6\":\"usd\",\"c2q2\":\"usd\",\"nnsw\":\"usd\",\"q8qw\":\"usd\",\"d9y6\":\"usd\",\"fx7w\":\"cad\",\"nwf2\":\"cad\"}},\"paypalClientId\":\"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v\",\"paypal\":{\"host\":\"https:\\u002F\\u002Fapi.paypal.com:443\",\"clientMode\":\"production\",\"serverMode\":\"live\",\"webhookId\":\"4G466076A0294510S\",\"monthlyPlan\":{\"planId\":\"P-9WR0658853113943TMU5FDQA\",\"name\":\"Medium Membership (Monthly) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"yearlyPlan\":{\"planId\":\"P-7N8963881P8875835MU5JOPQ\",\"name\":\"Medium Membership (Annual) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"oneYearGift\":{\"name\":\"Medium Membership (1 Year, Digital Gift Code)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\\u002Fredeem.\",\"price\":\"50.00\",\"currency\":\"USD\",\"sku\":\"membership-gift-1-yr\"},\"oldMonthlyPlan\":{\"planId\":\"P-96U02458LM656772MJZUVH2Y\",\"name\":\"Medium Membership (Monthly)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"oldYearlyPlan\":{\"planId\":\"P-59P80963JF186412JJZU3SMI\",\"name\":\"Medium Membership (Annual)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"monthlyPlanWithTrial\":{\"planId\":\"P-66C21969LR178604GJPVKUKY\",\"name\":\"Medium Membership (Monthly) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"yearlyPlanWithTrial\":{\"planId\":\"P-6XW32684EX226940VKCT2MFA\",\"name\":\"Medium Membership (Annual) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"oldMonthlyPlanNoSetupFee\":{\"planId\":\"P-4N046520HR188054PCJC7LJI\",\"name\":\"Medium Membership (Monthly)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"oldYearlyPlanNoSetupFee\":{\"planId\":\"P-7A4913502Y5181304CJEJMXQ\",\"name\":\"Medium Membership (Annual)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"sdkUrl\":\"https:\\u002F\\u002Fwww.paypal.com\\u002Fsdk\\u002Fjs\"},\"stripePublishableKey\":\"pk_live_7FReX44VnNIInZwrIIx6ghjl\",\"log\":{\"json\":true,\"level\":\"info\"},\"imageUploadMaxSizeMb\":25,\"staffPicks\":{\"title\":\"Staff Picks\",\"catalogId\":\"c7bc6e1ee00f\"}},\"session\":{\"xsrf\":\"\"}}</script><script>window.__APOLLO_STATE__ = {\"ROOT_QUERY\":{\"__typename\":\"Query\",\"viewer\":null,\"isLoggedIn\":false,\"collectionByDomainOrSlug({\\\"domainOrSlug\\\":\\\"medium.com\\\"})\":null,\"postResult({\\\"id\\\":\\\"7abfcb69da7f\\\"})\":{\"__ref\":\"Post:7abfcb69da7f\"}},\"LinkedAccounts:f6bbc6865a96\":{\"__typename\":\"LinkedAccounts\",\"mastodon\":null,\"id\":\"f6bbc6865a96\"},\"UserViewerEdge:userId:f6bbc6865a96-viewerId:lo_b0048b579e0c\":{\"__typename\":\"UserViewerEdge\",\"id\":\"userId:f6bbc6865a96-viewerId:lo_b0048b579e0c\",\"isFollowing\":false,\"isUser\":false,\"isMuting\":false},\"NewsletterV3:920e325a463a\":{\"__typename\":\"NewsletterV3\",\"id\":\"920e325a463a\",\"type\":\"NEWSLETTER_TYPE_AUTHOR\",\"slug\":\"f6bbc6865a96\",\"name\":\"f6bbc6865a96\",\"collection\":null,\"user\":{\"__ref\":\"User:f6bbc6865a96\"}},\"User:f6bbc6865a96\":{\"__typename\":\"User\",\"id\":\"f6bbc6865a96\",\"name\":\"The Data Beast\",\"username\":\"thedatabeast\",\"newsletterV3\":{\"__ref\":\"NewsletterV3:920e325a463a\"},\"linkedAccounts\":{\"__ref\":\"LinkedAccounts:f6bbc6865a96\"},\"isSuspended\":false,\"imageId\":\"1*A7eSHxmpHV-LOcc4-DxVnw.png\",\"mediumMemberAt\":0,\"verifications\":{\"__typename\":\"VerifiedInfo\",\"isBookAuthor\":false},\"socialStats\":{\"__typename\":\"SocialStats\",\"followerCount\":327},\"customDomainState\":null,\"hasSubdomain\":false,\"bio\":\"\",\"isPartnerProgramEnrolled\":true,\"viewerEdge\":{\"__ref\":\"UserViewerEdge:userId:f6bbc6865a96-viewerId:lo_b0048b579e0c\"},\"viewerIsUser\":false,\"postSubscribeMembershipUpsellShownAt\":0,\"allowNotes\":true,\"membership\":null,\"twitterScreenName\":\"the_data_beast\"},\"Paragraph:2cf054fdb682_0\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_0\",\"name\":\"1be4\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*DJCRL6_IaSlWcebV_rDzjw.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*DJCRL6_IaSlWcebV_rDzjw.png\",\"originalHeight\":1024,\"originalWidth\":1024,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"Paragraph:2cf054fdb682_1\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_1\",\"name\":\"bf56\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"__ref\":\"ImageMetadata:1*DJCRL6_IaSlWcebV_rDzjw.png\"},\"text\":\"\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_2\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_2\",\"name\":\"62b7\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_3\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_3\",\"name\":\"b27d\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Link: BERT Paper\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":6,\"end\":16,\"href\":\"https:\\u002F\\u002Farxiv.org\\u002Fabs\\u002F1810.04805\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_4\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_4\",\"name\":\"6118\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Details: Released by Google AI Language team, BERT introduced a deep bidirectional architecture, which enhanced transfer learning demonstrated by unsupervised pre-training.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_5\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_5\",\"name\":\"d23e\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Practical Implementation: Used for state-of-the-art (SOTA) models in language inference and simple question-answer tasks​​.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_6\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_6\",\"name\":\"2349\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"2. BlenderBot 3: A deployed conversational agent that continually learns to responsibly engage\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_7\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_7\",\"name\":\"685c\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Link: BlenderBot 3 Paper\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":6,\"end\":24,\"href\":\"https:\\u002F\\u002Farxiv.org\\u002Fabs\\u002F2208.03188\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_8\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_8\",\"name\":\"d33f\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Details: From Meta AI, BlenderBot 3, with its 175 billion parameters, can scour the internet, setting it apart from other conversational bots.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_9\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_9\",\"name\":\"7127\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Practical Implementation: Continually learns from deployment data, enhancing engagement and response quality​​.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_10\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_10\",\"name\":\"3dc8\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"3. Improving alignment of dialogue agents via targeted human judgements\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_11\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_11\",\"name\":\"0d47\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Link: Sparrow Paper\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":6,\"end\":19,\"href\":\"https:\\u002F\\u002Farxiv.org\\u002Fabs\\u002F2209.14375\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_12\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_12\",\"name\":\"3ef2\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Details: DeepMind’s Sparrow employs the RLHF method for training, focusing on using human feedback for generative models.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_13\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_13\",\"name\":\"3e22\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Practical Implementation: Helps in building complex goals in chatbots by integrating human feedback effectively​​.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_14\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_14\",\"name\":\"2012\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"4. Improving Language Understanding by Generative Pre-Training\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_15\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_15\",\"name\":\"51bc\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Link: GPT Paper\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":6,\"end\":15,\"href\":\"https:\\u002F\\u002Fwww.semanticscholar.org\\u002Fpaper\\u002FImproving-Language-Understanding-by-Generative-Radford-Narasimhan\\u002Fcd18800a0fe0b668a1cc19f2ec95b5003d0a5035\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_16\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_16\",\"name\":\"1e7c\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Details: OpenAI’s paper on GPT, demonstrating the use of a decoder-style LLM for generative modeling.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_17\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_17\",\"name\":\"979b\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Practical Implementation: Pioneered NLP tasks by generative pre-training of a language model​​.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_18\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_18\",\"name\":\"5157\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"5. Scaling Laws for Neural Language Models\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_19\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_19\",\"name\":\"0dfe\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Link: Scaling Laws Paper\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":6,\"end\":24,\"href\":\"https:\\u002F\\u002Farxiv.org\\u002Fabs\\u002F2001.08361\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_20\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_20\",\"name\":\"52b8\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Details: OpenAI’s theoretical investigation into the relationship between the size of a language model and its performance.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_21\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_21\",\"name\":\"292b\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Practical Implementation: Provides empirical evidence for the scaling laws that govern model performance​​.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_22\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_22\",\"name\":\"2f6f\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"6. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation…\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Tag:llm\":{\"__typename\":\"Tag\",\"id\":\"llm\",\"displayTitle\":\"Llm\",\"normalizedTagSlug\":\"llm\"},\"Tag:ai\":{\"__typename\":\"Tag\",\"id\":\"ai\",\"displayTitle\":\"AI\",\"normalizedTagSlug\":\"ai\"},\"Tag:data-science\":{\"__typename\":\"Tag\",\"id\":\"data-science\",\"displayTitle\":\"Data Science\",\"normalizedTagSlug\":\"data-science\"},\"Tag:research\":{\"__typename\":\"Tag\",\"id\":\"research\",\"displayTitle\":\"Research\",\"normalizedTagSlug\":\"research\"},\"Tag:beginner\":{\"__typename\":\"Tag\",\"id\":\"beginner\",\"displayTitle\":\"Beginner\",\"normalizedTagSlug\":\"beginner\"},\"Post:7abfcb69da7f\":{\"__typename\":\"Post\",\"id\":\"7abfcb69da7f\",\"collection\":null,\"content({\\\"postMeteringOptions\\\":{}})\":{\"__typename\":\"PostContent\",\"isLockedPreviewOnly\":true,\"bodyModel\":{\"__typename\":\"RichText\",\"sections\":[{\"__typename\":\"Section\",\"name\":null,\"startIndex\":0,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null}],\"paragraphs\":[{\"__ref\":\"Paragraph:2cf054fdb682_0\"},{\"__ref\":\"Paragraph:2cf054fdb682_1\"},{\"__ref\":\"Paragraph:2cf054fdb682_2\"},{\"__ref\":\"Paragraph:2cf054fdb682_3\"},{\"__ref\":\"Paragraph:2cf054fdb682_4\"},{\"__ref\":\"Paragraph:2cf054fdb682_5\"},{\"__ref\":\"Paragraph:2cf054fdb682_6\"},{\"__ref\":\"Paragraph:2cf054fdb682_7\"},{\"__ref\":\"Paragraph:2cf054fdb682_8\"},{\"__ref\":\"Paragraph:2cf054fdb682_9\"},{\"__ref\":\"Paragraph:2cf054fdb682_10\"},{\"__ref\":\"Paragraph:2cf054fdb682_11\"},{\"__ref\":\"Paragraph:2cf054fdb682_12\"},{\"__ref\":\"Paragraph:2cf054fdb682_13\"},{\"__ref\":\"Paragraph:2cf054fdb682_14\"},{\"__ref\":\"Paragraph:2cf054fdb682_15\"},{\"__ref\":\"Paragraph:2cf054fdb682_16\"},{\"__ref\":\"Paragraph:2cf054fdb682_17\"},{\"__ref\":\"Paragraph:2cf054fdb682_18\"},{\"__ref\":\"Paragraph:2cf054fdb682_19\"},{\"__ref\":\"Paragraph:2cf054fdb682_20\"},{\"__ref\":\"Paragraph:2cf054fdb682_21\"},{\"__ref\":\"Paragraph:2cf054fdb682_22\"}]},\"validatedShareKey\":\"\",\"shareKeyCreator\":null},\"creator\":{\"__ref\":\"User:f6bbc6865a96\"},\"inResponseToEntityType\":null,\"isLocked\":true,\"isMarkedPaywallOnly\":false,\"lockedSource\":\"LOCKED_POST_SOURCE_UGC\",\"mediumUrl\":\"https:\\u002F\\u002Fmedium.com\\u002F@thedatabeast\\u002Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\",\"primaryTopic\":null,\"topics\":[{\"__typename\":\"Topic\",\"slug\":\"machine-learning\"}],\"isPublished\":true,\"latestPublishedVersion\":\"2cf054fdb682\",\"visibility\":\"LOCKED\",\"postResponses\":{\"__typename\":\"PostResponses\",\"count\":1},\"createdAt\":1699866174818,\"firstPublishedAt\":1699866559152,\"latestPublishedAt\":1699866559152,\"clapCount\":53,\"allowResponses\":true,\"isLimitedState\":false,\"title\":\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering…\",\"isSeries\":false,\"sequence\":null,\"uniqueSlug\":\"top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\",\"socialTitle\":\"\",\"socialDek\":\"\",\"noIndex\":null,\"canonicalUrl\":\"\",\"metaDescription\":\"\",\"readingTime\":1.9622641509433962,\"previewContent\":{\"__typename\":\"PreviewContent\",\"subtitle\":\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"},\"previewImage\":{\"__ref\":\"ImageMetadata:1*DJCRL6_IaSlWcebV_rDzjw.png\"},\"isShortform\":false,\"seoTitle\":\"\",\"updatedAt\":1699939894681,\"shortformType\":\"SHORTFORM_TYPE_LINK\",\"seoDescription\":\"\",\"isSuspended\":false,\"license\":\"ALL_RIGHTS_RESERVED\",\"tags\":[{\"__ref\":\"Tag:llm\"},{\"__ref\":\"Tag:ai\"},{\"__ref\":\"Tag:data-science\"},{\"__ref\":\"Tag:research\"},{\"__ref\":\"Tag:beginner\"}],\"pendingCollection\":null,\"statusForCollection\":null,\"detectedLanguage\":\"en\",\"wordCount\":467,\"layerCake\":0}}</script><script>window.__MIDDLEWARE_STATE__={\"session\":{\"xsrf\":\"\"},\"cache\":{\"cacheStatus\":\"HIT\"}}</script><script src=\"https://cdn-client.medium.com/lite/static/js/manifest.9578a15b.js\"></script><script src=\"https://cdn-client.medium.com/lite/static/js/3057.5e22bbb0.js\"></script><script src=\"https://cdn-client.medium.com/lite/static/js/main.b343c43b.js\"></script><script src=\"https://cdn-client.medium.com/lite/static/js/instrumentation.5e7f2981.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/reporting.2021fe63.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/4398.db4d4378.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/7883.0e445e04.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/6733.1d85727b.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/6481.e3e8b67f.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/8695.9065ba3d.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/4341.e697d2a1.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/5971.2c86ab13.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/5203.e7a22052.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/5465.248bcf72.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/6487.1ef0637a.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/5459.80a6ee18.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/6804.2cda7ee2.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/1711.b70f1a35.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/7652.f5b06845.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/2462.16c628db.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/9174.24f568ee.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/1128.3c169592.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/4129.ee8ae2c8.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/8580.feeb2549.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/8883.c8b03d13.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/4078.da7800a7.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/9408.3df4db57.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/9150.42fafb2e.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/5005.b5d4a37c.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/6605.4982ed5d.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/2393.aaa1ee6d.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/2211.706ab0f5.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/PostPage.MainContent.327397e5.chunk.js\"></script><script>window.main();</script><script crossorigin=\"anonymous\" data-cf-beacon='{\"rayId\":\"8648e8d0bedb578b\",\"b\":1,\"version\":\"2024.2.4\",\"token\":\"0b5f665943484354a59c39c6833f7078\"}' defer=\"\" integrity=\"sha512-euoFGowhlaLqXsPWQ48qSkBSCFs3DPRyiwVu3FjR96cMPx+Fr+gpWRhIafcHwqwCqWS42RZhIudOvEI+Ckf6MA==\" src=\"https://static.cloudflareinsights.com/beacon.min.js/v84a3a4012de94ce1a686ba8c167c359c1696973893317\"></script>\n</body></html>\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/f96f8fe8-df0a-4045-af5f-a7d4b42a6de1"},{"cell_type":"markdown","metadata":{"id":"TR-IFH1YcDke","deepnote_app_block_visible":true,"cell_id":"6969273f02bb420281a927da879f8112","deepnote_cell_type":"markdown"},"source":"### Convert resulting html into markdown","block_group":"325824d38afb4e399934590f1fbda7be"},{"cell_type":"code","metadata":{"id":"SCfo_vCVcDke","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fb377d00-e08a-472e-e765-dac66e5f4cec","source_hash":"4374083","execution_start":1710468022033,"execution_millis":3226,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"38a6fd08b43c49af8acb319e7aa72510","deepnote_cell_type":"code"},"source":"import html2text\n# Function to convert HTML to Markdown\ndef html_to_markdown(html_content):\n    # Create a converter object\n    converter = html2text.HTML2Text()\n    converter.ignore_links = False  # Set to True if you want to ignore converting links\n    \n    # Convert the HTML content to Markdown\n    markdown = converter.handle(html_content)\n\n    return markdown\n\nmarkdown = html_to_markdown(str(response['soup']))\nprint(markdown)","block_group":"8cc94267aef54334ba4fa598f67235c4","execution_count":221,"outputs":[{"name":"stdout","text":"[Open in\napp](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7abfcb69da7f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------)\n\nSign up\n\n[Sign\nin](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&source=post_page---two_column_layout_nav\n-----------------------global_nav-----------)\n\n[](/?source=---two_column_layout_nav----------------------------------)\n\n[Write](/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-\nstory&source=---two_column_layout_nav-----------------------\nnew_post_topnav-----------)\n\n[](/search?source=---two_column_layout_nav----------------------------------)\n\nSign up\n\n[Sign\nin](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&source=post_page---two_column_layout_nav\n-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\nMember-only story\n\n# Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023:\nPioneering Developments and Practical Applications\n\n[![The Data Beast](https://miro.medium.com/v2/resize:fill:88:88/1*A7eSHxmpHV-\nLOcc4-DxVnw.png)](/@thedatabeast?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[The Data Beast](/@thedatabeast?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n·\n\n[Follow](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6bbc6865a96&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&user=The+Data+Beast&userId=f6bbc6865a96&source=post_page-f6bbc6865a96\n----7abfcb69da7f---------------------post_header-----------)\n\n2 min read\n\n·\n\nNov 13, 2023\n\n[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F7abfcb69da7f&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&user=The+Data+Beast&userId=f6bbc6865a96&source=-----7abfcb69da7f\n---------------------clap_footer-----------)\n\n\\--\n\n1\n\n[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7abfcb69da7f&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&source=-----7abfcb69da7f---------------------\nbookmark_footer-----------)\n\nShare\n\n  1. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n\n  * Link: [BERT Paper](https://arxiv.org/abs/1810.04805)\n  * Details: Released by Google AI Language team, BERT introduced a deep bidirectional architecture, which enhanced transfer learning demonstrated by unsupervised pre-training.\n  * Practical Implementation: Used for state-of-the-art (SOTA) models in language inference and simple question-answer tasks​​.\n\n2\\. BlenderBot 3: A deployed conversational agent that continually learns to\nresponsibly engage\n\n  * Link: [BlenderBot 3 Paper](https://arxiv.org/abs/2208.03188)\n  * Details: From Meta AI, BlenderBot 3, with its 175 billion parameters, can scour the internet, setting it apart from other conversational bots.\n  * Practical Implementation: Continually learns from deployment data, enhancing engagement and response quality​​.\n\n3\\. Improving alignment of dialogue agents via targeted human judgements\n\n  * Link: [Sparrow Paper](https://arxiv.org/abs/2209.14375)\n  * Details: DeepMind’s Sparrow employs the RLHF method for training, focusing on using human feedback for generative models.\n  * Practical Implementation: Helps in building complex goals in chatbots by integrating human feedback effectively​​.\n\n4\\. Improving Language Understanding by Generative Pre-Training\n\n  * Link: [GPT Paper](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n  * Details: OpenAI’s paper on GPT, demonstrating the use of a decoder-style LLM for generative modeling.\n  * Practical Implementation: Pioneered NLP tasks by generative pre-training of a language model​​.\n\n5\\. Scaling Laws for Neural Language Models\n\n  * Link: [Scaling Laws Paper](https://arxiv.org/abs/2001.08361)\n  * Details: OpenAI’s theoretical investigation into the relationship between the size of a language model and its performance.\n  * Practical Implementation: Provides empirical evidence for the scaling laws that govern model performance​​.\n\n6\\. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language\nGeneration…\n\n[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F7abfcb69da7f&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&user=The+Data+Beast&userId=f6bbc6865a96&source=-----7abfcb69da7f\n---------------------clap_footer-----------)\n\n\\--\n\n[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F7abfcb69da7f&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&user=The+Data+Beast&userId=f6bbc6865a96&source=-----7abfcb69da7f\n---------------------clap_footer-----------)\n\n\\--\n\n1\n\n[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7abfcb69da7f&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&source=--------------------------bookmark_footer-----------)\n\n[![The Data\nBeast](https://miro.medium.com/v2/resize:fill:144:144/1*A7eSHxmpHV-\nLOcc4-DxVnw.png)](/@thedatabeast?source=post_page-----\n7abfcb69da7f--------------------------------)\n\nFollow\n\n[](/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F920e325a463a&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&newsletterV3=f6bbc6865a96&newsletterV3Id=920e325a463a&user=The+Data+Beast&userId=f6bbc6865a96&source=-----7abfcb69da7f\n---------------------subscribe_user-----------)\n\n## [Written by The Data Beast](/@thedatabeast?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[327 Followers](/@thedatabeast/followers?source=post_page-----\n7abfcb69da7f--------------------------------)\n\nFollow\n\n[](/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F920e325a463a&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&newsletterV3=f6bbc6865a96&newsletterV3Id=920e325a463a&user=The+Data+Beast&userId=f6bbc6865a96&source=-----7abfcb69da7f\n---------------------subscribe_user-----------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[About](/about?autoplay=1&source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[Careers](/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[Blog](https://blog.medium.com/?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-\npolicy-f03bf92035c9?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-\nservice-9db0094a1e0f?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[Teams](/business?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/7ec4e240-7344-438e-807b-5c2140d89982"},{"cell_type":"markdown","metadata":{"id":"wlDwyEyvcDke","deepnote_app_block_visible":true,"cell_id":"5da578362ba7423a9a797927fcfc504f","deepnote_cell_type":"markdown"},"source":"### Convert markdown into a structured JSON format using function calling","block_group":"c264d0bf9f694ac2b4706ca00f463cb2"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"3da761f3e9e541b58f50f314a487b1c3","deepnote_cell_type":"text-cell-p"},"source":"we'll structure the JSON to include the page title, page summary, and details for each paragraph (title, content, and links)","block_group":"0082319406244e5c8b7fd49fa6fe28e5"},{"cell_type":"code","metadata":{"id":"8x0Cm5xycDke","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f79ef568-15f1-4c02-911b-8df9f3998327","source_hash":"1e42b023","execution_start":1710468022084,"execution_millis":7177,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"8830ea8a1c9948198cf2654fc5ccad79","deepnote_cell_type":"code"},"source":"from pydantic import BaseModel, HttpUrl\nfrom typing import List\nfrom llama_index.program.openai import OpenAIPydanticProgram\nfrom llama_index.llms.openai import OpenAI\nimport tiktoken\nfrom llama_index.core.callbacks import CallbackManager, TokenCountingHandler\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\n# Define your Pydantic models\nclass Link(BaseModel):\n    url: HttpUrl\n\nclass Paper(BaseModel):\n    paper_title: str\n    content: str\n    links: List[Link] = []\n\nclass Page(BaseModel):\n    title: str\n    summary: str\n    paragraphs: List[Paper]\n\n# Define the OpenAI Pydantic program\ndef process_markdown(markdown: str, query: str):\n    max_length: int = 16000  # Updated max length for token count\n\n    # Check token length before splitting\n    token_count = count_tokens(markdown)  # Implement this function\n    if token_count > max_length:\n        markdown_parts = split_into_parts(markdown, max_length)\n    else:\n        markdown_parts = [markdown]  # No need to split\n\n    results = []\n    for part in markdown_parts:\n        print(\"Current part length (tokens):\", count_tokens(part))\n\n        # Define the OpenAI Pydantic program\n        prompt_template_str = \"\"\"\n        Given the following markdown_content, extract only structured information about academic papers including paper title, content, and links. The papers should reflect answers to the user query {user_query}:\n        {markdown_content}\n        \"\"\"\n        program = OpenAIPydanticProgram.from_defaults(\n            output_cls=Page,\n            llm=OpenAI(model=\"gpt-3.5-turbo-1106\"),\n            prompt_template_str=prompt_template_str,\n            allow_multiple=False,\n            verbose=True,\n        )\n\n        # Run the program to get structured output\n        description_str = f\"Structured json of search results based on a user {query}\"\n        try:\n            output = program(markdown_content=part, user_query=query, description=description_str)\n            results.append(output)\n        except Exception as e:\n            # Catch all exceptions\n            if hasattr(e, 'error') and 'message' in e.error:\n                print(f\"Error: {e.error['message']}\")\n            elif hasattr(e, 'args') and e.args:\n                print(f\"Error: {e.args[0]}\")\n            else:\n                print(f\"An unexpected error occurred: {e}\")\n            continue\n\n    # Combine results from all parts or handle as needed\n    combined_result = combine_page_results(results)\n    return combined_result\n\n# Function to count tokens (replace with your implementation)\ndef count_tokens(text: str) -> int:\n    # Use your preferred tokenizer (e.g., tiktoken)\n    tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n    return len(tokenizer(text))\n\n# Assuming 'results' is a list of Page objects or similar structured data\ndef combine_page_results(results: List[Page]) -> Page:\n    if not results:\n        return None  # Or some default value\n    \n    # Start with the title and summary from the first result\n    combined_title = results[0].title\n    combined_summary = results[0].summary\n    combined_paragraphs = []\n\n    # Iterate through all results and combine the paragraphs\n    for result in results:\n        combined_paragraphs.extend(result.paragraphs)  # Assuming 'paragraphs' is a list of 'Paper' objects\n    \n    # Create a new combined Page object\n    combined_page = Page(\n        title=combined_title,\n        summary=combined_summary,\n        paragraphs=combined_paragraphs\n    )\n    return combined_page\n\ndef split_into_parts(text: str, max_length: int) -> List[str]:\n    paragraphs = text.split('\\n\\n')\n    parts = []\n    current_part = \"\"\n\n    for paragraph in paragraphs:\n        if count_tokens(current_part) + count_tokens(paragraph) + 2 > max_length:  # +2 for the two newlines\n            parts.append(current_part)\n            current_part = paragraph  # Start new part with the current paragraph\n        else:\n            # Add paragraph to current part, include two newlines if it's not the first paragraph\n            current_part += ('\\n\\n' + paragraph) if current_part else paragraph\n\n    if current_part:  # Add the last part if not empty\n        parts.append(current_part)\n    \n    return parts\n\n\nresult = process_markdown(markdown, query)\nprint(result)","block_group":"f4717fadcdb9479e882bccadb360185f","execution_count":222,"outputs":[{"name":"stdout","text":"Current part length (tokens): 2513\nFunction call: Page with args: {\"title\":\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications\",\"summary\":\"The following are the top 10 breakthrough research papers on large language models (LLMs) in 2023, along with their practical applications and details.\",\"paragraphs\":[{\"paper_title\":\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\"content\":\"Released by Google AI Language team, BERT introduced a deep bidirectional architecture, which enhanced transfer learning demonstrated by unsupervised pre-training.\",\"links\":[{\"url\":\"https://arxiv.org/abs/1810.04805\"}]},{\"paper_title\":\"BlenderBot 3: A deployed conversational agent that continually learns to responsibly engage\",\"content\":\"From Meta AI, BlenderBot 3, with its 175 billion parameters, can scour the internet, setting it apart from other conversational bots.\",\"links\":[{\"url\":\"https://arxiv.org/abs/2208.03188\"}]},{\"paper_title\":\"Improving alignment of dialogue agents via targeted human judgements\",\"content\":\"DeepMind’s Sparrow employs the RLHF method for training, focusing on using human feedback for generative models.\",\"links\":[{\"url\":\"https://arxiv.org/abs/2209.14375\"}]},{\"paper_title\":\"Improving Language Understanding by Generative Pre-Training\",\"content\":\"OpenAI’s paper on GPT, demonstrating the use of a decoder-style LLM for generative modeling.\",\"links\":[{\"url\":\"https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035\"}]},{\"paper_title\":\"Scaling Laws for Neural Language Models\",\"content\":\"OpenAI’s theoretical investigation into the relationship between the size of a language model and its performance.\",\"links\":[{\"url\":\"https://arxiv.org/abs/2001.08361\"}]}]}\ntitle='Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications' summary='The following are the top 10 breakthrough research papers on large language models (LLMs) in 2023, along with their practical applications and details.' paragraphs=[Paper(paper_title='BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', content='Released by Google AI Language team, BERT introduced a deep bidirectional architecture, which enhanced transfer learning demonstrated by unsupervised pre-training.', links=[Link(url=HttpUrl('https://arxiv.org/abs/1810.04805', ))]), Paper(paper_title='BlenderBot 3: A deployed conversational agent that continually learns to responsibly engage', content='From Meta AI, BlenderBot 3, with its 175 billion parameters, can scour the internet, setting it apart from other conversational bots.', links=[Link(url=HttpUrl('https://arxiv.org/abs/2208.03188', ))]), Paper(paper_title='Improving alignment of dialogue agents via targeted human judgements', content='DeepMind’s Sparrow employs the RLHF method for training, focusing on using human feedback for generative models.', links=[Link(url=HttpUrl('https://arxiv.org/abs/2209.14375', ))]), Paper(paper_title='Improving Language Understanding by Generative Pre-Training', content='OpenAI’s paper on GPT, demonstrating the use of a decoder-style LLM for generative modeling.', links=[Link(url=HttpUrl('https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035', ))]), Paper(paper_title='Scaling Laws for Neural Language Models', content='OpenAI’s theoretical investigation into the relationship between the size of a language model and its performance.', links=[Link(url=HttpUrl('https://arxiv.org/abs/2001.08361', ))])]\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/b1245c1f-60cd-42ea-9940-b447a7161309"},{"cell_type":"code","metadata":{"id":"DZb-IZjXcDke","colab":{"base_uri":"https://localhost:8080/"},"outputId":"89704bdc-dd84-4a75-a5cc-337437b66aaf","source_hash":"6aa8e4bb","execution_start":1710468029268,"execution_millis":758,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"0b6506fb99b44058bff8c039d8dae400","deepnote_cell_type":"code"},"source":"import json\n\n# Assuming `output` is your object and it has a method `.dict()` to convert it to a dictionary.\n# If `output` is already a dictionary, you can skip the `.dict()` conversion.\noutput_dict = result.dict() if hasattr(result, 'dict') else result\n\n# Convert to JSON string with indentation for readability\npretty_output = json.dumps(output_dict, indent=4, default=str)\n\n# Print with added line breaks\nprint(pretty_output)","block_group":"acb1e3543ff849019a00c96527b04748","execution_count":223,"outputs":[{"name":"stdout","text":"{\n    \"title\": \"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications\",\n    \"summary\": \"The following are the top 10 breakthrough research papers on large language models (LLMs) in 2023, along with their practical applications and details.\",\n    \"paragraphs\": [\n        {\n            \"paper_title\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n            \"content\": \"Released by Google AI Language team, BERT introduced a deep bidirectional architecture, which enhanced transfer learning demonstrated by unsupervised pre-training.\",\n            \"links\": [\n                {\n                    \"url\": \"https://arxiv.org/abs/1810.04805\"\n                }\n            ]\n        },\n        {\n            \"paper_title\": \"BlenderBot 3: A deployed conversational agent that continually learns to responsibly engage\",\n            \"content\": \"From Meta AI, BlenderBot 3, with its 175 billion parameters, can scour the internet, setting it apart from other conversational bots.\",\n            \"links\": [\n                {\n                    \"url\": \"https://arxiv.org/abs/2208.03188\"\n                }\n            ]\n        },\n        {\n            \"paper_title\": \"Improving alignment of dialogue agents via targeted human judgements\",\n            \"content\": \"DeepMind\\u2019s Sparrow employs the RLHF method for training, focusing on using human feedback for generative models.\",\n            \"links\": [\n                {\n                    \"url\": \"https://arxiv.org/abs/2209.14375\"\n                }\n            ]\n        },\n        {\n            \"paper_title\": \"Improving Language Understanding by Generative Pre-Training\",\n            \"content\": \"OpenAI\\u2019s paper on GPT, demonstrating the use of a decoder-style LLM for generative modeling.\",\n            \"links\": [\n                {\n                    \"url\": \"https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035\"\n                }\n            ]\n        },\n        {\n            \"paper_title\": \"Scaling Laws for Neural Language Models\",\n            \"content\": \"OpenAI\\u2019s theoretical investigation into the relationship between the size of a language model and its performance.\",\n            \"links\": [\n                {\n                    \"url\": \"https://arxiv.org/abs/2001.08361\"\n                }\n            ]\n        }\n    ]\n}\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/7585ed88-080d-4a09-8116-f6d3554433fa"},{"cell_type":"code","metadata":{"id":"M70X-8aepAuW","colab":{"height":297,"base_uri":"https://localhost:8080/"},"outputId":"11e1c1ba-2216-4163-f33d-6387a72c171d","source_hash":"cb7f2bbc","execution_start":1710468029296,"execution_millis":880,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"8b6c49cdea1748af9d89532fc84022d9","deepnote_cell_type":"code"},"source":"def insert_papers_into_db(result, query):\n    arxiv_links = []\n    if result is None:\n        print(\"No data to insert into Papers table.\")\n        return\n    # Parse JSON data\n    output_dict = result.dict() if hasattr(result, 'dict') else result\n    pretty_output = json.dumps(output_dict, indent=4, default=str)\n    data = json.loads(pretty_output)\n    if data is None or 'paragraphs' not in data:\n        print(\"Invalid or empty data.\")\n        print('Parsed website stuctured data=', data)\n        return\n\n    # Connect to SQLite database\n    conn = connection()\n    c = conn.cursor()\n\n    try:\n        # Start transaction\n        c.execute(\"BEGIN;\")\n        # Insert data into Papers table\n        for paragraph in data['paragraphs']:\n            paper_title = paragraph['paper_title']\n            source_content = paragraph['content']\n            links = json.dumps(paragraph['links'])  # Convert list of links to JSON string\n            # Initialize an empty arXiv link\n            arxiv_link = None\n            # Search for the arXiv link among the links\n            for link in paragraph['links']:\n                if 'arxiv.org' in link['url']:\n                    temp_link = link['url'].replace('.pdf', '')  # Remove .pdf if present\n                    # Remove any trailing file identifiers after the arXiv ID\n                    temp_link = temp_link.split('/abs/')[1] if '/abs/' in temp_link else temp_link.split('/')[-1]\n                    arxiv_link = 'https://arxiv.org/abs/' + temp_link  # Construct the cleaned arXiv link\n                    # Add the arXiv link to the list\n                    if arxiv_link not in arxiv_links:\n                        arxiv_links.append(arxiv_link)\n                    break  # Stop searching once the arXiv link is found\n            # Check if the arxiv_link already exists in the database\n            c.execute('SELECT COUNT(*) FROM Papers WHERE arxiv_link = %s', (arxiv_link,))\n            if c.fetchone()[0] == 0:  # If the count is 0, then the link does not exist\n                # SQL statement for inserting data\n                insert_sql = '''\n                INSERT INTO Papers (paper_title, source_content, links, arxiv_link) VALUES (%s, %s, %s, %s)\n                '''\n                c.execute(insert_sql, (paper_title, source_content, links, arxiv_link))\n            else:\n                print(f'Skipping insert: arXiv link already exists in the database: {arxiv_link}')\n\n        for link in arxiv_links:\n            # Insert new row into Query_Papers if it does not exist\n            c.execute(\"INSERT INTO Query_Papers (query, arxiv_link) SELECT %s, %s WHERE NOT EXISTS (SELECT 1 FROM Query_Papers WHERE query = %s AND arxiv_link = %s)\", (query, link, query, link))\n\n        # Commit the transaction\n        conn.commit()\n        print(f\"Processed and inserted links associated with the query '{query}' into the database.\")\n\n    except Exception as e:\n        # Rollback the transaction on error\n        conn.rollback()\n        print(f\"An error occurred: {e}. Transaction was rolled back.\")\n\n    finally:\n        if conn:\n            conn.close()\n        pass\n\n# Example data\nquery = \"Example Query for Testing\"\nresult_data = {\n    \"title\": \"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications\",\n    \"summary\": \"The following are the top 10 breakthrough research papers on large language models (LLMs) in 2023, along with their practical applications and details.\",\n    \"paragraphs\": [\n        {\"paper_title\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"content\": \"Released by Google AI Language team, BERT introduced a deep bidirectional architecture, which enhanced transfer learning demonstrated by unsupervised pre-training.\", \"links\": [{\"url\": \"https://arxiv.org/abs/1810.04805\"}]},\n        {\"paper_title\": \"BlenderBot 3: A deployed conversational agent that continually learns to responsibly engage\", \"content\": \"From Meta AI, BlenderBot 3, with its 175 billion parameters, can scour the internet, setting it apart from other conversational bots.\", \"links\": [{\"url\": \"https://arxiv.org/abs/2208.03188\"}]},\n        # Add more papers as needed...\n    ]\n}\n# result_string = json.dumps(result_data)\ninsert_papers_into_db(result_data, query)","block_group":"03b49bf5028a4bc396f92e67ac2011e8","execution_count":224,"outputs":[{"name":"stdout","text":"Skipping insert: arXiv link already exists in the database: https://arxiv.org/abs/1810.04805\nSkipping insert: arXiv link already exists in the database: https://arxiv.org/abs/2208.03188\nProcessed and inserted links associated with the query 'Example Query for Testing' into the database.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/a044a352-8005-4e48-9d0b-b4f09ae2773f"},{"cell_type":"code","metadata":{"id":"OWn2sNiIp8uf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2fa2724f-ea6c-4fe1-e2db-3821a368efeb","allow_embed":false,"source_hash":"26ec98cd","execution_start":1710468030185,"execution_millis":1237,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"50f9b9b63f9e4b70968896648923a5d6","deepnote_cell_type":"code"},"source":"def print_papers_table():\n    conn = connection()\n    c = conn.cursor()\n\n    try:\n        # Start transaction (mainly useful if there are preceding data manipulations)\n        c.execute(\"BEGIN;\")\n\n        # Query all records from Papers table\n        query_sql = 'SELECT * FROM Papers'  # Add any condition if necessary\n        c.execute(query_sql)\n\n        # Fetch all rows from the query\n        all_rows = c.fetchall()\n\n        # Get the column names\n        field_names = [description[0] for description in c.description]\n\n        # Check if the table is not empty\n        if all_rows:\n            print(\"Preview of Papers Table:\")\n            for row_counter, row in enumerate(all_rows, start=1):\n                print(f\"Row {row_counter}:\")\n                row_with_field_names = {\n                    field_name: (content[:60] + '...' if isinstance(content, str) and len(content) > 60 else content) \n                    for field_name, content in zip(field_names, row)\n                }\n                for field, content in row_with_field_names.items():\n                    print(f\"{field}: {content}\")\n                print(\"-------------\")  # Separator for readability\n        else:\n            print(\"The Papers table is currently empty.\")\n\n        # Commit if there were preceding changes; otherwise, this is optional for read-only operations\n        conn.commit()\n\n    except Exception as e:\n        # Rollback any changes if an exception occurs\n        conn.rollback()\n        print(f\"An error occurred: {e}\")\n\n    finally:\n        if conn:\n            conn.close()\n        pass\n\n# Call the function\nprint_papers_table()","block_group":"6ab16ec9c7a64d16b69f5328f45ed294","execution_count":225,"outputs":[{"name":"stdout","text":" 67\n-------------\nRow 63:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.11822\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 29\nversions: 4\nid: 54\n-------------\nRow 64:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2201.11990\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 454\nversions: 4\nid: 83\n-------------\nRow 65:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.13971\narxiv_title: LLaMA: Open and Efficient Foundation Language Models\narxiv_abstract: We introduce LLaMA, a collection of foundation language mode...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf\narxiv_paper_markdown: None\ncitations: 4450\nversions: 13\nid: 5\n-------------\nRow 66:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.04236\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 427\n-------------\nRow 67:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.06235\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 429\n-------------\nRow 68:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.15996\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 431\n-------------\nRow 69:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.10403\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 664\nversions: 2\nid: 12\n-------------\nRow 70:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.05100\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 2\nversions: 6\nid: 25\n-------------\nRow 71:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.16056\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 433\n-------------\nRow 72:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.07292\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 434\n-------------\nRow 73:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.03188\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 198\nversions: 2\nid: 23\n-------------\nRow 74:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.03442\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 569\nversions: 5\nid: 37\n-------------\nRow 75:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.05332\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 89\nversions: 3\nid: 31\n-------------\nRow 76:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.08332\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 435\n-------------\nRow 77:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.07825\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 436\n-------------\nRow 78:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.14896\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 437\n-------------\nRow 79:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.09800\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 438\n-------------\nRow 80:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.09788\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 439\n-------------\nRow 81:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.00720\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 172\nversions: 3\nid: 45\n-------------\nRow 82:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.01324\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 441\n-------------\nRow 83:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.14375\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 260\nversions: 3\nid: 24\n-------------\nRow 84:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.05176\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 67\nversions: 3\nid: 47\n-------------\nRow 85:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.00611\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 442\n-------------\nRow 86:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.04091\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 36\nversions: 6\nid: 43\n-------------\nRow 87:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.13227\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 445\n-------------\nRow 88:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.13752\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 448\n-------------\nRow 89:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.03142\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 449\n-------------\nRow 90:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2201.08239\narxiv_title: LaMDA: Language Models for Dialog Applications\narxiv_abstract: We present LaMDA: Language Models for Dialog Applications. L...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: LaMDA_Language_Models_for_Dialog_Applications.pdf\narxiv_paper_markdown: None\ncitations: 992\nversions: 7\nid: 18\n-------------\nRow 91:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.09288\narxiv_title: Llama 2: Open Foundation and Fine-Tuned Chat Models\narxiv_abstract: In this work, we develop and release Llama 2, a collection o...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Llama_2_Open_Foundation_and_Fine-Tuned_Chat_Models.pdf\narxiv_paper_markdown: None\ncitations: 3054\nversions: 2\nid: 6\n-------------\nRow 92:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1811.06219\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 16\nversions: 4\nid: 393\n-------------\nRow 93:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1405.0312\narxiv_title: Microsoft COCO: Common Objects in Context\narxiv_abstract: We present a new dataset with the goal of advancing the stat...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Microsoft_COCO_Common_Objects_in_Context.pdf\narxiv_paper_markdown: None\ncitations: 44585\nversions: 24\nid: 369\n-------------\nRow 94:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1503.02531\narxiv_title: Distilling the Knowledge in a Neural Network\narxiv_abstract: A very simple way to improve the performance of almost any m...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Distilling_the_Knowledge_in_a_Neural_Network.pdf\narxiv_paper_markdown: None\ncitations: 18313\nversions: 28\nid: 351\n-------------\nRow 95:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18601\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 612\n-------------\nRow 96:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2010.08461\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 6\nversions: 3\nid: 406\n-------------\nRow 97:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.01702\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 4\nversions: 4\nid: 333\n-------------\nRow 98:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2110.03589\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 12\nversions: 4\nid: 397\n-------------\nRow 99:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2104.08040\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 14\nversions: 3\nid: 392\n-------------\nRow 100:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.09015\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 6\nversions: 3\nid: 335\n-------------\nRow 101:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1404.7584 \narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 6465\nversions: 19\nid: 370\n-------------\nRow 102:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2107.13841\narxiv_title: Addressing materials' microstructure diversity using transfe...\narxiv_abstract: Materials' microstructures are signatures of their alloying ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Addressing_materials'_microstructure_diversity_using_transfe...\narxiv_paper_markdown: None\ncitations: 11\nversions: 11\nid: 402\n-------------\nRow 103:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.12597\narxiv_title: BLIP-2: Bootstrapping Language-Image Pre-training with Froze...\narxiv_abstract: The cost of vision-and-language pre-training has become incr...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: BLIP-2_Bootstrapping_Language-Image_Pre-training_with_Frozen...\narxiv_paper_markdown: None\ncitations: 1444\nversions: 6\nid: 9\n-------------\nRow 104:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.03378\narxiv_title: PaLM-E: An Embodied Multimodal Language Model\narxiv_abstract: Large language models excel at a wide range of complex tasks...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: PaLM-E_An_Embodied_Multimodal_Language_Model.pdf\narxiv_paper_markdown: None\ncitations: 677\nversions: 6\nid: 11\n-------------\nRow 105:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18455\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 613\n-------------\nRow 106:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.14881\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 8\nversions: 7\nid: 336\n-------------\nRow 107:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.10919\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 1\nversions: 4\nid: 331\n-------------\nRow 108:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2101.10553\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 9\nversions: 6\nid: 403\n-------------\nRow 109:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2110.09326\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 2\nversions: 2\nid: 404\n-------------\nRow 110:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1803.07640\narxiv_title: AllenNLP: A Deep Semantic Natural Language Processing Platfo...\narxiv_abstract: This paper describes AllenNLP, a platform for research on de...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: AllenNLP_A_Deep_Semantic_Natural_Language_Processing_Platfor...\narxiv_paper_markdown: None\ncitations: 1328\nversions: 8\nid: 405\n-------------\nRow 111:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.06313\narxiv_title: The Lazy Neuron Phenomenon: On Emergence of Activation Spars...\narxiv_abstract: This paper studies the curious phenomenon for machine learni...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: The_Lazy_Neuron_Phenomenon_On_Emergence_of_Activation_Sparsi...\narxiv_paper_markdown: None\ncitations: 27\nversions: 4\nid: 334\n-------------\nRow 112:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18375\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 614\n-------------\nRow 113:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18274\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 615\n-------------\nRow 114:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.17214\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 616\n-------------\nRow 115:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16317\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 617\n-------------\nRow 116:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16261\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 618\n-------------\nRow 117:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16269\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 619\n-------------\nRow 118:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16150\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 620\n-------------\nRow 119:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.14849\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 621\n-------------\nRow 120:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.15583\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 622\n-------------\nRow 121:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.15241\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 623\n-------------\nRow 122:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.14712\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 624\n-------------\nRow 123:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.13311\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 625\n-------------\nRow 124:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2403.03206\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 452\n-------------\nRow 125:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.02301\narxiv_title: Distilling Step-by-Step! Outperforming Larger Language Model...\narxiv_abstract: Deploying large language models (LLMs) is challenging becaus...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Distilling_Step-by-Step!_Outperforming_Larger_Language_Model...\narxiv_paper_markdown: None\ncitations: 143\nversions: 8\nid: 33\n-------------\nRow 126:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.03629\narxiv_title: ReAct: Synergizing Reasoning and Acting in Language Models\narxiv_abstract: While large language models (LLMs) have demonstrated impress...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: ReAct_Synergizing_Reasoning_and_Acting_in_Language_Models.pd...\narxiv_paper_markdown: None\ncitations: 709\nversions: 6\nid: 49\n-------------\nRow 127:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.09748\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 454\n-------------\nRow 128:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.11093\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 455\n-------------\nRow 129:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.03003\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 458\n-------------\nRow 130:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.15571\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 459\n-------------\nRow 131:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.02747\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 460\n-------------\nRow 132:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.00364\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 462\n-------------\nRow 133:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.11513\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 464\n-------------\nRow 134:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.11675\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 626\n-------------\nRow 135:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.10657\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 627\n-------------\nRow 136:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.11089\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 628\n-------------\nRow 137:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.10924\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 629\n-------------\nRow 138:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.10769\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 630\n-------------\nRow 139:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.10135\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 631\n-------------\nRow 140:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.06402\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 632\n-------------\nRow 141:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.03935\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 633\n-------------\nRow 142:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.03989\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 634\n-------------\nRow 143:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.03486\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 635\n-------------\nRow 144:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.00556\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 636\n-------------\nRow 145:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.14404\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 637\n-------------\nRow 146:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.13224\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 638\n-------------\nRow 147:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.12536\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 639\n-------------\nRow 148:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.12526\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 640\n-------------\nRow 149:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.12141\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 641\n-------------\nRow 150:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.11603\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 642\n-------------\nRow 151:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.11312\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 643\n-------------\nRow 152:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.09787\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 644\n-------------\nRow 153:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.09752\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 645\n-------------\nRow 154:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.07429\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 646\n-------------\nRow 155:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.07087\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 647\n-------------\nRow 156:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.07060\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 648\n-------------\nRow 157:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.06648\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 649\n-------------\nRow 158:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.06767\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 650\n-------------\nRow 159:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.08073\narxiv_title: Constitutional AI: Harmlessness from AI Feedback\narxiv_abstract: As AI systems become more capable, we would like to enlist t...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Constitutional_AI_Harmlessness_from_AI_Feedback.pdf\narxiv_paper_markdown: None\ncitations: 512\nversions: 6\nid: 21\n-------------\nRow 160:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.14034\narxiv_title: Cramming: Training a Language Model on a Single GPU in One D...\narxiv_abstract: Recent trends in language modeling have focused on increasin...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Cramming_Training_a_Language_Model_on_a_Single_GPU_in_One_Da...\narxiv_paper_markdown: None\ncitations: 37\nversions: 7\nid: 19\n-------------\nRow 161:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.03047\narxiv_title: Principle-Driven Self-Alignment of Language Models from Scra...\narxiv_abstract: Recent AI-assistant agents, such as ChatGPT, predominantly r...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Principle-Driven_Self-Alignment_of_Language_Models_from_Scra...\narxiv_paper_markdown: None\ncitations: 118\nversions: 7\nid: 39\n-------------\nRow 162:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.06025\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 651\n-------------\nRow 163:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.04740\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 652\n-------------\nRow 164:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.04820\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 653\n-------------\nRow 165:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.03283\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 654\n-------------\nRow 166:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.02321\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 655\n-------------\nRow 167:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.02012\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 656\n-------------\nRow 168:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.17604\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 657\n-------------\nRow 169:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.18181\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 658\n-------------\nRow 170:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.18242\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 659\n-------------\nRow 171:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.17905\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 660\n-------------\nRow 172:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.17598\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 661\n-------------\nRow 173:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.17076\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 662\n-------------\nRow 174:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.14389\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 663\n-------------\nRow 175:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.13744\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 664\n-------------\nRow 176:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.12346\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 665\n-------------\nRow 177:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.10834\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 666\n-------------\nRow 178:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.09508\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 667\n-------------\nRow 179:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.09556\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 668\n-------------\nRow 180:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08320\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 669\n-------------\nRow 181:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08063\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 670\n-------------\nRow 182:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.06424\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 671\n-------------\nRow 183:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.06464\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 672\n-------------\nRow 184:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.05334\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 673\n-------------\nRow 185:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.04772\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 674\n-------------\nRow 186:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.04248\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 675\n-------------\nRow 187:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.01748\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 676\n-------------\nRow 188:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.01469\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 677\n-------------\nRow 189:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.00165\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 678\n-------------\nRow 190:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.12469\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 679\n-------------\nRow 191:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.11552\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 680\n-------------\nRow 192:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.10781\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 681\n-------------\nRow 193:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.10688\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 682\n-------------\nRow 194:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.10586\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 683\n-------------\nRow 195:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.10167\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 684\n-------------\nRow 196:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.05456\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 685\n-------------\nRow 197:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.09057\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 686\n-------------\nRow 198:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.08908\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 687\n-------------\nRow 199:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2204.11824\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 22\nversions: 2\nid: 466\n-------------\nRow 200:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.01626\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 650\nversions: 3\nid: 467\n-------------\nRow 201:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2109.04003\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 12\nversions: 6\nid: 473\n-------------\nRow 202:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2107.09562\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 22\nversions: 5\nid: 472\n-------------\nRow 203:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.05559\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 37\nversions: 3\nid: 468\n-------------\nRow 204:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.01618\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 697\nversions: 5\nid: 469\n-------------\nRow 205:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.06336\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 73\nversions: 2\nid: 88\n-------------\nRow 206:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.12017\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 55\nversions: 2\nid: 95\n-------------\nRow 207:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.02414\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 205\nversions: 5\nid: 91\n-------------\nRow 208:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.13688\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 306\nversions: 7\nid: 96\n-------------\nRow 209:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.01373\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 343\nversions: 7\nid: 98\n-------------\nRow 210:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.09110\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 536\nversions: 6\nid: 92\n-------------\nRow 211:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.14045\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 230\nversions: 4\nid: 97\n-------------\nRow 212:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.04761\narxiv_title: Toolformer: Language Models Can Teach Themselves to Use Tool...\narxiv_abstract: Language models (LMs) exhibit remarkable abilities to solve ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Toolformer_Language_Models_Can_Teach_Themselves_to_Use_Tools...\narxiv_paper_markdown: None\ncitations: 652\nversions: 4\nid: 13\n-------------\nRow 213:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.02155\narxiv_title: Training language models to follow instructions with human f...\narxiv_abstract: Making language models bigger does not inherently make them ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Training_language_models_to_follow_instructions_with_human_f...\narxiv_paper_markdown: None\ncitations: 5366\nversions: 17\nid: 22\n-------------\nRow 214:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2110.11444\narxiv_title: Density of States Prediction for Materials Discovery via Con...\narxiv_abstract: Machine learning for materials discovery has largely focused...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Density_of_States_Prediction_for_Materials_Discovery_via_Con...\narxiv_paper_markdown: None\ncitations: 43\nversions: 18\nid: 399\n-------------\nRow 215:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.11970\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 480\n-------------\nRow 216:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.10647\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 482\n-------------\nRow 217:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.07204\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 483\n-------------\nRow 218:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.16750\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 484\n-------------\nRow 219:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.13142\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 485\n-------------\nRow 220:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09388\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 486\n-------------\nRow 221:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.14135\narxiv_title: FlashAttention: Fast and Memory-Efficient Exact Attention wi...\narxiv_abstract: Transformers are slow and memory-hungry on long sequences, s...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: FlashAttention_Fast_and_Memory-Efficient_Exact_Attention_wit...\narxiv_paper_markdown: None\ncitations: 608\nversions: 9\nid: 20\n-------------\nRow 222:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.06500\narxiv_title: InstructBLIP: Towards General-purpose Vision-Language Models...\narxiv_abstract: Large-scale pre-training and instruction tuning have been su...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: InstructBLIP_Towards_General-purpose_Vision-Language_Models_...\narxiv_paper_markdown: None\ncitations: 398\nversions: 6\nid: 10\n-------------\nRow 223:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1612.01474\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 411\n-------------\nRow 224:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1810.04066\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 412\n-------------\nRow 225:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2007.01988\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 413\n-------------\nRow 226:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2103.02068\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 414\n-------------\nRow 227:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1811.08283\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 415\n-------------\nRow 228:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2106.02225\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 416\n-------------\nRow 229:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1904.11834\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 417\n-------------\nRow 230:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2003.09523\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 418\n-------------\nRow 231:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.04139\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 487\n-------------\nRow 232:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.04542\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 488\n-------------\nRow 233:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.14671\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 489\n-------------\nRow 234:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.00624\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 490\n-------------\nRow 235:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.04262\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 491\n-------------\nRow 236:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.01565\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 492\n-------------\nRow 237:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.13336\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 493\n-------------\nRow 238:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18290\narxiv_title: Direct Preference Optimization: Your Language Model is Secre...\narxiv_abstract: While large-scale unsupervised language models (LMs) learn b...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Direct_Preference_Optimization_Your_Language_Model_is_Secret...\narxiv_paper_markdown: None\ncitations: 364\nversions: 8\nid: 101\n-------------\nRow 239:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2202.13277\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 318\n-------------\nRow 240:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.06675\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 319\n-------------\nRow 241:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.02186\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 320\n-------------\nRow 242:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.02041\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 322\n-------------\nRow 243:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.01241\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 323\n-------------\nRow 244:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.11565\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 324\n-------------\nRow 245:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.01918\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 325\n-------------\nRow 246:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.07597\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 326\n-------------\nRow 247:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.15317\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 338\n-------------\nRow 248:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2107.07999\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 339\n-------------\nRow 249:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1412.6980\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 340\n-------------\nRow 250:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2202.00980\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 341\n-------------\nRow 251:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2103.05896\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 343\n-------------\nRow 252:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.03171\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 344\n-------------\nRow 253:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.12052\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 345\n-------------\nRow 254:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2006.07710\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 346\n-------------\nRow 255:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.09139\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 347\n-------------\nRow 256:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.01360\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 348\n-------------\nRow 257:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.05310\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 352\n-------------\nRow 258:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.06711\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 353\n-------------\nRow 259:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.01213\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 354\n-------------\nRow 260:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.06825\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 355\n-------------\nRow 261:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2002.06275\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 357\n-------------\nRow 262:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.12005\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 358\n-------------\nRow 263:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.12245\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 359\n-------------\nRow 264:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1806.07572\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 360\n-------------\nRow 265:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.12923\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 361\n-------------\nRow 266:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18438\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 362\n-------------\nRow 267:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.14988\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 367\n-------------\nRow 268:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1911.00361\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 368\n-------------\nRow 269:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1605.08695\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 372\n-------------\nRow 270:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1512.01274\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 373\n-------------\nRow 271:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1811.03378\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 374\n-------------\nRow 272:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1207.0580\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 375\n-------------\nRow 273:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1909.01315\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 376\n-------------\nRow 274:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2106.14232\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 377\n-------------\nRow 275:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2003.03123\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 378\n-------------\nRow 276:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1609.02907\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 379\n-------------\nRow 277:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1710.10903\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 380\n-------------\nRow 278:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1703.06103\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 381\n-------------\nRow 279:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1810.00826\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 382\n-------------\nRow 280:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1705.08415\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 383\n-------------\nRow 281:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1805.11973\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 385\n-------------\nRow 282:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2104.03270\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 386\n-------------\nRow 283:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2006.11287\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 387\n-------------\nRow 284:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1704.01212\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 388\n-------------\nRow 285:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2010.09435\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 389\n-------------\nRow 286:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1806.02473\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 390\n-------------\nRow 287:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1910.04858\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 407\n-------------\nRow 288:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2007.01720\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 408\n-------------\nRow 289:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2104.02395\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 409\n-------------\nRow 290:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1912.02757\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 410\n-------------\nRow 291:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1407.1808\narxiv_title: Simultaneous Detection and Segmentation\narxiv_abstract: We aim to detect all instances of a category in an image and...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Simultaneous_Detection_and_Segmentation.pdf\narxiv_paper_markdown: None\ncitations: 1571\nversions: 11\nid: 371\n-------------\nRow 292:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.05758\narxiv_title: Decoupled Context Processing for Context Augmented Language ...\narxiv_abstract: Language models can be augmented with a context retriever to...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Decoupled_Context_Processing_for_Context_Augmented_Language_...\narxiv_paper_markdown: None\ncitations: 14\nversions: 5\nid: 329\n-------------\nRow 293:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2009.14794\narxiv_title: Rethinking Attention with Performers\narxiv_abstract: We introduce Performers, Transformer architectures which can...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Rethinking_Attention_with_Performers.pdf\narxiv_paper_markdown: None\ncitations: 1211\nversions: 8\nid: 337\n-------------\nRow 294:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.05110\narxiv_title: Large Language Models with Controllable Working Memory\narxiv_abstract: Large language models (LLMs) have led to a series of breakth...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Large_Language_Models_with_Controllable_Working_Memory.pdf\narxiv_paper_markdown: None\ncitations: 42\nversions: 8\nid: 330\n-------------\nRow 295:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2003.00994\narxiv_title: Machine learning spectral indicators of topology\narxiv_abstract: Topological materials discovery has emerged as an important ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Machine_learning_spectral_indicators_of_topology.pdf\narxiv_paper_markdown: None\ncitations: 22\nversions: 16\nid: 398\n-------------\nRow 296:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1907.03222\narxiv_title: IRNet: A General Purpose Deep Residual Regression Framework ...\narxiv_abstract: Materials discovery is crucial for making scientific advance...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: IRNet_A_General_Purpose_Deep_Residual_Regression_Framework_f...\narxiv_paper_markdown: None\ncitations: 48\nversions: 8\nid: 396\n-------------\nRow 297:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2106.11132\narxiv_title: Rapid Discovery of Stable Materials by Coordinate-free Coars...\narxiv_abstract: A fundamental challenge in materials science pertains to elu...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Rapid_Discovery_of_Stable_Materials_by_Coordinate-free_Coars...\narxiv_paper_markdown: None\ncitations: 29\nversions: 8\nid: 394\n-------------\nRow 298:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1810.11203\narxiv_title: CrystalGAN: Learning to Discover Crystallographic Structures...\narxiv_abstract: Our main motivation is to propose an efficient approach to g...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: CrystalGAN_Learning_to_Discover_Crystallographic_Structures_...\narxiv_paper_markdown: None\ncitations: 89\nversions: 7\nid: 391\n-------------\nRow 299:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2104.10242\narxiv_title: Accelerating Materials Discovery with Bayesian Optimization ...\narxiv_abstract: Machine learning (ML) models utilizing structure-based featu...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Accelerating_Materials_Discovery_with_Bayesian_Optimization_...\narxiv_paper_markdown: None\ncitations: 76\nversions: 5\nid: 395\n-------------\nRow 300:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.10231\narxiv_title: Graph Neural Network Predictions of Metal Organic Framework ...\narxiv_abstract: The increasing CO2 level is a critical concern and suitable ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Graph_Neural_Network_Predictions_of_Metal_Organic_Framework_...\narxiv_paper_markdown: None\ncitations: 23\nversions: 6\nid: 401\n-------------\nRow 301:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.04461\narxiv_title: A Theoretical View on Sparsely Activated Networks\narxiv_abstract: Deep and wide neural networks successfully fit very complex ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: A_Theoretical_View_on_Sparsely_Activated_Networks.pdf\narxiv_paper_markdown: None\ncitations: 8\nversions: 8\nid: 332\n-------------\nRow 302:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2201.08348\narxiv_title: Prediction of the electron density of states for crystalline...\narxiv_abstract: Machine learning (ML) based models have greatly enhanced the...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Prediction_of_the_electron_density_of_states_for_crystalline...\narxiv_paper_markdown: None\ncitations: 21\nversions: 6\nid: 400\n-------------\nRow 303:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.07576\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 494\n-------------\nRow 304:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.07909\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 495\n-------------\nRow 305:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.06574\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 496\n-------------\nRow 306:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.10907\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 497\n-------------\nRow 307:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.02591\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 498\n-------------\nRow 308:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.07804\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 499\n-------------\nRow 309:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.09292\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 500\n-------------\nRow 310:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.04747\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 501\n-------------\nRow 311:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.02646\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 502\n-------------\nRow 312:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.00796\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 503\n-------------\nRow 313:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.19789\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 504\n-------------\nRow 314:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.07359\narxiv_title: Post-hoc Uncertainty Learning using a Dirichlet Meta-Model\narxiv_abstract: It is known that neural networks have the problem of being o...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Post-hoc_Uncertainty_Learning_using_a_Dirichlet_Meta-Model.p...\narxiv_paper_markdown: None\ncitations: 6\nversions: 7\nid: 327\n-------------\nRow 315:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.19653\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 505\n-------------\nRow 316:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.18823\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 506\n-------------\nRow 317:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.17590\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 507\n-------------\nRow 318:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.17467\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 508\n-------------\nRow 319:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.17167\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 509\n-------------\nRow 320:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.17153\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 510\n-------------\nRow 321:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.16074\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 511\n-------------\nRow 322:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.14189\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 512\n-------------\nRow 323:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.13545\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 513\n-------------\nRow 324:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.13102\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 514\n-------------\nRow 325:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.12395\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 515\n-------------\nRow 326:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.11311\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 516\n-------------\nRow 327:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.11142\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 517\n-------------\nRow 328:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.09912\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 518\n-------------\nRow 329:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.09469\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 519\n-------------\nRow 330:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.09213\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 520\n-------------\nRow 331:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.08442\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 521\n-------------\nRow 332:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.08337\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 522\n-------------\nRow 333:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.07894\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 523\n-------------\nRow 334:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.06389\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 524\n-------------\nRow 335:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.05737\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 525\n-------------\nRow 336:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.05264\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 526\n-------------\nRow 337:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.04750\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 527\n-------------\nRow 338:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.04041\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 528\n-------------\nRow 339:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.04378\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 529\n-------------\nRow 340:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.03337\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 530\n-------------\nRow 341:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.03270\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 531\n-------------\nRow 342:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.03218\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 532\n-------------\nRow 343:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.02664\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 533\n-------------\nRow 344:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.01400\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 534\n-------------\nRow 345:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.02279\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 535\n-------------\nRow 346:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.00808\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 536\n-------------\nRow 347:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.00318\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 537\n-------------\nRow 348:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.00106\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 538\n-------------\nRow 349:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.16948\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 539\n-------------\nRow 350:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.17074\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 540\n-------------\nRow 351:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.16421\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 541\n-------------\nRow 352:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.15726\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 542\n-------------\nRow 353:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.14564\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 543\n-------------\nRow 354:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.14068\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 544\n-------------\nRow 355:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.13274\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 545\n-------------\nRow 356:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.11043\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 546\n-------------\nRow 357:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.08511\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 547\n-------------\nRow 358:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.07906\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 548\n-------------\nRow 359:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.07867\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 549\n-------------\nRow 360:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.06642\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 550\n-------------\nRow 361:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.06169\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 551\n-------------\nRow 362:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.05153\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 552\n-------------\nRow 363:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.03350\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 553\n-------------\nRow 364:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.01875\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 554\n-------------\nRow 365:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.02119\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 555\n-------------\nRow 366:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.01274\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 556\n-------------\nRow 367:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.16682\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 557\n-------------\nRow 368:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.16534\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 558\n-------------\nRow 369:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.15321\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 559\n-------------\nRow 370:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.13712\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 560\n-------------\nRow 371:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.11948\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 561\n-------------\nRow 372:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.11941\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 562\n-------------\nRow 373:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.10257\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 563\n-------------\nRow 374:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.10187\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 564\n-------------\nRow 375:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.07896\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 565\n-------------\nRow 376:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.02157\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 566\n-------------\nRow 377:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.01316\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 567\n-------------\nRow 378:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.14648\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 568\n-------------\nRow 379:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.12122\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 569\n-------------\nRow 380:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.11308\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 570\n-------------\nRow 381:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.11118\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 571\n-------------\nRow 382:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.08698\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 572\n-------------\nRow 383:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.08199\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 573\n-------------\nRow 384:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.08283\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 574\n-------------\nRow 385:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.04787\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 575\n-------------\nRow 386:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.01924\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 576\n-------------\nRow 387:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.01952\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 577\n-------------\nRow 388:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.00574\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 578\n-------------\nRow 389:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.17046\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 579\n-------------\nRow 390:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.14153\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 580\n-------------\nRow 391:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.13720\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 581\n-------------\nRow 392:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.13078\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 582\n-------------\nRow 393:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.12511\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 583\n-------------\nRow 394:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.11251\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 584\n-------------\nRow 395:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.11173\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 585\n-------------\nRow 396:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.10441\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 586\n-------------\nRow 397:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.09192\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 587\n-------------\nRow 398:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.09274\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 588\n-------------\nRow 399:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.09305\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 589\n-------------\nRow 400:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.09086\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 590\n-------------\nRow 401:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.07685\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 688\n-------------\nRow 402:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.06908\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 689\n-------------\nRow 403:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.07261\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 690\n-------------\nRow 404:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.06504\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 691\n-------------\nRow 405:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.05259\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 692\n-------------\nRow 406:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.04867\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 693\n-------------\nRow 407:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.04411\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 694\n-------------\nRow 408:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.04304\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 695\n-------------\nRow 409:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.04265\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 696\n-------------\nRow 410:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.03686\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 697\n-------------\nRow 411:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.03130\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 698\n-------------\nRow 412:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.02373\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 699\n-------------\nRow 413:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.02272\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 700\n-------------\nRow 414:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.00670\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 701\n-------------\nRow 415:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.13721\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 702\n-------------\nRow 416:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.13362\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 703\n-------------\nRow 417:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.13622\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 704\n-------------\nRow 418:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.12935\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 705\n-------------\nRow 419:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.12334\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 706\n-------------\nRow 420:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.11558\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 707\n-------------\nRow 421:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.11706\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 708\n-------------\nRow 422:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.12003\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 709\n-------------\nRow 423:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.10972\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 710\n-------------\nRow 424:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.07969\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 712\n-------------\nRow 425:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.14678\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 713\n-------------\nRow 426:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.12990\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 714\n-------------\nRow 427:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.11972\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 715\n-------------\nRow 428:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.10777\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 716\n-------------\nRow 429:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.09478\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 717\n-------------\nRow 430:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.08861\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 719\n-------------\nRow 431:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.05973\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 720\n-------------\nRow 432:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.06726\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 721\n-------------\nRow 433:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.05199\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 722\n-------------\nRow 434:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.02802\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 723\n-------------\nRow 435:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.02024\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 724\n-------------\nRow 436:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.00235\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 725\n-------------\nRow 437:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.00362\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 726\n-------------\nRow 438:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.17084\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 727\n-------------\nRow 439:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.16750\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 728\n-------------\nRow 440:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.16152\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 729\n-------------\nRow 441:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.16032\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 730\n-------------\nRow 442:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.17091\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 731\n-------------\nRow 443:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.17106\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 732\n-------------\nRow 444:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.13449\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 733\n-------------\nRow 445:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.13221\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 734\n-------------\nRow 446:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.12445\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 736\n-------------\nRow 447:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.12039\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 737\n-------------\nRow 448:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.11742\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 738\n-------------\nRow 449:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.11138\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 739\n-------------\nRow 450:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.11743\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 740\n-------------\nRow 451:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.11018\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 741\n-------------\nRow 452:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.06956\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 742\n-------------\nRow 453:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.03264\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 743\n-------------\nRow 454:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.03595\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 744\n-------------\nRow 455:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.02048\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 745\n-------------\nRow 456:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.01364\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 746\n-------------\nRow 457:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.01156\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 747\n-------------\nRow 458:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.01095\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 748\n-------------\nRow 459:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.12254\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 749\n-------------\nRow 460:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.12867\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 750\n-------------\nRow 461:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.11058\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 751\n-------------\nRow 462:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.06462\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 752\n-------------\nRow 463:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.05475\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 753\n-------------\nRow 464:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.04955\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 754\n-------------\nRow 465:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.00939\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 756\n-------------\nRow 466:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.00471\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 757\n-------------\nRow 467:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.00586\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 758\n-------------\nRow 468:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.14593\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 759\n-------------\nRow 469:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.12152\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 760\n-------------\nRow 470:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.08725\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 761\n-------------\nRow 471:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.08256\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 762\n-------------\nRow 472:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.05557\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 763\n-------------\nRow 473:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.05442\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 764\n-------------\nRow 474:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.04439\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 765\n-------------\nRow 475:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.14699\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 766\n-------------\nRow 476:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.13753\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 767\n-------------\nRow 477:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.12675\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 768\n-------------\nRow 478:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.09392\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 769\n-------------\nRow 479:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.08664\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 770\n-------------\nRow 480:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.07791\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 771\n-------------\nRow 481:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.07131\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 772\n-------------\nRow 482:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.04202\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 773\n-------------\nRow 483:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.01864\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 774\n-------------\nRow 484:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2207.11192\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 775\n-------------\nRow 485:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2207.04316\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 776\n-------------\nRow 486:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2207.02196\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 777\n-------------\nRow 487:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.14464\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 778\n-------------\nRow 488:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.11474\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 779\n-------------\nRow 489:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.13397\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 780\n-------------\nRow 490:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.09012\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 781\n-------------\nRow 491:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.10365\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 782\n-------------\nRow 492:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.08889\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 783\n-------------\nRow 493:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.08265\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 784\n-------------\nRow 494:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.07309\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 785\n-------------\nRow 495:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.07696\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 786\n-------------\nRow 496:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.07771\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 787\n-------------\nRow 497:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.05564\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 788\n-------------\nRow 498:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.05173\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 789\n-------------\nRow 499:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.05039\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 790\n-------------\nRow 500:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.00070\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 795\n-------------\nRow 501:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.15463\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 796\n-------------\nRow 502:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.14987\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 797\n-------------\nRow 503:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.13699\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 798\n-------------\nRow 504:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.12524\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 799\n-------------\nRow 505:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.11495\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 800\n-------------\nRow 506:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.03859\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 802\n-------------\nRow 507:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.01490\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 803\n-------------\nRow 508:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2204.13902\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 804\n-------------\nRow 509:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2204.03458\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 806\n-------------\nRow 510:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2204.00227\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 807\n-------------\nRow 511:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.17260\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 808\n-------------\nRow 512:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.15636\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 809\n-------------\nRow 513:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.14206\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 810\n-------------\nRow 514:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.09481\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 811\n-------------\nRow 515:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.04304\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 812\n-------------\nRow 516:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2202.13460\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 813\n-------------\nRow 517:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2202.10166\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 814\n-------------\nRow 518:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2202.09778\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 815\n-------------\nRow 519:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2202.09671\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 816\n-------------\nRow 520:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2202.07477\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 817\n-------------\nRow 521:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2202.05830\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 818\n-------------\nRow 522:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2202.04895\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 819\n-------------\nRow 523:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2202.00512\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 820\n-------------\nRow 524:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2201.06503\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 821\n-------------\nRow 525:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2201.00308\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 822\n-------------\nRow 526:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.13339\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 823\n-------------\nRow 527:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.10741\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 824\n-------------\nRow 528:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.09788\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 826\n-------------\nRow 529:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.09164\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 827\n-------------\nRow 530:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.07804\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 828\n-------------\nRow 531:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.07068\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 829\n-------------\nRow 532:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.05744\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 830\n-------------\nRow 533:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.01799\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 831\n-------------\nRow 534:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2111.15640\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 832\n-------------\nRow 535:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2111.13606\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 833\n-------------\nRow 536:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2111.12701\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 834\n-------------\nRow 537:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2110.07579\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 835\n-------------\nRow 538:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2110.05948\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 836\n-------------\nRow 539:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2110.03237\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 837\n-------------\nRow 540:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2110.00473\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 838\n-------------\nRow 541:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2207.12598\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 839\n-------------\nRow 542:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2108.11514\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 840\n-------------\nRow 543:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2108.08827\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 841\n-------------\nRow 544:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2108.02938\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 842\n-------------\nRow 545:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2108.01073\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 843\n-------------\nRow 546:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2107.03006\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 844\n-------------\nRow 547:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2107.00630\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 845\n-------------\nRow 548:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2106.15671\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 846\n-------------\nRow 549:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2106.10410\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 847\n-------------\nRow 550:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2106.07582\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 848\n-------------\nRow 551:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2106.06819\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 849\n-------------\nRow 552:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2106.05931\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 850\n-------------\nRow 553:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2106.03802\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 851\n-------------\nRow 554:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2106.02808\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 852\n-------------\nRow 555:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2106.05527\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 853\n-------------\nRow 556:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2106.01357\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 854\n-------------\nRow 557:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2106.00132\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 855\n-------------\nRow 558:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2106.15282\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 856\n-------------\nRow 559:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2105.14080\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 857\n-------------\nRow 560:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.17432\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 874\n-------------\nRow 561:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.16779\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 875\n-------------\nRow 562:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.16349\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 876\n-------------\nRow 563:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.07138\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 877\n-------------\nRow 564:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.13415\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 879\n-------------\nRow 565:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.13097\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 880\n-------------\nRow 566:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.11125\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 881\n-------------\nRow 567:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.05956\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 882\n-------------\nRow 568:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.03893\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 883\n-------------\nRow 569:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.02049\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 884\n-------------\nRow 570:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.10916\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 885\n-------------\nRow 571:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09905\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 886\n-------------\nRow 572:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.07687\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 887\n-------------\nRow 573:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.04995\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 888\n-------------\nRow 574:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.00994\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 889\n-------------\nRow 575:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.00303\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 890\n-------------\nRow 576:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.16687\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 891\n-------------\nRow 577:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.16424\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 892\n-------------\nRow 578:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.09756\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 893\n-------------\nRow 579:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.08702\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 894\n-------------\nRow 580:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.08076\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 895\n-------------\nRow 581:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.07487\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 896\n-------------\nRow 582:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.14770\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 897\n-------------\nRow 583:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.11363\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 898\n-------------\nRow 584:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.10721\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 899\n-------------\nRow 585:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.09762\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 900\n-------------\nRow 586:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.08964\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 901\n-------------\nRow 587:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.05957\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 902\n-------------\nRow 588:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.15957\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 905\n-------------\nRow 589:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.15316\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 906\n-------------\nRow 590:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.12954\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 907\n-------------\nRow 591:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.12252\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 908\n-------------\nRow 592:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.08092\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 909\n-------------\nRow 593:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.00562\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 910\n-------------\nRow 594:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.08466\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 911\n-------------\nRow 595:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.08408\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 912\n-------------\nRow 596:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.16203\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 913\n-------------\nRow 597:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.06372\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 2\nversions: 3\nid: 878\n-------------\nRow 598:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.09853\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 133\nversions: 6\nid: 801\n-------------\nRow 599:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.15233\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 914\n-------------\nRow 600:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.14961\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 915\n-------------\nRow 601:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.14126\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 916\n-------------\nRow 602:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.09769\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 917\n-------------\nRow 603:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.03298\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 918\n-------------\nRow 604:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.08420\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 919\n-------------\nRow 605:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.05404\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 920\n-------------\nRow 606:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.11255\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 921\n-------------\nRow 607:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.07740\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 923\n-------------\nRow 608:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.16870\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 924\n-------------\nRow 609:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.13774\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 925\n-------------\nRow 610:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.12100\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 926\n-------------\nRow 611:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.08942\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 927\n-------------\nRow 612:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.18642\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 928\n-------------\nRow 613:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.15737\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 929\n-------------\nRow 614:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.14197\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 930\n-------------\nRow 615:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.12868\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 931\n-------------\nRow 616:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.10912\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 932\n-------------\nRow 617:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.11320\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 933\n-------------\nRow 618:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.09760\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 934\n-------------\nRow 619:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.00224\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 935\n-------------\nRow 620:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.14303\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 937\n-------------\nRow 621:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.13042\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 938\n-------------\nRow 622:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.05929\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 940\n-------------\nRow 623:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.04109\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 941\n-------------\nRow 624:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.03179\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 942\n-------------\nRow 625:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.02773\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 943\n-------------\nRow 626:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.01487\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 944\n-------------\nRow 627:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.01369\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 945\n-------------\nRow 628:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.01111\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 946\n-------------\nRow 629:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.16777\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 947\n-------------\nRow 630:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.16150\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 948\n-------------\nRow 631:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.16355\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 949\n-------------\nRow 632:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.12469\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 950\n-------------\nRow 633:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.12350\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 951\n-------------\nRow 634:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09223\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 952\n-------------\nRow 635:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.05695\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 953\n-------------\nRow 636:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.02959\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 954\n-------------\nRow 637:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.01127\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 955\n-------------\nRow 638:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.00122\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 956\n-------------\nRow 639:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.14066\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 957\n-------------\nRow 640:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.11654\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 958\n-------------\nRow 641:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.02138\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 960\n-------------\nRow 642:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.00773\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 961\n-------------\nRow 643:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.09949\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 962\n-------------\nRow 644:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.09316\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 963\n-------------\nRow 645:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.09004\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 964\n-------------\nRow 646:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.04321\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 965\n-------------\nRow 647:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.03878\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 966\n-------------\nRow 648:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.03437\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 967\n-------------\nRow 649:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.01721\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 968\n-------------\nRow 650:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.09447\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 969\n-------------\nRow 651:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.05424\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 970\n-------------\nRow 652:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.03048\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 971\n-------------\nRow 653:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.00067\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 973\n-------------\nRow 654:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.13416\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 974\n-------------\nRow 655:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.09534\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 975\n-------------\nRow 656:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.09383\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 976\n-------------\nRow 657:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.04745\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 977\n-------------\nRow 658:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.04429\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 978\n-------------\nRow 659:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.12313\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 979\n-------------\nRow 660:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.12031\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 980\n-------------\nRow 661:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.12343\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 981\n-------------\nRow 662:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.11681\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 982\n-------------\nRow 663:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.10326\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 984\n-------------\nRow 664:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.09813\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 985\n-------------\nRow 665:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08888\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 986\n-------------\nRow 666:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08333\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 987\n-------------\nRow 667:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.06040\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 988\n-------------\nRow 668:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.05105\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 989\n-------------\nRow 669:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.04803\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 990\n-------------\nRow 670:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.11798\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 991\n-------------\nRow 671:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.02773\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 992\n-------------\nRow 672:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.00787\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 993\n-------------\nRow 673:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.13224\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 994\n-------------\nRow 674:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.06150\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 995\n-------------\nRow 675:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.17408\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 997\n-------------\nRow 676:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.01713\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 998\n-------------\nRow 677:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.14566\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 999\n-------------\nRow 678:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2207.00050\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1002\n-------------\nRow 679:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.11892\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1003\n-------------\nRow 680:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.03461\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1005\n-------------\nRow 681:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.11423\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1006\n-------------\nRow 682:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.03145\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1007\n-------------\nRow 683:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.03126\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1008\n-------------\nRow 684:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.00390\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1009\n-------------\nRow 685:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.06668\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1011\n-------------\nRow 686:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.03729\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1012\n-------------\nRow 687:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.16490\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1013\n-------------\nRow 688:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.15854\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1014\n-------------\nRow 689:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.13767\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1015\n-------------\nRow 690:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.10156\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1016\n-------------\nRow 691:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.10079\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1017\n-------------\nRow 692:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.07863\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1018\n-------------\nRow 693:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.07665\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1019\n-------------\nRow 694:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.06101\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1020\n-------------\nRow 695:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.06057\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1021\n-------------\nRow 696:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.03183\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1022\n-------------\nRow 697:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.02154\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1023\n-------------\nRow 698:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.12560\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1024\n-------------\nRow 699:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.12493\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1025\n-------------\nRow 700:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.05899\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1026\n-------------\nRow 701:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.04157\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1027\n-------------\nRow 702:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.02698\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1028\n-------------\nRow 703:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.02421\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1029\n-------------\nRow 704:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.14435\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1030\n-------------\nRow 705:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.09330\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1031\n-------------\nRow 706:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.08757\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1032\n-------------\nRow 707:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.08276\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1033\n-------------\nRow 708:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.04396\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1034\n-------------\nRow 709:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18812\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1035\n-------------\nRow 710:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18729\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1036\n-------------\nRow 711:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18286\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1037\n-------------\nRow 712:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16289\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1038\n-------------\nRow 713:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.15086\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1039\n-------------\nRow 714:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.13839\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1040\n-------------\nRow 715:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.06710\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1041\n-------------\nRow 716:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.04651\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1042\n-------------\nRow 717:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.11829\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1043\n-------------\nRow 718:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.06711\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1044\n-------------\nRow 719:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.03199\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1045\n-------------\nRow 720:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.09748\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1046\n-------------\nRow 721:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.15403\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1047\n-------------\nRow 722:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.12724\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1048\n-------------\nRow 723:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.03231\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1049\n-------------\nRow 724:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.06826\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1050\n-------------\nRow 725:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.05872\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1051\n-------------\nRow 726:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.13743\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1052\n-------------\nRow 727:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.13344\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1053\n-------------\nRow 728:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.06458\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1054\n-------------\nRow 729:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.13203\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1055\n-------------\nRow 730:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.12500\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1056\n-------------\nRow 731:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.15264\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1059\n-------------\nRow 732:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.11047\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1060\n-------------\nRow 733:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2207.14626\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1061\n-------------\nRow 734:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2207.09786\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1062\n-------------\nRow 735:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2207.08208\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1063\n-------------\nRow 736:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2207.06635\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1064\n-------------\nRow 737:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.12952\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1066\n-------------\nRow 738:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.07680\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1067\n-------------\nRow 739:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2204.02641\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1068\n-------------\nRow 740:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.08382\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1069\n-------------\nRow 741:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2201.11793\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1070\n-------------\nRow 742:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.05149\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1071\n-------------\nRow 743:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.19288\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1076\n-------------\nRow 744:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.17577\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1077\n-------------\nRow 745:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.16047\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1078\n-------------\nRow 746:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.14458\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1079\n-------------\nRow 747:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.14237\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1080\n-------------\nRow 748:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.12004\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1081\n-------------\nRow 749:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.10325\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1082\n-------------\nRow 750:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.10123\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1083\n-------------\nRow 751:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.09484\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1084\n-------------\nRow 752:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.06949\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1085\n-------------\nRow 753:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.01799\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1086\n-------------\nRow 754:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.01407\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1087\n-------------\nRow 755:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.01130\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1088\n-------------\nRow 756:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.01110\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1089\n-------------\nRow 757:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.15117\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1091\n-------------\nRow 758:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.14709\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1092\n-------------\nRow 759:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.14394\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1093\n-------------\nRow 760:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.14360\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1094\n-------------\nRow 761:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.12506\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1095\n-------------\nRow 762:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.11715\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1096\n-------------\nRow 763:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.11321\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1097\n-------------\nRow 764:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.10810\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1098\n-------------\nRow 765:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.10714\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1099\n-------------\nRow 766:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.09614\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1100\n-------------\nRow 767:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.11507\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1101\n-------------\nRow 768:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.03445\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1102\n-------------\nRow 769:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.01949\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1103\n-------------\nRow 770:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.00783\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1104\n-------------\nRow 771:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.00853\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1105\n-------------\nRow 772:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.00287\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1106\n-------------\nRow 773:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.16742\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1107\n-------------\nRow 774:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.15942\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1108\n-------------\nRow 775:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.15918\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1109\n-------------\nRow 776:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.15070\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1110\n-------------\nRow 777:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.14469\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1111\n-------------\nRow 778:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.14437\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1112\n-------------\nRow 779:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.13164\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1114\n-------------\nRow 780:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.13072\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1115\n-------------\nRow 781:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.12465\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1116\n-------------\nRow 782:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.11949\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1117\n-------------\nRow 783:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.10510\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1118\n-------------\nRow 784:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.10157\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1119\n-------------\nRow 785:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09279\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1120\n-------------\nRow 786:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.08730\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1121\n-------------\nRow 787:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.07983\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1122\n-------------\nRow 788:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.07652\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1123\n-------------\nRow 789:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.07977\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1124\n-------------\nRow 790:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.06743\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1125\n-------------\nRow 791:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.06725\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1126\n-------------\nRow 792:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.02283\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1127\n-------------\nRow 793:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.02228\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1128\n-------------\nRow 794:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.01594\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1129\n-------------\nRow 795:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.01096\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1130\n-------------\nRow 796:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.15990\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1131\n-------------\nRow 797:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.14659\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1132\n-------------\nRow 798:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.14262\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1133\n-------------\nRow 799:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.12348\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1134\n-------------\nRow 800:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.12070\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1135\n-------------\nRow 801:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.11926\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1136\n-------------\nRow 802:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.10584\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1137\n-------------\nRow 803:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.09481\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1138\n-------------\nRow 804:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.08996\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1139\n-------------\nRow 805:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.08585\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1141\n-------------\nRow 806:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.08123\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1142\n-------------\nRow 807:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.07710\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1143\n-------------\nRow 808:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.04946\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1144\n-------------\nRow 809:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.03992\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1145\n-------------\nRow 810:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.03177\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1146\n-------------\nRow 811:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.02814\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1147\n-------------\nRow 812:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.00781\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1148\n-------------\nRow 813:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.00522\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1149\n-------------\nRow 814:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.00619\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1150\n-------------\nRow 815:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.17717\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1151\n-------------\nRow 816:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.16654\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1152\n-------------\nRow 817:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.16052\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1153\n-------------\nRow 818:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.15832\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1154\n-------------\nRow 819:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.14227\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1155\n-------------\nRow 820:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.13384\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1156\n-------------\nRow 821:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.12867\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1157\n-------------\nRow 822:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.12109\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1158\n-------------\nRow 823:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.12085\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1159\n-------------\nRow 824:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.11719\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1160\n-------------\nRow 825:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.07440\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1161\n-------------\nRow 826:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.03727\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1162\n-------------\nRow 827:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.02949\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1163\n-------------\nRow 828:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.01923\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1164\n-------------\nRow 829:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00714\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1165\n-------------\nRow 830:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00306\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1166\n-------------\nRow 831:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.20049\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1167\n-------------\nRow 832:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.19809\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1168\n-------------\nRow 833:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16965\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1169\n-------------\nRow 834:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16301\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1170\n-------------\nRow 835:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.15887\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1171\n-------------\nRow 836:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.15357\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1172\n-------------\nRow 837:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.13819\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1173\n-------------\nRow 838:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.12170\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1174\n-------------\nRow 839:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.11147\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1175\n-------------\nRow 840:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.10028\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1176\n-------------\nRow 841:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.09121\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1177\n-------------\nRow 842:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.08995\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1178\n-------------\nRow 843:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.07015\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1179\n-------------\nRow 844:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.05077\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1180\n-------------\nRow 845:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.04745\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1181\n-------------\nRow 846:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.04517\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1182\n-------------\nRow 847:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.04457\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1183\n-------------\nRow 848:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.04391\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1184\n-------------\nRow 849:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.03901\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1185\n-------------\nRow 850:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.03892\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1186\n-------------\nRow 851:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.01166\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1187\n-------------\nRow 852:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.01165\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1188\n-------------\nRow 853:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.11751\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1189\n-------------\nRow 854:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.11105\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1190\n-------------\nRow 855:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.09479\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1191\n-------------\nRow 856:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.06790\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1192\n-------------\nRow 857:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.08291\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1193\n-------------\nRow 858:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.05060\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1194\n-------------\nRow 859:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.03760\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1195\n-------------\nRow 860:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.03174\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1196\n-------------\nRow 861:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.03246\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1197\n-------------\nRow 862:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.03322\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1198\n-------------\nRow 863:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.02742\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1199\n-------------\nRow 864:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.01994\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1200\n-------------\nRow 865:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.01814\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1201\n-------------\nRow 866:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.01247\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1202\n-------------\nRow 867:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.16491\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1203\n-------------\nRow 868:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.14353\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1204\n-------------\nRow 869:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.14139\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1205\n-------------\nRow 870:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.13933\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1206\n-------------\nRow 871:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.12861\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1207\n-------------\nRow 872:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.12618\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1208\n-------------\nRow 873:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.11435\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1209\n-------------\nRow 874:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.10096\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1210\n-------------\nRow 875:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.09627\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1211\n-------------\nRow 876:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.09642\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1212\n-------------\nRow 877:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.09472\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1213\n-------------\nRow 878:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08714\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1214\n-------------\nRow 879:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08863\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1215\n-------------\nRow 880:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08189\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1216\n-------------\nRow 881:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.06994\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1217\n-------------\nRow 882:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.06885\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1218\n-------------\nRow 883:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.06682\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1219\n-------------\nRow 884:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.05754\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1220\n-------------\nRow 885:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.05686\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1221\n-------------\nRow 886:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.05021\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1222\n-------------\nRow 887:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.04603\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1223\n-------------\nRow 888:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.00354\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1224\n-------------\nRow 889:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.10326\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1225\n-------------\nRow 890:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.08411\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1227\n-------------\nRow 891:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.07864\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1228\n-------------\nRow 892:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.12831\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1229\n-------------\nRow 893:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.03791\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1230\n-------------\nRow 894:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.03018\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1231\n-------------\nRow 895:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.02398\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1232\n-------------\nRow 896:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.01217\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1233\n-------------\nRow 897:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.12686\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1234\n-------------\nRow 898:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.11482\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1236\n-------------\nRow 899:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.11376\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1237\n-------------\nRow 900:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.11785\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1238\n-------------\nRow 901:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.09430\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1239\n-------------\nRow 902:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.05290\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1241\n-------------\nRow 903:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.11699\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1242\n-------------\nRow 904:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.08815\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1243\n-------------\nRow 905:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.07557\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1244\n-------------\nRow 906:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.03027\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1245\n-------------\nRow 907:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.13771\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1246\n-------------\nRow 908:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.12678\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1247\n-------------\nRow 909:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.07352\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1248\n-------------\nRow 910:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.11274\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1249\n-------------\nRow 911:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.07599\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1250\n-------------\nRow 912:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.06512\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1251\n-------------\nRow 913:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.04711\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1252\n-------------\nRow 914:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.03630\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1253\n-------------\nRow 915:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.02963\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1254\n-------------\nRow 916:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.03221\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1255\n-------------\nRow 917:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.01789\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1256\n-------------\nRow 918:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.00490\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1257\n-------------\nRow 919:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.16678\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1258\n-------------\nRow 920:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.14286\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1259\n-------------\nRow 921:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.12340\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1260\n-------------\nRow 922:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.12343\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1261\n-------------\nRow 923:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.10656\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1262\n-------------\nRow 924:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.10655\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1263\n-------------\nRow 925:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.10388\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1264\n-------------\nRow 926:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.10437\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1265\n-------------\nRow 927:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.09795\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1266\n-------------\nRow 928:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.12845\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1267\n-------------\nRow 929:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.09206\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1268\n-------------\nRow 930:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.08089\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1269\n-------------\nRow 931:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.06757\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1270\n-------------\nRow 932:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.13006\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1272\n-------------\nRow 933:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.17106\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1273\n-------------\nRow 934:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.12113\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1274\n-------------\nRow 935:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.08573\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1275\n-------------\nRow 936:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.15136\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1276\n-------------\nRow 937:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.14687\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1277\n-------------\nRow 938:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.12064\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1278\n-------------\nRow 939:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.11888\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1279\n-------------\nRow 940:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.08814\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1280\n-------------\nRow 941:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.08217\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1281\n-------------\nRow 942:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.06167\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1282\n-------------\nRow 943:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.00835\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1283\n-------------\nRow 944:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.11284\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1284\n-------------\nRow 945:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.05481\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1286\n-------------\nRow 946:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2207.05876\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1290\n-------------\nRow 947:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2207.03430\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1291\n-------------\nRow 948:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.04514\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1292\n-------------\nRow 949:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.00941\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1293\n-------------\nRow 950:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.12621\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1295\n-------------\nRow 951:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.04292\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1296\n-------------\nRow 952:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.03623\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1297\n-------------\nRow 953:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2202.01479\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1298\n-------------\nRow 954:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2201.11760\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1299\n-------------\nRow 955:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2201.09865\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1301\n-------------\nRow 956:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.05146\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1304\n-------------\nRow 957:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.02475\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1305\n-------------\nRow 958:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2111.08005\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1307\n-------------\nRow 959:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2111.04639\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1308\n-------------\nRow 960:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2110.05243\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1309\n-------------\nRow 961:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2110.02037\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1310\n-------------\nRow 962:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.10209\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1318\n-------------\nRow 963:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.09625\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1319\n-------------\nRow 964:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.08654\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1320\n-------------\nRow 965:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.07131\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1321\n-------------\nRow 966:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.05299\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1323\n-------------\nRow 967:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.05237\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1324\n-------------\nRow 968:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.03893\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1325\n-------------\nRow 969:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.03559\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1326\n-------------\nRow 970:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.03118\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1327\n-------------\nRow 971:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.16205\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1329\n-------------\nRow 972:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.09328\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1330\n-------------\nRow 973:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.05406\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1332\n-------------\nRow 974:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.01611\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1334\n-------------\nRow 975:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.00748\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1338\n-------------\nRow 976:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.12453\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1346\n-------------\nRow 977:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.10490\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1348\n-------------\nRow 978:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09345\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1350\n-------------\nRow 979:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.08339\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1352\n-------------\nRow 980:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.06781\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1353\n-------------\nRow 981:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.04020\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1355\n-------------\nRow 982:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.03354\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1356\n-------------\nRow 983:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.02587\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1358\n-------------\nRow 984:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.02062\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1359\n-------------\nRow 985:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.01328\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1361\n-------------\nRow 986:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.00193\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1363\n-------------\nRow 987:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.12035\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1367\n-------------\nRow 988:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.10094\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1370\n-------------\nRow 989:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.09794\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1371\n-------------\nRow 990:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.09547\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1372\n-------------\nRow 991:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.09000\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1373\n-------------\nRow 992:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.07929\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1375\n-------------\nRow 993:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.06507\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1376\n-------------\nRow 994:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.02452\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1378\n-------------\nRow 995:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.01740\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1379\n-------------\nRow 996:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.01148\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1380\n-------------\nRow 997:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.16324\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1383\n-------------\nRow 998:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.14132\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1384\n-------------\nRow 999:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.11984\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1387\n-------------\nRow 1000:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.12438\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1388\n-------------\nRow 1001:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.03022\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1392\n-------------\nRow 1002:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.03284\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1393\n-------------\nRow 1003:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.02986\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1394\n-------------\nRow 1004:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.19867\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1395\n-------------\nRow 1005:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.19643\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1396\n-------------\nRow 1006:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.19467\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1397\n-------------\nRow 1007:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18453\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1398\n-------------\nRow 1008:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16037\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1399\n-------------\nRow 1009:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.07644\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1402\n-------------\nRow 1010:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.06813\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1403\n-------------\nRow 1011:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.01138\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1408\n-------------\nRow 1012:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.00042\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1410\n-------------\nRow 1013:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.05623\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1414\n-------------\nRow 1014:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.05899\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1415\n-------------\nRow 1015:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.05233\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1417\n-------------\nRow 1016:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.04106\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1420\n-------------\nRow 1017:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.03941\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1421\n-------------\nRow 1018:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.01053\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1425\n-------------\nRow 1019:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.17908\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1426\n-------------\nRow 1020:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.15770\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1427\n-------------\nRow 1021:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.15288\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1428\n-------------\nRow 1022:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.14845\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1429\n-------------\nRow 1023:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.14081\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1430\n-------------\nRow 1024:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.13430\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1432\n-------------\nRow 1025:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.12644\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1434\n-------------\nRow 1026:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.11477\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1437\n-------------\nRow 1027:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.11224\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1438\n-------------\nRow 1028:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.10610\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1439\n-------------\nRow 1029:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08452\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1441\n-------------\nRow 1030:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08440\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1442\n-------------\nRow 1031:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08216\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1446\n-------------\nRow 1032:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.06500\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1447\n-------------\nRow 1033:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.06371\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1448\n-------------\nRow 1034:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.06410\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1449\n-------------\nRow 1035:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.03758\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1453\n-------------\nRow 1036:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.02094\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1454\n-------------\nRow 1037:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.14696\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1455\n-------------\nRow 1038:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.08330\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1461\n-------------\nRow 1039:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.04802\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1462\n-------------\nRow 1040:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.10227\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1464\n-------------\nRow 1041:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.00409\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1465\n-------------\nRow 1042:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.08228\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1466\n-------------\nRow 1043:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.08034\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1468\n-------------\nRow 1044:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.07501\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1470\n-------------\nRow 1045:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.03250\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1472\n-------------\nRow 1046:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.13352\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1473\n-------------\nRow 1047:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.12737\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1474\n-------------\nRow 1048:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.08901\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1478\n-------------\nRow 1049:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.06146\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1480\n-------------\nRow 1050:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.03364\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1481\n-------------\nRow 1051:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.01323\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1482\n-------------\nRow 1052:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.00902\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1483\n-------------\nRow 1053:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.04133\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1487\n-------------\nRow 1054:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.12104\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1491\n-------------\nRow 1055:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.07162\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1492\n-------------\nRow 1056:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2207.12268\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1496\n-------------\nRow 1057:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.13393\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1500\n-------------\nRow 1058:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.13295\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1501\n-------------\nRow 1059:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.04306\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1506\n-------------\nRow 1060:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.19540\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1514\n-------------\nRow 1061:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.19512\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1515\n-------------\nRow 1062:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.19248\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1516\n-------------\nRow 1063:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.19784\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1517\n-------------\nRow 1064:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.19581\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1518\n-------------\nRow 1065:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.19415\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1519\n-------------\nRow 1066:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.18840\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1520\n-------------\nRow 1067:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.17569\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1521\n-------------\nRow 1068:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.17347\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1522\n-------------\nRow 1069:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.17189\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1523\n-------------\nRow 1070:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.16656\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1524\n-------------\nRow 1071:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.16825\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1525\n-------------\nRow 1072:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.16613\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1526\n-------------\nRow 1073:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.16400\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1527\n-------------\nRow 1074:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.16573\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1528\n-------------\nRow 1075:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.16003\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1529\n-------------\nRow 1076:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.15948\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1530\n-------------\nRow 1077:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.15169\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1531\n-------------\nRow 1078:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.15247\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1532\n-------------\nRow 1079:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.15111\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1533\n-------------\nRow 1080:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.14804\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1534\n-------------\nRow 1081:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.13828\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1535\n-------------\nRow 1082:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.13772\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1536\n-------------\nRow 1083:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.13268\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1537\n-------------\nRow 1084:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.13730\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1538\n-------------\nRow 1085:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.12678\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1539\n-------------\nRow 1086:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.13165\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1540\n-------------\nRow 1087:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.13119\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1541\n-------------\nRow 1088:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.12583\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1543\n-------------\nRow 1089:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.12190\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1544\n-------------\nRow 1090:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.11784\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1545\n-------------\nRow 1091:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.11778\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1546\n-------------\nRow 1092:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.10769\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1551\n-------------\nRow 1093:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.10338\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1552\n-------------\nRow 1094:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.10012\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1553\n-------------\nRow 1095:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.10639\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1554\n-------------\nRow 1096:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.10543\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1555\n-------------\nRow 1097:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.10644\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1556\n-------------\nRow 1098:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.10640\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1557\n-------------\nRow 1099:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.09711\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1558\n-------------\nRow 1100:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.09458\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1559\n-------------\nRow 1101:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.09336\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1560\n-------------\nRow 1102:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.09247\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1561\n-------------\nRow 1103:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.08949\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1562\n-------------\nRow 1104:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.08872\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1563\n-------------\nRow 1105:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.08785\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1564\n-------------\nRow 1106:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.08580\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1565\n-------------\nRow 1107:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.08579\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1566\n-------------\nRow 1108:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.08529\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1567\n-------------\nRow 1109:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.08465\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1568\n-------------\nRow 1110:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.07972\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1569\n-------------\nRow 1111:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.07771\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1570\n-------------\nRow 1112:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.07702\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1571\n-------------\nRow 1113:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.07697\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1572\n-------------\nRow 1114:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.07653\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1573\n-------------\nRow 1115:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.07419\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1574\n-------------\nRow 1116:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.07222\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1575\n-------------\nRow 1117:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.06968\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1576\n-------------\nRow 1118:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.06347\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1577\n-------------\nRow 1119:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.06311\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1578\n-------------\nRow 1120:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.05873\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1579\n-------------\nRow 1121:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.05922\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1580\n-------------\nRow 1122:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.05375\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1582\n-------------\nRow 1123:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.03937\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1583\n-------------\nRow 1124:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.03739\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1584\n-------------\nRow 1125:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.03602\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1585\n-------------\nRow 1126:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.03502\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1587\n-------------\nRow 1127:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.03363\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1588\n-------------\nRow 1128:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.02977\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1589\n-------------\nRow 1129:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.02906\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1590\n-------------\nRow 1130:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.02848\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1591\n-------------\nRow 1131:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.02712\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1592\n-------------\nRow 1132:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.02596\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1593\n-------------\nRow 1133:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.02426\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1594\n-------------\nRow 1134:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.02401\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1595\n-------------\nRow 1135:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.01819\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1596\n-------------\nRow 1136:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.01701\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1597\n-------------\nRow 1137:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.01506\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1599\n-------------\nRow 1138:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.00902\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1601\n-------------\nRow 1139:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.01107\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1602\n-------------\nRow 1140:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.00455\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1603\n-------------\nRow 1141:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.00434\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1604\n-------------\nRow 1142:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.00426\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1605\n-------------\nRow 1143:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.00390\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1606\n-------------\nRow 1144:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.17400\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1608\n-------------\nRow 1145:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.00031\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1609\n-------------\nRow 1146:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.17444\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1610\n-------------\nRow 1147:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.16608\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1611\n-------------\nRow 1148:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.16496\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1612\n-------------\nRow 1149:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.15818\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1613\n-------------\nRow 1150:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.15664\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1614\n-------------\nRow 1151:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.15508\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1615\n-------------\nRow 1152:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.15238\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1616\n-------------\nRow 1153:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.15103\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1617\n-------------\nRow 1154:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.14934\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1619\n-------------\nRow 1155:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.14859\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1620\n-------------\nRow 1156:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.14751\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1621\n-------------\nRow 1157:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.14494\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1622\n-------------\nRow 1158:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.14356\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1623\n-------------\nRow 1159:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.12792\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1626\n-------------\nRow 1160:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.11497\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1627\n-------------\nRow 1161:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.11140\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1628\n-------------\nRow 1162:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.10740\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1629\n-------------\nRow 1163:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.10556\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1630\n-------------\nRow 1164:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.09944\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1631\n-------------\nRow 1165:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.09553\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1632\n-------------\nRow 1166:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.09466\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1633\n-------------\nRow 1167:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.09294\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1634\n-------------\nRow 1168:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.08140\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1635\n-------------\nRow 1169:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.08030\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1636\n-------------\nRow 1170:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.07986\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1637\n-------------\nRow 1171:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.07944\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1638\n-------------\nRow 1172:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.07920\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1639\n-------------\nRow 1173:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.07509\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1640\n-------------\nRow 1174:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.07195\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1641\n-------------\nRow 1175:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.06933\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1642\n-------------\nRow 1176:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.06787\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1643\n-------------\nRow 1177:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.06380\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1644\n-------------\nRow 1178:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.06284\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1645\n-------------\nRow 1179:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.06135\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1646\n-------------\nRow 1180:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.05793\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1647\n-------------\nRow 1181:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.05534\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1648\n-------------\nRow 1182:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.05455\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1649\n-------------\nRow 1183:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.04907\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1650\n-------------\nRow 1184:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.04965\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1651\n-------------\nRow 1185:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.04917\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1652\n-------------\nRow 1186:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.04509\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1653\n-------------\nRow 1187:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.04430\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1654\n-------------\nRow 1188:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.04399\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1655\n-------------\nRow 1189:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.04372\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1656\n-------------\nRow 1190:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.03895\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1658\n-------------\nRow 1191:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.03869\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1659\n-------------\nRow 1192:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.03550\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1660\n-------------\nRow 1193:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.03549\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1661\n-------------\nRow 1194:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.03453\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1662\n-------------\nRow 1195:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.03031\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1663\n-------------\nRow 1196:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.02405\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1665\n-------------\nRow 1197:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.01728\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1666\n-------------\nRow 1198:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.01141\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1667\n-------------\nRow 1199:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.00952\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1668\n-------------\nRow 1200:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.00908\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1669\n-------------\nRow 1201:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.00613\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1670\n-------------\nRow 1202:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.00248\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1671\n-------------\nRow 1203:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.00398\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1673\n-------------\nRow 1204:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.16611\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1674\n-------------\nRow 1205:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.16582\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1675\n-------------\nRow 1206:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.16512\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1676\n-------------\nRow 1207:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.15692\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1677\n-------------\nRow 1208:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.15109\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1678\n-------------\nRow 1209:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.15016\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1679\n-------------\nRow 1210:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.14686\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1680\n-------------\nRow 1211:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.14480\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1681\n-------------\nRow 1212:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.14191\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1682\n-------------\nRow 1213:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.13812\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1683\n-------------\nRow 1214:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.13785\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1684\n-------------\nRow 1215:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.13879\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1685\n-------------\nRow 1216:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.13223\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1686\n-------------\nRow 1217:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.14761\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1687\n-------------\nRow 1218:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.12964\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1688\n-------------\nRow 1219:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.12605\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1689\n-------------\nRow 1220:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.12059\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1690\n-------------\nRow 1221:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.05934\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1691\n-------------\nRow 1222:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.11473\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1692\n-------------\nRow 1223:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.11206\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1693\n-------------\nRow 1224:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.11329\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1694\n-------------\nRow 1225:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.10899\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1695\n-------------\nRow 1226:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.10648\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1696\n-------------\nRow 1227:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.10718\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1697\n-------------\nRow 1228:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09991\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1698\n-------------\nRow 1229:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09306\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1699\n-------------\nRow 1230:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09278\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1700\n-------------\nRow 1231:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09716\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1701\n-------------\nRow 1232:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09705\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1702\n-------------\nRow 1233:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09599\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1703\n-------------\nRow 1234:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09710\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1704\n-------------\nRow 1235:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09592\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1705\n-------------\nRow 1236:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09091\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1706\n-------------\nRow 1237:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.08947\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1707\n-------------\nRow 1238:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.08157\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1708\n-------------\nRow 1239:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.08089\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1709\n-------------\nRow 1240:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.08316\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1710\n-------------\nRow 1241:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.07787\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1711\n-------------\nRow 1242:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.07605\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1712\n-------------\nRow 1243:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.07749\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1713\n-------------\nRow 1244:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.07151\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1714\n-------------\nRow 1245:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.07316\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1715\n-------------\nRow 1246:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.07428\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1716\n-------------\nRow 1247:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.06739\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1717\n-------------\nRow 1248:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.06721\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1718\n-------------\nRow 1249:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.06713\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1719\n-------------\nRow 1250:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.06571\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1720\n-------------\nRow 1251:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.06160\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1721\n-------------\nRow 1252:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.06038\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1722\n-------------\nRow 1253:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.06027\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1723\n-------------\nRow 1254:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.05995\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1724\n-------------\nRow 1255:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.05976\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1725\n-------------\nRow 1256:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.05184\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1726\n-------------\nRow 1257:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.05095\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1727\n-------------\nRow 1258:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.04288\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1728\n-------------\nRow 1259:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.04249\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1729\n-------------\nRow 1260:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.04265\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1730\n-------------\nRow 1261:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.03463\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1731\n-------------\nRow 1262:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.03610\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1732\n-------------\nRow 1263:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.03024\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1733\n-------------\nRow 1264:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.02874\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1734\n-------------\nRow 1265:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.02669\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1735\n-------------\nRow 1266:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.01655\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1736\n-------------\nRow 1267:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.01850\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1737\n-------------\nRow 1268:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.01472\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1738\n-------------\nRow 1269:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.02552\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1739\n-------------\nRow 1270:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.00906\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1740\n-------------\nRow 1271:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.00755\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1741\n-------------\nRow 1272:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.16489\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1742\n-------------\nRow 1273:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.16371\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1743\n-------------\nRow 1274:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.16579\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1745\n-------------\nRow 1275:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.16183\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1746\n-------------\nRow 1276:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.02510\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1747\n-------------\nRow 1277:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.14073\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1748\n-------------\nRow 1278:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.13908\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1749\n-------------\nRow 1279:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.14331\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1750\n-------------\nRow 1280:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.13720\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1751\n-------------\nRow 1281:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.13240\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1752\n-------------\nRow 1282:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.12868\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1753\n-------------\nRow 1283:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.00135\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1754\n-------------\nRow 1284:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.11410\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1755\n-------------\nRow 1285:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.10864\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1756\n-------------\nRow 1286:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.10711\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1757\n-------------\nRow 1287:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.10816\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1758\n-------------\nRow 1288:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.09781\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1759\n-------------\nRow 1289:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.10159\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1760\n-------------\nRow 1290:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.10373\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1761\n-------------\nRow 1291:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.08597\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1762\n-------------\nRow 1292:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.08448\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1763\n-------------\nRow 1293:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.07205\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1764\n-------------\nRow 1294:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.06949\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1765\n-------------\nRow 1295:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.10829\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1766\n-------------\nRow 1296:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.04725\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1767\n-------------\nRow 1297:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.04749\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1768\n-------------\nRow 1298:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.05564\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1769\n-------------\nRow 1299:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.04028\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1770\n-------------\nRow 1300:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.03108\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1771\n-------------\nRow 1301:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.01097\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1774\n-------------\nRow 1302:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.17567\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1775\n-------------\nRow 1303:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.17115\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1776\n-------------\nRow 1304:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.17154\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1777\n-------------\nRow 1305:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.17203\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1778\n-------------\nRow 1306:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.16894\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1779\n-------------\nRow 1307:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.14685\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1780\n-------------\nRow 1308:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.14544\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1781\n-------------\nRow 1309:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.14408\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1782\n-------------\nRow 1310:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.13754\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1783\n-------------\nRow 1311:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.12422\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1784\n-------------\nRow 1312:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.11504\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1785\n-------------\nRow 1313:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.11496\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1786\n-------------\nRow 1314:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.11300\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1787\n-------------\nRow 1315:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.10813\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1788\n-------------\nRow 1316:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.10804\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1789\n-------------\nRow 1317:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.10533\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1790\n-------------\nRow 1318:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.09869\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1791\n-------------\nRow 1319:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.13103\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1792\n-------------\nRow 1320:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.09635\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1793\n-------------\nRow 1321:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.10065\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1794\n-------------\nRow 1322:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.09417\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1795\n-------------\nRow 1323:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.08877\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1797\n-------------\nRow 1324:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.08966\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1798\n-------------\nRow 1325:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.08707\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1799\n-------------\nRow 1326:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.08687\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1800\n-------------\nRow 1327:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.08645\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1801\n-------------\nRow 1328:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.08251\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1802\n-------------\nRow 1329:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.08247\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1803\n-------------\nRow 1330:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.07954\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1804\n-------------\nRow 1331:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.07596\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1805\n-------------\nRow 1332:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.07280\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1806\n-------------\nRow 1333:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.07257\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1807\n-------------\nRow 1334:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.07154\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1808\n-------------\nRow 1335:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.06344\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1809\n-------------\nRow 1336:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.05544\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1810\n-------------\nRow 1337:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.05427\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1811\n-------------\nRow 1338:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.05178\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1812\n-------------\nRow 1339:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.05414\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1813\n-------------\nRow 1340:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.04744\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1814\n-------------\nRow 1341:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.04695\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1815\n-------------\nRow 1342:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.04632\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1816\n-------------\nRow 1343:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.04445\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1817\n-------------\nRow 1344:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.04607\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1818\n-------------\nRow 1345:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.02583\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1820\n-------------\nRow 1346:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.03258\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1821\n-------------\nRow 1347:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.03038\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1822\n-------------\nRow 1348:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.02903\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1823\n-------------\nRow 1349:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.02898\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1824\n-------------\nRow 1350:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.02717\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1825\n-------------\nRow 1351:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.02236\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1826\n-------------\nRow 1352:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.02018\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1827\n-------------\nRow 1353:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.05500\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1828\n-------------\nRow 1354:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.02083\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1829\n-------------\nRow 1355:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.01872\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1830\n-------------\nRow 1356:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.01732\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1831\n-------------\nRow 1357:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.01432\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1832\n-------------\nRow 1358:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.01322\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1833\n-------------\nRow 1359:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00984\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1834\n-------------\nRow 1360:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00986\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1835\n-------------\nRow 1361:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00983\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1836\n-------------\nRow 1362:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00974\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1837\n-------------\nRow 1363:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00973\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1838\n-------------\nRow 1364:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00971\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1839\n-------------\nRow 1365:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00966\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1840\n-------------\nRow 1366:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00964\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1841\n-------------\nRow 1367:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00943\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1842\n-------------\nRow 1368:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00926\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1843\n-------------\nRow 1369:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00637\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1844\n-------------\nRow 1370:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00813\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1845\n-------------\nRow 1371:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00800\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1846\n-------------\nRow 1372:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00219\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1847\n-------------\nRow 1373:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.20086\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1848\n-------------\nRow 1374:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.20082\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1849\n-------------\nRow 1375:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.19599\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1850\n-------------\nRow 1376:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.19195\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1852\n-------------\nRow 1377:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.19193\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1853\n-------------\nRow 1378:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.19066\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1854\n-------------\nRow 1379:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.19012\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1855\n-------------\nRow 1380:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18766\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1856\n-------------\nRow 1381:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18676\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1857\n-------------\nRow 1382:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18583\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1858\n-------------\nRow 1383:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18433\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1859\n-------------\nRow 1384:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18295\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1860\n-------------\nRow 1385:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18292\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1861\n-------------\nRow 1386:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18264\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1862\n-------------\nRow 1387:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18072\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1863\n-------------\nRow 1388:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18047\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1864\n-------------\nRow 1389:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18007\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1865\n-------------\nRow 1390:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.17489\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1866\n-------------\nRow 1391:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.17431\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1867\n-------------\nRow 1392:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.17423\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1868\n-------------\nRow 1393:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.17098\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1869\n-------------\nRow 1394:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16811\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1870\n-------------\nRow 1395:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16807\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1871\n-------------\nRow 1396:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16397\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1872\n-------------\nRow 1397:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16381\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1873\n-------------\nRow 1398:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16322\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1874\n-------------\nRow 1399:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16311\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1876\n-------------\nRow 1400:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16223\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1878\n-------------\nRow 1401:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16225\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1879\n-------------\nRow 1402:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16213\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1880\n-------------\nRow 1403:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.15798\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1881\n-------------\nRow 1404:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.15779\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1882\n-------------\nRow 1405:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.15296\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1883\n-------------\nRow 1406:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.14742\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1884\n-------------\nRow 1407:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.15194\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1885\n-------------\nRow 1408:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.14724\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1886\n-------------\nRow 1409:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.14720\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1887\n-------------\nRow 1410:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.14384\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1888\n-------------\nRow 1411:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.13921\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1889\n-------------\nRow 1412:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.13873\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1890\n-------------\nRow 1413:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.13840\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1891\n-------------\nRow 1414:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.13773\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1892\n-------------\nRow 1415:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.13655\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1893\n-------------\nRow 1416:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.13501\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1894\n-------------\nRow 1417:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.05523\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1895\n-------------\nRow 1418:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.13301\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1896\n-------------\nRow 1419:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.13308\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1897\n-------------\nRow 1420:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.13077\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1898\n-------------\nRow 1421:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.13050\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1899\n-------------\nRow 1422:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.12716\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1900\n-------------\nRow 1423:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.12328\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1901\n-------------\nRow 1424:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.12082\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1902\n-------------\nRow 1425:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.11520\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1903\n-------------\nRow 1426:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.11846\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1904\n-------------\nRow 1427:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.11588\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1905\n-------------\nRow 1428:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.11560\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1906\n-------------\nRow 1429:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.11540\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1907\n-------------\nRow 1430:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.10722\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1908\n-------------\nRow 1431:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.10701\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1909\n-------------\nRow 1432:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.10834\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1910\n-------------\nRow 1433:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.10855\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1911\n-------------\nRow 1434:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.10874\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1912\n-------------\nRow 1435:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.10853\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1913\n-------------\nRow 1436:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.10843\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1914\n-------------\nRow 1437:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.11080\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1915\n-------------\nRow 1438:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.10474\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1916\n-------------\nRow 1439:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.09381\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1917\n-------------\nRow 1440:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.11067\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1918\n-------------\nRow 1441:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.09662\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1919\n-------------\nRow 1442:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.08850\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1920\n-------------\nRow 1443:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.08891\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1921\n-------------\nRow 1444:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.05182\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1922\n-------------\nRow 1445:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.05947\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1924\n-------------\nRow 1446:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.05189\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1925\n-------------\nRow 1447:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.05464\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1926\n-------------\nRow 1448:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.04919\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1927\n-------------\nRow 1449:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.04497\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1928\n-------------\nRow 1450:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.04441\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1930\n-------------\nRow 1451:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.04175\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1931\n-------------\nRow 1452:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.04001\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1932\n-------------\nRow 1453:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.03610\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1933\n-------------\nRow 1454:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.03374\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1934\n-------------\nRow 1455:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.03382\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1935\n-------------\nRow 1456:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.03509\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1936\n-------------\nRow 1457:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.02594\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1937\n-------------\nRow 1458:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.01855\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1938\n-------------\nRow 1459:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.01115\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1939\n-------------\nRow 1460:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.14573\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1940\n-------------\nRow 1461:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.14530\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1941\n-------------\nRow 1462:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.14006\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1942\n-------------\nRow 1463:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.13427\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1943\n-------------\nRow 1464:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.12439\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1944\n-------------\nRow 1465:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.10182\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1945\n-------------\nRow 1466:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.10261\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1946\n-------------\nRow 1467:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.08870\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1947\n-------------\nRow 1468:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.08821\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1948\n-------------\nRow 1469:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.08818\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1949\n-------------\nRow 1470:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.08483\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1950\n-------------\nRow 1471:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.08477\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1951\n-------------\nRow 1472:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.08465\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1952\n-------------\nRow 1473:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.07410\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1953\n-------------\nRow 1474:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.07090\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1954\n-------------\nRow 1475:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.06720\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1955\n-------------\nRow 1476:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.06818\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1956\n-------------\nRow 1477:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.05568\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1957\n-------------\nRow 1478:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.06140\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1958\n-------------\nRow 1479:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.06027\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1959\n-------------\nRow 1480:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.05390\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1960\n-------------\nRow 1481:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.04968\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1961\n-------------\nRow 1482:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.04344\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1962\n-------------\nRow 1483:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.04269\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1963\n-------------\nRow 1484:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.03869\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1964\n-------------\nRow 1485:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.03119\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1965\n-------------\nRow 1486:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.03373\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1966\n-------------\nRow 1487:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.02963\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1967\n-------------\nRow 1488:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.02827\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1968\n-------------\nRow 1489:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.02642\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1969\n-------------\nRow 1490:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.02192\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1970\n-------------\nRow 1491:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.01919\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1971\n-------------\nRow 1492:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.02051\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1972\n-------------\nRow 1493:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.01900\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1973\n-------------\nRow 1494:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.01515\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1974\n-------------\nRow 1495:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.01116\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1975\n-------------\nRow 1496:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.00916\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1976\n-------------\nRow 1497:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.03117\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1977\n-------------\nRow 1498:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.17870\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1978\n-------------\nRow 1499:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.17606\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1979\n-------------\nRow 1500:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.17546\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1980\n-------------\nRow 1501:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.06034\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1981\n-------------\nRow 1502:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.17591\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1982\n-------------\nRow 1503:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.17599\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1984\n-------------\nRow 1504:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.17155\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1985\n-------------\nRow 1505:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.17550\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1986\n-------------\nRow 1506:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.17189\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1987\n-------------\nRow 1507:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.16611\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1988\n-------------\nRow 1508:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.16765\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1989\n-------------\nRow 1509:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.15780\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1990\n-------------\nRow 1510:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.15649\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1991\n-------------\nRow 1511:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.14897\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1992\n-------------\nRow 1512:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.15413\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1993\n-------------\nRow 1513:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.15433\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1994\n-------------\nRow 1514:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.14613\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1995\n-------------\nRow 1515:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.14420\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1996\n-------------\nRow 1516:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.15181\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1997\n-------------\nRow 1517:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.14207\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1998\n-------------\nRow 1518:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.13843\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1999\n-------------\nRow 1519:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.13873\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2000\n-------------\nRow 1520:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.13495\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2001\n-------------\nRow 1521:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.13516\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2002\n-------------\nRow 1522:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.13439\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2003\n-------------\nRow 1523:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.13126\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2004\n-------------\nRow 1524:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.12688\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2005\n-------------\nRow 1525:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.12789\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2006\n-------------\nRow 1526:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.12236\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2007\n-------------\nRow 1527:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.12048\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2008\n-------------\nRow 1528:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.11916\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2009\n-------------\nRow 1529:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.11938\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2010\n-------------\nRow 1530:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.11396\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2011\n-------------\nRow 1531:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.11306\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2012\n-------------\nRow 1532:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.11305\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2013\n-------------\nRow 1533:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.11073\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2014\n-------------\nRow 1534:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.10735\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2015\n-------------\nRow 1535:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.10073\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2016\n-------------\nRow 1536:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.10056\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2017\n-------------\nRow 1537:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.09867\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2018\n-------------\nRow 1538:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.09833\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2019\n-------------\nRow 1539:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.09319\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2020\n-------------\nRow 1540:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.09535\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2021\n-------------\nRow 1541:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.09618\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2022\n-------------\nRow 1542:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.09522\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2023\n-------------\nRow 1543:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08767\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2024\n-------------\nRow 1544:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.11444\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2025\n-------------\nRow 1545:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08622\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2026\n-------------\nRow 1546:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.07945\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2027\n-------------\nRow 1547:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08084\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2028\n-------------\nRow 1548:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.07937\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2029\n-------------\nRow 1549:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.04671\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2030\n-------------\nRow 1550:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.04761\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2031\n-------------\nRow 1551:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.07345\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2032\n-------------\nRow 1552:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.06555\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2033\n-------------\nRow 1553:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.05125\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2034\n-------------\nRow 1554:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.04587\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2035\n-------------\nRow 1555:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.03751\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2036\n-------------\nRow 1556:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.02153\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2037\n-------------\nRow 1557:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.00262\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2038\n-------------\nRow 1558:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.14368\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2039\n-------------\nRow 1559:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.13153\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2040\n-------------\nRow 1560:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.12764\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2041\n-------------\nRow 1561:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.11797\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2042\n-------------\nRow 1562:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.11710\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2043\n-------------\nRow 1563:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.09301\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2046\n-------------\nRow 1564:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.08510\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2047\n-------------\nRow 1565:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.08453\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2048\n-------------\nRow 1566:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.08113\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2049\n-------------\nRow 1567:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.08357\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2050\n-------------\nRow 1568:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.07865\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2051\n-------------\nRow 1569:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.07979\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2052\n-------------\nRow 1570:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.06883\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2053\n-------------\nRow 1571:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.07121\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2054\n-------------\nRow 1572:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.10305\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2056\n-------------\nRow 1573:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.04841\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2057\n-------------\nRow 1574:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.04222\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2059\n-------------\nRow 1575:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.03900\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2060\n-------------\nRow 1576:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.10893\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2061\n-------------\nRow 1577:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.03668\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2062\n-------------\nRow 1578:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.03027\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2063\n-------------\nRow 1579:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.03011\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2064\n-------------\nRow 1580:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.02412\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2065\n-------------\nRow 1581:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.02285\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2066\n-------------\nRow 1582:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.02394\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2067\n-------------\nRow 1583:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.02070\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2068\n-------------\nRow 1584:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.01721\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2069\n-------------\nRow 1585:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.01329\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2070\n-------------\nRow 1586:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.00561\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2071\n-------------\nRow 1587:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.13826\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2072\n-------------\nRow 1588:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.13591\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2073\n-------------\nRow 1589:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.13173\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2074\n-------------\nRow 1590:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.12914\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2075\n-------------\nRow 1591:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.12959\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2076\n-------------\nRow 1592:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.12247\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2077\n-------------\nRow 1593:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.12073\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2078\n-------------\nRow 1594:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.11280\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2079\n-------------\nRow 1595:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.05221\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2080\n-------------\nRow 1596:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.04474\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2081\n-------------\nRow 1597:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.02777\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2082\n-------------\nRow 1598:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.03786\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2083\n-------------\nRow 1599:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.03396\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2085\n-------------\nRow 1600:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.00704\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2086\n-------------\nRow 1601:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.14704\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2087\n-------------\nRow 1602:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.11261\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2090\n-------------\nRow 1603:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.09611\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2091\n-------------\nRow 1604:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.08698\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2092\n-------------\nRow 1605:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.07839\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2093\n-------------\nRow 1606:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.07476\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2094\n-------------\nRow 1607:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.06858\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2095\n-------------\nRow 1608:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.06909\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2096\n-------------\nRow 1609:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.06013\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2097\n-------------\nRow 1610:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.05034\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2098\n-------------\nRow 1611:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.05032\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2099\n-------------\nRow 1612:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.04495\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2100\n-------------\nRow 1613:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.04493\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2101\n-------------\nRow 1614:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.04489\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2102\n-------------\nRow 1615:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.04488\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2103\n-------------\nRow 1616:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.04473\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2104\n-------------\nRow 1617:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.04048\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2105\n-------------\nRow 1618:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.04248\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2106\n-------------\nRow 1619:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.03741\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2107\n-------------\nRow 1620:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.03507\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2108\n-------------\nRow 1621:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.03267\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2109\n-------------\nRow 1622:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.03099\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2110\n-------------\nRow 1623:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.03293\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2111\n-------------\nRow 1624:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.02936\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2113\n-------------\nRow 1625:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.00793\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2115\n-------------\nRow 1626:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.00210\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2116\n-------------\nRow 1627:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.16582\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2117\n-------------\nRow 1628:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.16374\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2118\n-------------\nRow 1629:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.15076\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2119\n-------------\nRow 1630:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.14842\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2120\n-------------\nRow 1631:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.14108\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2121\n-------------\nRow 1632:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.14305\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2122\n-------------\nRow 1633:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.15388\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2124\n-------------\nRow 1634:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.13319\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2125\n-------------\nRow 1635:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.13095\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2126\n-------------\nRow 1636:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.12446\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2127\n-------------\nRow 1637:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.12572\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2128\n-------------\nRow 1638:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.12112\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2129\n-------------\nRow 1639:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.11694\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2132\n-------------\nRow 1640:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.15462\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2133\n-------------\nRow 1641:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.11319\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2134\n-------------\nRow 1642:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.10950\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2135\n-------------\nRow 1643:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.10682\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2136\n-------------\nRow 1644:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.10440\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2137\n-------------\nRow 1645:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.10370\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2138\n-------------\nRow 1646:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.09794\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2139\n-------------\nRow 1647:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.07751\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2143\n-------------\nRow 1648:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.05105\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2144\n-------------\nRow 1649:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.02408\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2145\n-------------\nRow 1650:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.16031\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2148\n-------------\nRow 1651:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.15230\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2149\n-------------\nRow 1652:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.15257\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2150\n-------------\nRow 1653:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.14124\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2152\n-------------\nRow 1654:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.12965\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2153\n-------------\nRow 1655:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.12192\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2154\n-------------\nRow 1656:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.12565\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2155\n-------------\nRow 1657:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.11427\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2156\n-------------\nRow 1658:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.10960\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2157\n-------------\nRow 1659:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.09477\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2158\n-------------\nRow 1660:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.09549\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2159\n-------------\nRow 1661:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.09276\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2160\n-------------\nRow 1662:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.05872\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2161\n-------------\nRow 1663:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.02303\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2163\n-------------\nRow 1664:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.02438\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2164\n-------------\nRow 1665:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.02249\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2165\n-------------\nRow 1666:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.02347\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2166\n-------------\nRow 1667:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.00968\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2167\n-------------\nRow 1668:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.14792\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2168\n-------------\nRow 1669:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.14491\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2170\n-------------\nRow 1670:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.14697\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2171\n-------------\nRow 1671:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.13360\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2172\n-------------\nRow 1672:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.12330\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2173\n-------------\nRow 1673:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.11711\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2174\n-------------\nRow 1674:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.08891\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2175\n-------------\nRow 1675:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.06970\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2176\n-------------\nRow 1676:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.04145\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2177\n-------------\nRow 1677:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.12242\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2178\n-------------\nRow 1678:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2207.13038\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2179\n-------------\nRow 1679:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.02779\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2181\n-------------\nRow 1680:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.01714\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2182\n-------------\nRow 1681:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.00386\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2183\n-------------\nRow 1682:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.16007\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2184\n-------------\nRow 1683:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2204.02849\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2189\n-------------\nRow 1684:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2111.14822\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2192\n-------------\nRow 1685:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2111.14818\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2193\n-------------\nRow 1686:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2110.02711\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2195\n-------------\nRow 1687:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.18986\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2197\n-------------\nRow 1688:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.17359\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2198\n-------------\nRow 1689:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.17649\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2199\n-------------\nRow 1690:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.16818\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2200\n-------------\nRow 1691:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.16167\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2202\n-------------\nRow 1692:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.15008\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2203\n-------------\nRow 1693:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.14729\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2204\n-------------\nRow 1694:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.13157\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2207\n-------------\nRow 1695:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.12474\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2209\n-------------\nRow 1696:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.11106\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2211\n-------------\nRow 1697:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.10624\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2212\n-------------\nRow 1698:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.10343\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2213\n-------------\nRow 1699:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.08092\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2216\n-------------\nRow 1700:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.06836\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2217\n-------------\nRow 1701:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.06744\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2218\n-------------\nRow 1702:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.04561\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2220\n-------------\nRow 1703:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.03420\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2222\n-------------\nRow 1704:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.03020\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2223\n-------------\nRow 1705:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.03015\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2224\n-------------\nRow 1706:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.02601\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2227\n-------------\nRow 1707:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.02242\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2229\n-------------\nRow 1708:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.01406\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2230\n-------------\nRow 1709:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.00362\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2231\n-------------\nRow 1710:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.00049\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2232\n-------------\nRow 1711:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.17261\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2233\n-------------\nRow 1712:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.16237\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2234\n-------------\nRow 1713:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.14872\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2235\n-------------\nRow 1714:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.11525\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2236\n-------------\nRow 1715:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.11601\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2237\n-------------\nRow 1716:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.11306\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2238\n-------------\nRow 1717:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.11258\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2239\n-------------\nRow 1718:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.10911\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2240\n-------------\nRow 1719:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.08289\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2241\n-------------\nRow 1720:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.08273\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2242\n-------------\nRow 1721:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.07973\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2243\n-------------\nRow 1722:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.07051\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2245\n-------------\nRow 1723:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.03335\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2248\n-------------\nRow 1724:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.01372\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2250\n-------------\nRow 1725:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.00158\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2251\n-------------\nRow 1726:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.16905\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2254\n-------------\nRow 1727:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.14244\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2256\n-------------\nRow 1728:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.14152\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2257\n-------------\nRow 1729:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.14078\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2258\n-------------\nRow 1730:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.14035\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2259\n-------------\nRow 1731:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.13369\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2261\n-------------\nRow 1732:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.11945\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2264\n-------------\nRow 1733:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09910\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2267\n-------------\nRow 1734:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.10705\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2268\n-------------\nRow 1735:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09591\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2270\n-------------\nRow 1736:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09523\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2271\n-------------\nRow 1737:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09678\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2272\n-------------\nRow 1738:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09712\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2275\n-------------\nRow 1739:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.08545\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2277\n-------------\nRow 1740:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.07837\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2278\n-------------\nRow 1741:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.04468\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2280\n-------------\nRow 1742:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.02963\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2283\n-------------\nRow 1743:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.02915\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2284\n-------------\nRow 1744:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.02000\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2286\n-------------\nRow 1745:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.16106\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2289\n-------------\nRow 1746:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.15042\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2290\n-------------\nRow 1747:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.13639\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2292\n-------------\nRow 1748:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.07511\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2293\n-------------\nRow 1749:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.06526\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2294\n-------------\nRow 1750:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.04859\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2295\n-------------\nRow 1751:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.03833\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2296\n-------------\nRow 1752:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.05445\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2297\n-------------\nRow 1753:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.02270\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2298\n-------------\nRow 1754:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.01831\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2299\n-------------\nRow 1755:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.17843\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2300\n-------------\nRow 1756:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.16329\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2302\n-------------\nRow 1757:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.09864\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2306\n-------------\nRow 1758:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.09551\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2307\n-------------\nRow 1759:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.08103\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2308\n-------------\nRow 1760:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.07881\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2309\n-------------\nRow 1761:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.07473\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2310\n-------------\nRow 1762:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.05668\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2312\n-------------\nRow 1763:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.05421\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2313\n-------------\nRow 1764:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.04619\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2314\n-------------\nRow 1765:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.04411\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2315\n-------------\nRow 1766:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00547\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2316\n-------------\nRow 1767:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00519\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2317\n-------------\nRow 1768:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00416\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2318\n-------------\nRow 1769:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.00783\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2319\n-------------\nRow 1770:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.20089\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2320\n-------------\nRow 1771:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16411\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2324\n-------------\nRow 1772:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16315\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2325\n-------------\nRow 1773:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.16283\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2326\n-------------\nRow 1774:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.15873\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2329\n-------------\nRow 1775:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.15171\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2330\n-------------\nRow 1776:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.15586\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2331\n-------------\nRow 1777:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.15399\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2332\n-------------\nRow 1778:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.13705\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2334\n-------------\nRow 1779:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.12577\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2335\n-------------\nRow 1780:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.12554\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2336\n-------------\nRow 1781:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.11664\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2337\n-------------\nRow 1782:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.11870\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2338\n-------------\nRow 1783:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.11337\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2340\n-------------\nRow 1784:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.09641\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2343\n-------------\nRow 1785:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.05901\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2345\n-------------\nRow 1786:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.06077\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2346\n-------------\nRow 1787:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.04847\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2347\n-------------\nRow 1788:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.04461\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2348\n-------------\nRow 1789:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.01921\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2349\n-------------\nRow 1790:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.02463\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2350\n-------------\nRow 1791:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.01618\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2351\n-------------\nRow 1792:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.01257\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2352\n-------------\nRow 1793:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.14473\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2353\n-------------\nRow 1794:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.10532\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2355\n-------------\nRow 1795:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.10535\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2356\n-------------\nRow 1796:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.08577\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2358\n-------------\nRow 1797:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.07132\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2359\n-------------\nRow 1798:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.06700\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2360\n-------------\nRow 1799:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.06714\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2361\n-------------\nRow 1800:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.06024\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2362\n-------------\nRow 1801:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.05684\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2363\n-------------\nRow 1802:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.04133\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2365\n-------------\nRow 1803:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.02602\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2367\n-------------\nRow 1804:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.01893\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2368\n-------------\nRow 1805:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.04681\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2371\n-------------\nRow 1806:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.16509\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2375\n-------------\nRow 1807:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.15880\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2378\n-------------\nRow 1808:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.14184\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2380\n-------------\nRow 1809:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.13397\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2384\n-------------\nRow 1810:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.12786\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2386\n-------------\nRow 1811:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.12218\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2388\n-------------\nRow 1812:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.11579\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2389\n-------------\nRow 1813:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.12538\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2391\n-------------\nRow 1814:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.11686\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2393\n-------------\nRow 1815:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.11328\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2395\n-------------\nRow 1816:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.10406\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2397\n-------------\nRow 1817:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.09119\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2398\n-------------\nRow 1818:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.09541\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2399\n-------------\nRow 1819:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.09375\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2400\n-------------\nRow 1820:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.07938\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2402\n-------------\nRow 1821:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08133\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2403\n-------------\nRow 1822:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08061\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2404\n-------------\nRow 1823:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.05916\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2406\n-------------\nRow 1824:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.05371\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2407\n-------------\nRow 1825:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.01418\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2408\n-------------\nRow 1826:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.14503\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2409\n-------------\nRow 1827:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.12231\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2410\n-------------\nRow 1828:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.10668\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2411\n-------------\nRow 1829:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.10109\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2412\n-------------\nRow 1830:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.05905\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2413\n-------------\nRow 1831:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.05573\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2414\n-------------\nRow 1832:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.03665\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2415\n-------------\nRow 1833:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.00190\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2418\n-------------\nRow 1834:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.11445\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2419\n-------------\nRow 1835:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.10047\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2420\n-------------\nRow 1836:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.10134\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2421\n-------------\nRow 1837:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.06015\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2422\n-------------\nRow 1838:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.03949\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2423\n-------------\nRow 1839:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.00527\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2424\n-------------\nRow 1840:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.08751\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2426\n-------------\nRow 1841:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.09315\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2427\n-------------\nRow 1842:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.08526\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2428\n-------------\nRow 1843:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.06135\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2429\n-------------\nRow 1844:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.05993\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2430\n-------------\nRow 1845:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.04636\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2431\n-------------\nRow 1846:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.02837\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2438\n-------------\nRow 1847:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.02796\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2439\n-------------\nRow 1848:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.02500\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2440\n-------------\nRow 1849:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.01747\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2441\n-------------\nRow 1850:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.01206\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2442\n-------------\nRow 1851:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.00842\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2443\n-------------\nRow 1852:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.00774\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2444\n-------------\nRow 1853:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.00792\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2445\n-------------\nRow 1854:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.16677\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2446\n-------------\nRow 1855:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.16940\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2447\n-------------\nRow 1856:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.16487\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2448\n-------------\nRow 1857:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.16431\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2450\n-------------\nRow 1858:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.16247\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2451\n-------------\nRow 1859:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.16016\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2452\n-------------\nRow 1860:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.13757\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2454\n-------------\nRow 1861:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.13220\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2455\n-------------\nRow 1862:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.10865\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2456\n-------------\nRow 1863:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.09707\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2457\n-------------\nRow 1864:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.09869\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2458\n-------------\nRow 1865:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.07600\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2459\n-------------\nRow 1866:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.04753\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2460\n-------------\nRow 1867:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.04604\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2461\n-------------\nRow 1868:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.12315\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2462\n-------------\nRow 1869:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.06978\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2463\n-------------\nRow 1870:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.05976\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2464\n-------------\nRow 1871:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.05669\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2465\n-------------\nRow 1872:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.04628\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2466\n-------------\nRow 1873:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.03158\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2467\n-------------\nRow 1874:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.14828\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2468\n-------------\nRow 1875:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.14916\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2469\n-------------\nRow 1876:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.03855\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2471\n-------------\nRow 1877:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.01170\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2472\n-------------\nRow 1878:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.00349\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2473\n-------------\nRow 1879:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.15001\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2475\n-------------\nRow 1880:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.14125\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2476\n-------------\nRow 1881:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.09801\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2477\n-------------\nRow 1882:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.03530\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2478\n-------------\nRow 1883:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2107.10981\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2479\n-------------\nRow 1884:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2207.08000\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2480\n-------------\nRow 1885:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.19410\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2483\n-------------\nRow 1886:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.18936\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2484\n-------------\nRow 1887:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.18762\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2485\n-------------\nRow 1888:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.18677\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2486\n-------------\nRow 1889:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.10461\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2487\n-------------\nRow 1890:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.07492\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2488\n-------------\nRow 1891:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.07084\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2489\n-------------\nRow 1892:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.04687\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2490\n-------------\nRow 1893:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.07398\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2491\n-------------\nRow 1894:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.05940\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2492\n-------------\nRow 1895:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.05330\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2493\n-------------\nRow 1896:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.03702\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2494\n-------------\nRow 1897:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.03198\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2495\n-------------\nRow 1898:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.01582\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2496\n-------------\nRow 1899:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.14333\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2498\n-------------\nRow 1900:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.12143\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2499\n-------------\nRow 1901:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.06405\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2500\n-------------\nRow 1902:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2311.09355\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2502\n-------------\nRow 1903:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2010.08258\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 8\nversions: 6\nid: 866\n-------------\nRow 1904:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2101.03288\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 202\nversions: 3\nid: 481\n-------------\nRow 1905:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2009.05475\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 105\nversions: 3\nid: 868\n-------------\nRow 1906:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2012.08125\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 101\nversions: 4\nid: 864\n-------------\nRow 1907:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2006.11239\narxiv_title: Denoising Diffusion Probabilistic Models\narxiv_abstract: We present high quality image synthesis results using diffus...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Denoising_Diffusion_Probabilistic_Models.pdf\narxiv_paper_markdown: None\ncitations: 7275\nversions: 9\nid: 420\n-------------\nRow 1908:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2004.05582\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 25\nversions: 7\nid: 471\n-------------\nRow 1909:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2104.02600\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 76\nversions: 2\nid: 860\n-------------\nRow 1910:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1905.09883\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 174\nversions: 2\nid: 872\n-------------\nRow 1911:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2009.08348\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 3\nversions: 3\nid: 474\n-------------\nRow 1912:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2104.05358\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 108\nversions: 3\nid: 1075\n-------------\nRow 1913:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2101.02388\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 131\nversions: 2\nid: 863\n-------------\nRow 1914:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2010.02502\narxiv_title: Denoising Diffusion Implicit Models\narxiv_abstract: Denoising diffusion probabilistic models (DDPMs) have achiev...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Denoising_Diffusion_Implicit_Models.pdf\narxiv_paper_markdown: None\ncitations: 2685\nversions: 4\nid: 867\n-------------\nRow 1915:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.05444\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 18\nversions: 3\nid: 4610\n-------------\nRow 1916:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.05638\narxiv_title: Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheap...\narxiv_abstract: Few-shot in-context learning (ICL) enables pre-trained langu...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Few-Shot_Parameter-Efficient_Fine-Tuning_is_Better_and_Cheap...\narxiv_paper_markdown: None\ncitations: 350\nversions: 7\nid: 4611\n-------------\nRow 1917:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2104.07636\narxiv_title: Image Super-Resolution via Iterative Refinement\narxiv_abstract: We present SR3, an approach to image Super-Resolution via Re...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Image_Super-Resolution_via_Iterative_Refinement.pdf\narxiv_paper_markdown: None\ncitations: 938\nversions: 12\nid: 859\n-------------\nRow 1918:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2105.05233\narxiv_title: Diffusion Models Beat GANs on Image Synthesis\narxiv_abstract: We show that diffusion models can achieve image sample quali...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Diffusion_Models_Beat_GANs_on_Image_Synthesis.pdf\narxiv_paper_markdown: None\ncitations: 3592\nversions: 8\nid: 422\n-------------\nRow 1919:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2101.09258\narxiv_title: Maximum Likelihood Training of Score-Based Diffusion Models\narxiv_abstract: Score-based diffusion models synthesize samples by reversing...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Maximum_Likelihood_Training_of_Score-Based_Diffusion_Models....\narxiv_paper_markdown: None\ncitations: 351\nversions: 9\nid: 862\n-------------\nRow 1920:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2103.01458\narxiv_title: Diffusion Probabilistic Models for 3D Point Cloud Generation\narxiv_abstract: We present a probabilistic model for point cloud generation,...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Diffusion_Probabilistic_Models_for_3D_Point_Cloud_Generation...\narxiv_paper_markdown: None\ncitations: 420\nversions: 8\nid: 2482\n-------------\nRow 1921:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1503.03585\narxiv_title: Deep Unsupervised Learning using Nonequilibrium Thermodynami...\narxiv_abstract: A central problem in machine learning involves modeling comp...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Deep_Unsupervised_Learning_using_Nonequilibrium_Thermodynami...\narxiv_paper_markdown: None\ncitations: 3538\nversions: 6\nid: 419\n-------------\nRow 1922:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2102.09672\narxiv_title: Improved Denoising Diffusion Probabilistic Models\narxiv_abstract: Denoising diffusion probabilistic models (DDPM) are a class ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Improved_Denoising_Diffusion_Probabilistic_Models.pdf\narxiv_paper_markdown: None\ncitations: 1762\nversions: 6\nid: 861\n-------------\nRow 1923:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2107.06994\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4589\n-------------\nRow 1924:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1606.08415\narxiv_title: Gaussian Error Linear Units (GELUs)\narxiv_abstract: We propose the Gaussian Error Linear Unit (GELU), a high-per...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Gaussian_Error_Linear_Units_(GELUs).pdf\narxiv_paper_markdown: None\ncitations: 4653\nversions: 4\nid: 4595\n-------------\nRow 1925:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2311.07509\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 2\nversions: 2\nid: 4617\n-------------\nRow 1926:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2010.02650\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4642\n-------------\nRow 1927:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2104.03670\narxiv_title: 3D Shape Generation and Completion through Point-Voxel Diffu...\narxiv_abstract: We propose a novel approach for probabilistic generative mod...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: 3D_Shape_Generation_and_Completion_through_Point-Voxel_Diffu...\narxiv_paper_markdown: None\ncitations: 311\nversions: 6\nid: 2481\n-------------\nRow 1928:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08774\narxiv_title: GPT-4 Technical Report\narxiv_abstract: We report the development of GPT-4, a large-scale, multimoda...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: GPT-4_Technical_Report.pdf\narxiv_paper_markdown: None\ncitations: 349\nversions: 2\nid: 7\n-------------\nRow 1929:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2004.11714\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4649\n-------------\nRow 1930:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.08291\narxiv_title: Large Language Model Guided Tree-of-Thought\narxiv_abstract: In this paper, we introduce the Tree-of-Thought (ToT) framew...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Large_Language_Model_Guided_Tree-of-Thought.pdf\narxiv_paper_markdown: None\ncitations: 58\nversions: 3\nid: 4625\n-------------\nRow 1931:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.11916\narxiv_title: Large Language Models are Zero-Shot Reasoners\narxiv_abstract: Pretrained large language models (LLMs) are widely used in m...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Large_Language_Models_are_Zero-Shot_Reasoners.pdf\narxiv_paper_markdown: None\ncitations: 1609\nversions: 7\nid: 4619\n-------------\nRow 1932:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.11171\narxiv_title: Self-Consistency Improves Chain of Thought Reasoning in Lang...\narxiv_abstract: Chain-of-thought prompting combined with pre-trained large l...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Self-Consistency_Improves_Chain_of_Thought_Reasoning_in_Lang...\narxiv_paper_markdown: None\ncitations: 516\nversions: 5\nid: 4620\n-------------\nRow 1933:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.03401\narxiv_title: The Unreliability of Explanations in Few-shot Prompting for ...\narxiv_abstract: Does prompting a large language model (LLM) like GPT-3 with ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: The_Unreliability_of_Explanations_in_Few-shot_Prompting_for_...\narxiv_paper_markdown: None\ncitations: 81\nversions: 6\nid: 4623\n-------------\nRow 1934:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2010.15980\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4652\n-------------\nRow 1935:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2101.00190\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4659\n-------------\nRow 1936:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2103.10385\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4662\n-------------\nRow 1937:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2104.08691\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4663\n-------------\nRow 1938:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2009.01325\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4692\n-------------\nRow 1939:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1609.06647\narxiv_title: Show and Tell: Lessons learned from the 2015 MSCOCO Image Ca...\narxiv_abstract: Automatically describing the content of an image is a fundam...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Show_and_Tell_Lessons_learned_from_the_2015_MSCOCO_Image_Cap...\narxiv_paper_markdown: None\ncitations: 1043\nversions: 21\nid: 4831\n-------------\nRow 1940:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2003.11596\narxiv_title: Learning Multi-Scale Photo Exposure Correction\narxiv_abstract: Capturing photographs with wrong exposures remains a major s...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Learning_Multi-Scale_Photo_Exposure_Correction.pdf\narxiv_paper_markdown: None\ncitations: 142\nversions: 9\nid: 478\n-------------\nRow 1941:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2103.17185\narxiv_title: Rethinking Style Transfer: From Pixels to Parameterized Brus...\narxiv_abstract: There have been many successful implementations of neural st...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Rethinking_Style_Transfer_From_Pixels_to_Parameterized_Brush...\narxiv_paper_markdown: None\ncitations: 55\nversions: 7\nid: 476\n-------------\nRow 1942:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2010.05906\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4704\n-------------\nRow 1943:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1912.13503\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4705\n-------------\nRow 1944:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2009.06367\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4711\n-------------\nRow 1945:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2012.11635\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4713\n-------------\nRow 1946:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1912.08517\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 22\nversions: 3\nid: 4714\n-------------\nRow 1947:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.15217\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4900\n-------------\nRow 1948:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2006.16823\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 11\nversions: 2\nid: 4708\n-------------\nRow 1949:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1709.07871\narxiv_title: FiLM: Visual Reasoning with a General Conditioning Layer\narxiv_abstract: We introduce a general-purpose conditioning method for neura...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: FiLM_Visual_Reasoning_with_a_General_Conditioning_Layer.pdf\narxiv_paper_markdown: None\ncitations: 1713\nversions: 14\nid: 4707\n-------------\nRow 1950:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2312.05934\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4902\n-------------\nRow 1951:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2306.01116\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4905\n-------------\nRow 1952:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.03109\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4906\n-------------\nRow 1953:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.01558\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4912\n-------------\nRow 1954:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2204.04991\narxiv_title: TRUE: Re-evaluating Factual Consistency Evaluation\narxiv_abstract: Grounded text generation systems often generate text that co...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: TRUE_Re-evaluating_Factual_Consistency_Evaluation.pdf\narxiv_paper_markdown: None\ncitations: 102\nversions: 10\nid: 4907\n-------------\nRow 1955:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2105.04551\narxiv_title: Stochastic Image-to-Video Synthesis using cINNs\narxiv_abstract: Video understanding calls for a model to learn the character...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Stochastic_Image-to-Video_Synthesis_using_cINNs.pdf\narxiv_paper_markdown: None\ncitations: 42\nversions: 11\nid: 475\n-------------\nRow 1956:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2003.11113\narxiv_title: PADS: Policy-Adapted Sampling for Visual Similarity Learning\narxiv_abstract: Learning visual similarity requires to learn relations, typi...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: PADS_Policy-Adapted_Sampling_for_Visual_Similarity_Learning....\narxiv_paper_markdown: None\ncitations: 44\nversions: 10\nid: 479\n-------------\nRow 1957:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2104.14951\narxiv_title: SRDiff: Single Image Super-Resolution with Diffusion Probabi...\narxiv_abstract: Single image super-resolution (SISR) aims to reconstruct hig...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: SRDiff_Single_Image_Super-Resolution_with_Diffusion_Probabil...\narxiv_paper_markdown: None\ncitations: 288\nversions: 5\nid: 1313\n-------------\nRow 1958:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1702.02429\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 85\nversions: 7\nid: 4645\n-------------\nRow 1959:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1805.06087\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 254\nversions: 3\nid: 4637\n-------------\nRow 1960:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1809.01215\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 103\nversions: 5\nid: 4639\n-------------\nRow 1961:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1605.03835\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 67\nversions: 4\nid: 4646\n-------------\nRow 1962:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1906.03351\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 116\nversions: 2\nid: 4650\n-------------\nRow 1963:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1909.01214\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 106\nversions: 6\nid: 4690\n-------------\nRow 1964:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1701.06549\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 66\nversions: 2\nid: 4638\n-------------\nRow 1965:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1706.03741\narxiv_title: Deep reinforcement learning from human preferences\narxiv_abstract: For sophisticated reinforcement learning (RL) systems to int...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Deep_reinforcement_learning_from_human_preferences.pdf\narxiv_paper_markdown: None\ncitations: 1935\nversions: 14\nid: 4688\n-------------\nRow 1966:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1511.06732\narxiv_title: Sequence Level Training with Recurrent Neural Networks\narxiv_abstract: Many natural language processing applications use language m...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Sequence_Level_Training_with_Recurrent_Neural_Networks.pdf\narxiv_paper_markdown: None\ncitations: 1764\nversions: 5\nid: 4676\n-------------\nRow 1967:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1909.08593\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 783\nversions: 2\nid: 4691\n-------------\nRow 1968:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.04029\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 7\nversions: 2\nid: 791\n-------------\nRow 1969:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.00927\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 517\nversions: 6\nid: 793\n-------------\nRow 1970:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.02262\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 124\nversions: 6\nid: 792\n-------------\nRow 1971:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1901.05103\narxiv_title: DeepSDF: Learning Continuous Signed Distance Functions for S...\narxiv_abstract: Computer graphics, 3D computer vision and robotics communiti...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Sh...\narxiv_paper_markdown: None\ncitations: 2974\nversions: 19\nid: 451\n-------------\nRow 1972:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1907.05600\narxiv_title: Generative Modeling by Estimating Gradients of the Data Dist...\narxiv_abstract: We introduce a new generative model where samples are produc...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Generative_Modeling_by_Estimating_Gradients_of_the_Data_Dist...\narxiv_paper_markdown: None\ncitations: 2204\nversions: 10\nid: 871\n-------------\nRow 1973:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2006.09011\narxiv_title: Improved Techniques for Training Score-Based Generative Mode...\narxiv_abstract: Score-based generative models can produce high quality image...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Improved_Techniques_for_Training_Score-Based_Generative_Mode...\narxiv_paper_markdown: None\ncitations: 720\nversions: 7\nid: 870\n-------------\nRow 1974:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2102.05379\narxiv_title: Argmax Flows and Multinomial Diffusion: Learning Categorical...\narxiv_abstract: Generative flows and diffusion models have been predominantl...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Argmax_Flows_and_Multinomial_Diffusion_Learning_Categorical\n...\narxiv_paper_markdown: None\ncitations: 229\nversions: 6\nid: 1010\n-------------\nRow 1975:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1908.07125\narxiv_title: Universal Adversarial Triggers for Attacking and Analyzing N...\narxiv_abstract: Adversarial examples highlight model vulnerabilities and are...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Universal_Adversarial_Triggers_for_Attacking_and_Analyzing_N...\narxiv_paper_markdown: None\ncitations: 704\nversions: 7\nid: 4656\n-------------\nRow 1976:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.20158\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4925\n-------------\nRow 1977:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.06117\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4914\n-------------\nRow 1978:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.17331\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4918\n-------------\nRow 1979:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.04408\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4921\n-------------\nRow 1980:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.14696\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4927\n-------------\nRow 1981:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.11511\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4929\n-------------\nRow 1982:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.10633\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4930\n-------------\nRow 1983:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2309.01431\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4931\n-------------\nRow 1984:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2311.08147\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4932\n-------------\nRow 1985:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2311.09476\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4934\n-------------\nRow 1986:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2401.15884\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4937\n-------------\nRow 1987:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2401.18059\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4938\n-------------\nRow 1988:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2401.12178\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4939\n-------------\nRow 1989:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2011.13456\narxiv_title: Score-Based Generative Modeling through Stochastic Different...\narxiv_abstract: Creating noise from data is easy; creating data from noise i...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Score-Based_Generative_Modeling_through_Stochastic_Different...\narxiv_paper_markdown: None\ncitations: 2850\nversions: 5\nid: 865\n-------------\nRow 1990:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2103.04677\narxiv_title: Behavior-Driven Synthesis of Human Dynamics\narxiv_abstract: Generating and representing human behavior are of major impo...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Behavior-Driven_Synthesis_of_Human_Dynamics.pdf\narxiv_paper_markdown: None\ncitations: 12\nversions: 9\nid: 477\n-------------\nRow 1991:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1611.05763\narxiv_title: Learning to reinforcement learn\narxiv_abstract: In recent years deep reinforcement learning (RL) systems hav...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Learning_to_reinforcement_learn.pdf\narxiv_paper_markdown: None\ncitations: 978\nversions: 8\nid: 4593\n-------------\nRow 1992:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2204.06125\narxiv_title: Hierarchical Text-Conditional Image Generation with CLIP Lat...\narxiv_abstract: Contrastive models like CLIP have been shown to learn robust...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Hierarchical_Text-Conditional_Image_Generation_with_CLIP_Lat...\narxiv_paper_markdown: None\ncitations: 3795\nversions: 3\nid: 424\n-------------\nRow 1993:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.06729\narxiv_title: Meta-Learned Models of Cognition\narxiv_abstract: Meta-learning is a framework for learning learning algorithm...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Meta-Learned_Models_of_Cognition.pdf\narxiv_paper_markdown: None\ncitations: 13\nversions: 9\nid: 4592\n-------------\nRow 1994:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2110.14168\narxiv_title: Training Verifiers to Solve Math Word Problems\narxiv_abstract: State-of-the-art language models can match human performance...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Training_Verifiers_to_Solve_Math_Word_Problems.pdf\narxiv_paper_markdown: None\ncitations: 951\nversions: 3\nid: 4600\n-------------\nRow 1995:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2006.14799\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 302\nversions: 2\nid: 4764\n-------------\nRow 1996:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1807.07545\narxiv_title: Rearranging the Familiar: Testing Compositional Generalizati...\narxiv_abstract: Systematic compositionality is the ability to recombine mean...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Rearranging_the_Familiar_Testing_Compositional_Generalizatio...\narxiv_paper_markdown: None\ncitations: 147\nversions: 6\nid: 4596\n-------------\nRow 1997:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.03350\narxiv_title: Measuring and Narrowing the Compositionality Gap in Language...\narxiv_abstract: We investigate the ability of language models to perform com...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Measuring_and_Narrowing_the_Compositionality_Gap_in_Language...\narxiv_paper_markdown: None\ncitations: 175\nversions: 6\nid: 4590\n-------------\nRow 1998:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2201.11903\narxiv_title: Chain-of-Thought Prompting Elicits Reasoning in Large Langua...\narxiv_abstract: We explore how generating a chain of thought -- a series of ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Chain-of-Thought_Prompting_Elicits_Reasoning_in_Large_Langua...\narxiv_paper_markdown: None\ncitations: 3807\nversions: 12\nid: 80\n-------------\nRow 1999:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2103.03874\narxiv_title: Measuring Mathematical Problem Solving With the MATH Dataset\narxiv_abstract: Many intellectual endeavors require mathematical problem sol...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Measuring_Mathematical_Problem_Solving_With_the_MATH_Dataset...\narxiv_paper_markdown: None\ncitations: 428\nversions: 7\nid: 4601\n-------------\nRow 2000:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2006.09891\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 6\nversions: 2\nid: 4777\n-------------\nRow 2001:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.14217\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4769\n-------------\nRow 2002:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2107.13586\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4772\n-------------\nRow 2003:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2104.14795\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4773\n-------------\nRow 2004:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2101.00822\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 12\nversions: 2\nid: 4766\n-------------\nRow 2005:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1809.09600\narxiv_title: HotpotQA: A Dataset for Diverse, Explainable Multi-hop Quest...\narxiv_abstract: Existing question answering (QA) datasets fail to train QA s...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: HotpotQA_A_Dataset_for_Diverse,_Explainable_Multi-hop_Questi...\narxiv_paper_markdown: None\ncitations: 1616\nversions: 9\nid: 4606\n-------------\nRow 2006:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1905.10044\narxiv_title: BoolQ: Exploring the Surprising Difficulty of Natural Yes/No...\narxiv_abstract: In this paper we study yes/no questions that are naturally o...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: BoolQ_Exploring_the_Surprising_Difficulty_of_Natural_Yes/No_...\narxiv_paper_markdown: None\ncitations: 727\nversions: 7\nid: 4599\n-------------\nRow 2007:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1911.11641\narxiv_title: PIQA: Reasoning about Physical Commonsense in Natural Langua...\narxiv_abstract: To apply eyeshadow without a brush, should I use a cotton sw...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: PIQA_Reasoning_about_Physical_Commonsense_in_Natural_Languag...\narxiv_paper_markdown: None\ncitations: 588\nversions: 9\nid: 4603\n-------------\nRow 2008:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1904.09728\narxiv_title: SocialIQA: Commonsense Reasoning about Social Interactions\narxiv_abstract: We introduce Social IQa, the first largescale benchmark for ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: SocialIQA_Commonsense_Reasoning_about_Social_Interactions.pd...\narxiv_paper_markdown: None\ncitations: 492\nversions: 9\nid: 4604\n-------------\nRow 2009:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.10601\narxiv_title: Tree of Thoughts: Deliberate Problem Solving with Large Lang...\narxiv_abstract: Language models are increasingly being deployed for general ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Tree_of_Thoughts_Deliberate_Problem_Solving_with_Large_Langu...\narxiv_paper_markdown: None\ncitations: 608\nversions: 7\nid: 14\n-------------\nRow 2010:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.14542\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4629\n-------------\nRow 2011:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.04674\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4842\n-------------\nRow 2012:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1809.02789\narxiv_title: Can a Suit of Armor Conduct Electricity? A New Dataset for O...\narxiv_abstract: We present a new kind of question answering dataset, OpenBoo...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Can_a_Suit_of_Armor_Conduct_Electricity?_A_New_Dataset_for_O...\narxiv_paper_markdown: None\ncitations: 677\nversions: 4\nid: 4605\n-------------\nRow 2013:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.14999\narxiv_title: Empirical Analysis of the Strengths and Weaknesses of PEFT T...\narxiv_abstract: As foundation models continue to exponentially scale in size...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Empirical_Analysis_of_the_Strengths_and_Weaknesses_of_PEFT_T...\narxiv_paper_markdown: None\ncitations: 8\nversions: 3\nid: 4607\n-------------\nRow 2014:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1609.08144\narxiv_title: Google's Neural Machine Translation System: Bridging the Gap...\narxiv_abstract: Neural Machine Translation (NMT) is an end-to-end learning a...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Google's_Neural_Machine_Translation_System_Bridging_the_Gap_...\narxiv_paper_markdown: None\ncitations: 8224\nversions: 13\nid: 4678\n-------------\nRow 2015:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1801.06146\narxiv_title: Universal Language Model Fine-tuning for Text Classification\narxiv_abstract: Inductive transfer learning has greatly impacted computer vi...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Universal_Language_Model_Fine-tuning_for_Text_Classification...\narxiv_paper_markdown: None\ncitations: 4245\nversions: 11\nid: 4630\n-------------\nRow 2016:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1911.12543\narxiv_title: How Can We Know What Language Models Know?\narxiv_abstract: Recent work has presented intriguing results examining the k...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: How_Can_We_Know_What_Language_Models_Know?.pdf\narxiv_paper_markdown: None\ncitations: 1031\nversions: 17\nid: 4653\n-------------\nRow 2017:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1908.10084\narxiv_title: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networ...\narxiv_abstract: BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) ha...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Sentence-BERT_Sentence_Embeddings_using_Siamese_BERT-Network...\narxiv_paper_markdown: None\ncitations: 9317\nversions: 8\nid: 4627\n-------------\nRow 2018:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1904.09751\narxiv_title: The Curious Case of Neural Text Degeneration\narxiv_abstract: Despite considerable advancements with deep neural language ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: The_Curious_Case_of_Neural_Text_Degeneration.pdf\narxiv_paper_markdown: None\ncitations: 2330\nversions: 8\nid: 4631\n-------------\nRow 2019:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1612.00005\narxiv_title: Plug & Play Generative Networks: Conditional Iterative Gener...\narxiv_abstract: Generating high-resolution, photo-realistic images has been ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Plug_&_Play_Generative_Networks_Conditional_Iterative_Genera...\narxiv_paper_markdown: None\ncitations: 951\nversions: 10\nid: 4699\n-------------\nRow 2020:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1805.04833\narxiv_title: Hierarchical Neural Story Generation\narxiv_abstract: We explore story generation: creative systems that can build...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Hierarchical_Neural_Story_Generation.pdf\narxiv_paper_markdown: None\ncitations: 1406\nversions: 6\nid: 4632\n-------------\nRow 2021:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1906.09531\narxiv_title: Bias Correction of Learned Generative Models using Likelihoo...\narxiv_abstract: A learned generative model often produces biased statistics ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Bias_Correction_of_Learned_Generative_Models_using_Likelihoo...\narxiv_paper_markdown: None\ncitations: 127\nversions: 9\nid: 4647\n-------------\nRow 2022:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1804.07036\narxiv_title: Learning to Extract Coherent Summary via Deep Reinforcement ...\narxiv_abstract: Coherence plays a critical role in producing a high-quality ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Learning_to_Extract_Coherent_Summary_via_Deep_Reinforcement_...\narxiv_paper_markdown: None\ncitations: 175\nversions: 8\nid: 4682\n-------------\nRow 2023:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2207.09094\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4848\n-------------\nRow 2024:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1912.02164\narxiv_title: Plug and Play Language Models: A Simple Approach to Controll...\narxiv_abstract: Large transformer-based language models (LMs) trained on hug...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Plug_and_Play_Language_Models_A_Simple_Approach_to_Controlle...\narxiv_paper_markdown: None\ncitations: 761\nversions: 8\nid: 4700\n-------------\nRow 2025:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1705.04304\narxiv_title: A Deep Reinforced Model for Abstractive Summarization\narxiv_abstract: Attentional, RNN-based encoder-decoder models for abstractiv...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: A_Deep_Reinforced_Model_for_Abstractive_Summarization.pdf\narxiv_paper_markdown: None\ncitations: 1782\nversions: 4\nid: 4681\n-------------\nRow 2026:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1707.07402\narxiv_title: Reinforcement Learning for Bandit Neural Machine Translation...\narxiv_abstract: Machine translation is a natural candidate problem for reinf...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Reinforcement_Learning_for_Bandit_Neural_Machine_Translation...\narxiv_paper_markdown: None\ncitations: 107\nversions: 8\nid: 4679\n-------------\nRow 2027:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1904.13015\narxiv_title: Towards Coherent and Engaging Spoken Dialog Response Generat...\narxiv_abstract: Encoder-decoder based neural architectures serve as the basi...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Towards_Coherent_and_Engaging_Spoken_Dialog_Response_Generat...\narxiv_paper_markdown: None\ncitations: 49\nversions: 11\nid: 4689\n-------------\nRow 2028:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1809.00125\narxiv_title: Simple Fusion: Return of the Language Model\narxiv_abstract: Neural Machine Translation (NMT) typically leverages monolin...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Simple_Fusion_Return_of_the_Language_Model.pdf\narxiv_paper_markdown: None\ncitations: 74\nversions: 8\nid: 4702\n-------------\nRow 2029:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2205.10937\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4852\n-------------\nRow 2030:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.02770\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4853\n-------------\nRow 2031:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1809.10736\narxiv_title: Controllable Neural Story Plot Generation via Reward Shaping\narxiv_abstract: Language-modeling--based approaches to story plot generation...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Controllable_Neural_Story_Plot_Generation_via_Reward_Shaping...\narxiv_paper_markdown: None\ncitations: 113\nversions: 6\nid: 4683\n-------------\nRow 2032:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1907.00151\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 49\nversions: 2\nid: 4770\n-------------\nRow 2033:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2104.08857\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 12\nversions: 6\nid: 4776\n-------------\nRow 2034:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2103.10685\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 44\nversions: 7\nid: 4786\n-------------\nRow 2035:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2104.04039\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 18\nversions: 6\nid: 4771\n-------------\nRow 2036:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1812.08318\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 14\nversions: 4\nid: 4779\n-------------\nRow 2037:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1810.04805\narxiv_title: BERT: Pre-training of Deep Bidirectional Transformers for La...\narxiv_abstract: We introduce a new language representation model called BERT...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Lan...\narxiv_paper_markdown: None\ncitations: 94319\nversions: 49\nid: 16\n-------------\nRow 2038:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1910.13461\narxiv_title: BART: Denoising Sequence-to-Sequence Pre-training for Natura...\narxiv_abstract: We present BART, a denoising autoencoder for pretraining seq...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: BART_Denoising_Sequence-to-Sequence_Pre-training_for_Natural...\narxiv_paper_markdown: None\ncitations: 8554\nversions: 7\nid: 17\n-------------\nRow 2039:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2102.01454\narxiv_title: MAUVE: Measuring the Gap Between Neural Text and Human Text ...\narxiv_abstract: As major progress is made in open-ended text generation, mea...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: MAUVE_Measuring_the_Gap_Between_Neural_Text_and_Human_Text_u...\narxiv_paper_markdown: None\ncitations: 188\nversions: 8\nid: 4775\n-------------\nRow 2040:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2103.03404\narxiv_title: Attention is Not All You Need: Pure Attention Loses Rank Dou...\narxiv_abstract: Attention-based architectures have become ubiquitous in mach...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Attention_is_Not_All_You_Need_Pure_Attention_Loses_Rank_Doub...\narxiv_paper_markdown: None\ncitations: 271\nversions: 7\nid: 4765\n-------------\nRow 2041:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2206.03382\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4873\n-------------\nRow 2042:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2207.04648\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4875\n-------------\nRow 2043:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1706.03762\narxiv_title: Attention Is All You Need\narxiv_abstract: The dominant sequence transduction models are based on compl...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Attention_Is_All_You_Need.pdf\narxiv_paper_markdown: None\ncitations: 111587\nversions: 87\nid: 15\n-------------\nRow 2044:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1910.10683\narxiv_title: Exploring the Limits of Transfer Learning with a Unified Tex...\narxiv_abstract: Transfer learning, where a model is first pre-trained on a d...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Exploring_the_Limits_of_Transfer_Learning_with_a_Unified_Tex...\narxiv_paper_markdown: None\ncitations: 13834\nversions: 16\nid: 106\n-------------\nRow 2045:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1907.11692\narxiv_title: RoBERTa: A Robustly Optimized BERT Pretraining Approach\narxiv_abstract: Language model pretraining has led to significant performanc...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: RoBERTa_A_Robustly_Optimized_BERT_Pretraining_Approach.pdf\narxiv_paper_markdown: None\ncitations: 11875\nversions: 9\nid: 4774\n-------------\nRow 2046:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1703.00955\narxiv_title: Toward Controlled Generation of Text\narxiv_abstract: Generic generation and manipulation of text is challenging a...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Toward_Controlled_Generation_of_Text.pdf\narxiv_paper_markdown: None\ncitations: 1092\nversions: 11\nid: 4767\n-------------\nRow 2047:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1908.04319\narxiv_title: Neural Text Generation with Unlikelihood Training\narxiv_abstract: Neural text generation is a key tool in natural language app...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Neural_Text_Generation_with_Unlikelihood_Training.pdf\narxiv_paper_markdown: None\ncitations: 461\nversions: 5\nid: 4719\n-------------\nRow 2048:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1910.02054\narxiv_title: ZeRO: Memory Optimizations Toward Training Trillion Paramete...\narxiv_abstract: Large deep learning models offer significant accuracy gains,...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: ZeRO_Memory_Optimizations_Toward_Training_Trillion_Parameter...\narxiv_paper_markdown: None\ncitations: 711\nversions: 12\nid: 72\n-------------\nRow 2049:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2004.05150\narxiv_title: Longformer: The Long-Document Transformer\narxiv_abstract: Transformer-based models are unable to process long sequence...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Longformer_The_Long-Document_Transformer.pdf\narxiv_paper_markdown: None\ncitations: 3137\nversions: 5\nid: 133\n-------------\nRow 2050:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2002.10375\narxiv_title: Discriminative Adversarial Search for Abstractive Summarizat...\narxiv_abstract: We introduce a novel approach for sequence decoding, Discrim...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Discriminative_Adversarial_Search_for_Abstractive_Summarizat...\narxiv_paper_markdown: None\ncitations: 36\nversions: 15\nid: 4641\n-------------\nRow 2051:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1909.08053\narxiv_title: Megatron-LM: Training Multi-Billion Parameter Language Model...\narxiv_abstract: Recent work in language modeling demonstrates that training ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Megatron-LM_Training_Multi-Billion_Parameter_Language_Models...\narxiv_paper_markdown: None\ncitations: 1215\nversions: 4\nid: 71\n-------------\nRow 2052:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2311.06595\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4940\n-------------\nRow 2053:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2108.05036\narxiv_title: DEMix Layers: Disentangling Domains for Modular Language Mod...\narxiv_abstract: We introduce a new domain expert mixture (DEMix) layer that ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: DEMix_Layers_Disentangling_Domains_for_Modular_Language_Mode...\narxiv_paper_markdown: None\ncitations: 82\nversions: 6\nid: 4877\n-------------\nRow 2054:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2311.09210\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4941\n-------------\nRow 2055:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.13682\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4942\n-------------\nRow 2056:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.12836\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4943\n-------------\nRow 2057:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.07713\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4947\n-------------\nRow 2058:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.03025\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4950\n-------------\nRow 2059:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.05149\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4952\n-------------\nRow 2060:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.05002\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4954\n-------------\nRow 2061:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1711.07632\narxiv_title: Generating Thematic Chinese Poetry using Conditional Variati...\narxiv_abstract: Computer poetry generation is our first step towards compute...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Generating_Thematic_Chinese_Poetry_using_Conditional_Variati...\narxiv_paper_markdown: None\ncitations: 53\nversions: 6\nid: 4782\n-------------\nRow 2062:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1905.11975\narxiv_title: On Variational Learning of Controllable Representations for ...\narxiv_abstract: The variational autoencoder (VAE) can learn the manifold of ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: On_Variational_Learning_of_Controllable_Representations_for_...\narxiv_paper_markdown: None\ncitations: 58\nversions: 6\nid: 4781\n-------------\nRow 2063:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1909.05858\narxiv_title: CTRL: A Conditional Transformer Language Model for Controlla...\narxiv_abstract: Large-scale language models show promising text generation c...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: CTRL_A_Conditional_Transformer_Language_Model_for_Controllab...\narxiv_paper_markdown: None\ncitations: 1049\nversions: 2\nid: 4635\n-------------\nRow 2064:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2002.05058\narxiv_title: Learning to Compare for Better Training and Evaluation of Op...\narxiv_abstract: Automated evaluation of open domain natural language generat...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Learning_to_Compare_for_Better_Training_and_Evaluation_of_Op...\narxiv_paper_markdown: None\ncitations: 35\nversions: 7\nid: 4784\n-------------\nRow 2065:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1604.00562\narxiv_title: Reasoning About Pragmatics with Neural Listeners and Speaker...\narxiv_abstract: We present a model for pragmatically describing scenes, in w...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Reasoning_About_Pragmatics_with_Neural_Listeners_and_Speaker...\narxiv_paper_markdown: None\ncitations: 170\nversions: 11\nid: 4832\n-------------\nRow 2066:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2009.09708\narxiv_title: Knowledge Bridging for Empathetic Dialogue Generation\narxiv_abstract: Lack of external knowledge makes empathetic dialogue systems...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Knowledge_Bridging_for_Empathetic_Dialogue_Generation.pdf\narxiv_paper_markdown: None\ncitations: 74\nversions: 10\nid: 4830\n-------------\nRow 2067:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2201.10890\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 9\nversions: 3\nid: 4847\n-------------\nRow 2068:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.14397\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 13\nversions: 2\nid: 4851\n-------------\nRow 2069:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2202.01169\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 28\nversions: 3\nid: 4841\n-------------\nRow 2070:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2110.03360\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 15\nversions: 3\nid: 4863\n-------------\nRow 2071:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2109.02008\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 9\nversions: 2\nid: 4866\n-------------\nRow 2072:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.14685\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 14\nversions: 2\nid: 4872\n-------------\nRow 2073:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2110.07431\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 9\nversions: 2\nid: 4864\n-------------\nRow 2074:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2202.08906\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 41\nversions: 2\nid: 4843\n-------------\nRow 2075:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2109.11817\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 8\nversions: 3\nid: 4860\n-------------\nRow 2076:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.06850\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 11\nversions: 2\nid: 4846\n-------------\nRow 2077:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1701.06538\narxiv_title: Outrageously Large Neural Networks: The Sparsely-Gated\n  Mix...\narxiv_abstract: The capacity of a neural network to absorb information is li...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Outrageously_Large_Neural_Networks_The_Sparsely-Gated\n__Mixt...\narxiv_paper_markdown: None\ncitations: 1702\nversions: 16\nid: 136\n-------------\nRow 2078:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.12533\narxiv_title: Pathways: Asynchronous Distributed Dataflow for ML\narxiv_abstract: We present the design of a new large scale orchestration lay...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Pathways_Asynchronous_Distributed_Dataflow_for_ML.pdf\narxiv_paper_markdown: None\ncitations: 103\nversions: 6\nid: 4871\n-------------\nRow 2079:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2107.11817\narxiv_title: Go Wider Instead of Deeper\narxiv_abstract: More transformer blocks with residual connections have recen...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Go_Wider_Instead_of_Deeper.pdf\narxiv_paper_markdown: None\ncitations: 51\nversions: 6\nid: 4839\n-------------\nRow 2080:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2110.01786\narxiv_title: MoEfication: Transformer Feed-forward Layers are Mixtures of...\narxiv_abstract: Recent work has shown that feed-forward networks (FFNs) in p...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: MoEfication_Transformer_Feed-forward_Layers_are_Mixtures_of_...\narxiv_paper_markdown: None\ncitations: 42\nversions: 6\nid: 4865\n-------------\nRow 2081:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.11761\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4957\n-------------\nRow 2082:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.07922\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4958\n-------------\nRow 2083:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2307.03172\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4960\n-------------\nRow 2084:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.19912\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4964\n-------------\nRow 2085:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.18846\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4966\n-------------\nRow 2086:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2201.12023\narxiv_title: Alpa: Automating Inter- and Intra-Operator Parallelism for D...\narxiv_abstract: Alpa automates model-parallel training of large deep learnin...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Alpa_Automating_Inter-_and_Intra-Operator_Parallelism_for_Di...\narxiv_paper_markdown: None\ncitations: 164\nversions: 15\nid: 4870\n-------------\nRow 2087:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1902.07816\narxiv_title: Mixture Models for Diverse Machine Translation: Tricks of th...\narxiv_abstract: Mixture models trained via EM are among the simplest, most w...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Mixture_Models_for_Diverse_Machine_Translation_Tricks_of_the...\narxiv_paper_markdown: None\ncitations: 123\nversions: 10\nid: 4867\n-------------\nRow 2088:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2106.03760\narxiv_title: DSelect-k: Differentiable Selection in the Mixture of Expert...\narxiv_abstract: The Mixture-of-Experts (MoE) architecture is showing promisi...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: DSelect-k_Differentiable_Selection_in_the_Mixture_of_Experts...\narxiv_paper_markdown: None\ncitations: 80\nversions: 8\nid: 4859\n-------------\nRow 2089:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.06905\narxiv_title: GLaM: Efficient Scaling of Language Models with Mixture-of-E...\narxiv_abstract: Scaling language models with more data, compute and paramete...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: GLaM_Efficient_Scaling_of_Language_Models_with_Mixture-of-Ex...\narxiv_paper_markdown: None\ncitations: 308\nversions: 5\nid: 78\n-------------\nRow 2090:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2106.04426\narxiv_title: Hash Layers For Large Sparse Models\narxiv_abstract: We investigate the training of sparse layers that use differ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Hash_Layers_For_Large_Sparse_Models.pdf\narxiv_paper_markdown: None\ncitations: 119\nversions: 5\nid: 4885\n-------------\nRow 2091:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1911.02150\narxiv_title: Fast Transformer Decoding: One Write-Head is All You Need\narxiv_abstract: Multi-head attention layers, as used in the Transformer neur...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Fast_Transformer_Decoding_One_Write-Head_is_All_You_Need.pdf\narxiv_paper_markdown: None\ncitations: 125\nversions: 2\nid: 4886\n-------------\nRow 2092:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.17653\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4971\n-------------\nRow 2093:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2201.05596\narxiv_title: DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Tr...\narxiv_abstract: As the training of giant dense models hits the boundary on t...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: DeepSpeed-MoE_Advancing_Mixture-of-Experts_Inference_and_Tra...\narxiv_paper_markdown: None\ncitations: 133\nversions: 5\nid: 4836\n-------------\nRow 2094:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2202.09368\narxiv_title: Mixture-of-Experts with Expert Choice Routing\narxiv_abstract: Sparsely-activated Mixture-of-experts (MoE) models allow the...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Mixture-of-Experts_with_Expert_Choice_Routing.pdf\narxiv_paper_markdown: None\ncitations: 99\nversions: 5\nid: 4609\n-------------\nRow 2095:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2110.03742\narxiv_title: Beyond Distillation: Task-level Mixture-of-Experts for Effic...\narxiv_abstract: Sparse Mixture-of-Experts (MoE) has been a successful approa...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Beyond_Distillation_Task-level_Mixture-of-Experts_for_Effici...\narxiv_paper_markdown: None\ncitations: 64\nversions: 5\nid: 4876\n-------------\nRow 2096:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2110.04260\narxiv_title: Taming Sparsely Activated Transformer with Stochastic Expert...\narxiv_abstract: Sparsely activated models (SAMs), such as Mixture-of-Experts...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Taming_Sparsely_Activated_Transformer_with_Stochastic_Expert...\narxiv_paper_markdown: None\ncitations: 66\nversions: 4\nid: 4838\n-------------\nRow 2097:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.10684\narxiv_title: Efficient Large Scale Language Modeling with Mixtures of Exp...\narxiv_abstract: Mixture of Experts layers (MoEs) enable efficient scaling of...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Efficient_Large_Scale_Language_Modeling_with_Mixtures_of_Exp...\narxiv_paper_markdown: None\ncitations: 69\nversions: 4\nid: 4858\n-------------\nRow 2098:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.01352\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4892\n-------------\nRow 2099:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2204.09179\narxiv_title: On the Representation Collapse of Sparse Mixture of Experts\narxiv_abstract: Sparse mixture of experts provides larger model capacity whi...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: On_the_Representation_Collapse_of_Sparse_Mixture_of_Experts....\narxiv_paper_markdown: None\ncitations: 39\nversions: 6\nid: 4849\n-------------\nRow 2100:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2103.13262\narxiv_title: FastMoE: A Fast Mixture-of-Expert Training System\narxiv_abstract: Mixture-of-Expert (MoE) presents a strong potential in enlar...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: FastMoE_A_Fast_Mixture-of-Expert_Training_System.pdf\narxiv_paper_markdown: None\ncitations: 54\nversions: 2\nid: 4874\n-------------\nRow 2101:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.01104\narxiv_title: Parameter-Efficient Mixture-of-Experts Architecture for Pre-...\narxiv_abstract: Recently, Mixture-of-Experts (short as MoE) architecture has...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Parameter-Efficient_Mixture-of-Experts_Architecture_for_Pre-...\narxiv_paper_markdown: None\ncitations: 15\nversions: 5\nid: 4845\n-------------\nRow 2102:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2204.08396\narxiv_title: StableMoE: Stable Routing Strategy for Mixture of Experts\narxiv_abstract: The Mixture-of-Experts (MoE) technique can scale up the mode...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: StableMoE_Stable_Routing_Strategy_for_Mixture_of_Experts.pdf\narxiv_paper_markdown: None\ncitations: 31\nversions: 4\nid: 4840\n-------------\nRow 2103:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2105.15082\narxiv_title: M6-T: Exploring Sparse Expert Models and Beyond\narxiv_abstract: Mixture-of-Experts (MoE) models can achieve promising result...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: M6-T_Exploring_Sparse_Expert_Models_and_Beyond.pdf\narxiv_paper_markdown: None\ncitations: 44\nversions: 2\nid: 4861\n-------------\nRow 2104:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2107.03374\narxiv_title: Evaluating Large Language Models Trained on Code\narxiv_abstract: We introduce Codex, a GPT language model fine-tuned on publi...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Evaluating_Large_Language_Models_Trained_on_Code.pdf\narxiv_paper_markdown: None\ncitations: 1780\nversions: 2\nid: 75\n-------------\nRow 2105:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1806.03377\narxiv_title: PipeDream: Fast and Efficient Pipeline Parallel DNN Training\narxiv_abstract: PipeDream is a Deep Neural Network(DNN) training system for ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: PipeDream_Fast_and_Efficient_Pipeline_Parallel_DNN_Training....\narxiv_paper_markdown: None\ncitations: 230\nversions: 10\nid: 4883\n-------------\nRow 2106:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2101.03961\narxiv_title: Switch Transformers: Scaling to Trillion Parameter Models wi...\narxiv_abstract: In deep learning, models typically reuse the same parameters...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Switch_Transformers_Scaling_to_Trillion_Parameter_Models_wit...\narxiv_paper_markdown: None\ncitations: 1212\nversions: 4\nid: 74\n-------------\nRow 2107:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1803.05457\narxiv_title: Think you have Solved Question Answering? Try ARC, the AI2 R...\narxiv_abstract: We present a new question set, text corpus, and baselines as...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Think_you_have_Solved_Question_Answering?_Try_ARC,_the_AI2_R...\narxiv_paper_markdown: None\ncitations: 803\nversions: 3\nid: 4602\n-------------\nRow 2108:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2312.10997\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4895\n-------------\nRow 2109:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2311.05232\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4987\n-------------\nRow 2110:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2004.04906\narxiv_title: Dense Passage Retrieval for Open-Domain Question Answering\narxiv_abstract: Open-domain question answering relies on efficient passage r...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Dense_Passage_Retrieval_for_Open-Domain_Question_Answering.p...\narxiv_paper_markdown: None\ncitations: 2254\nversions: 10\nid: 4986\n-------------\nRow 2111:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.00083\narxiv_title: In-Context Retrieval-Augmented Language Models\narxiv_abstract: Retrieval-Augmented Language Modeling (RALM) methods, which ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: In-Context_Retrieval-Augmented_Language_Models.pdf\narxiv_paper_markdown: None\ncitations: 147\nversions: 7\nid: 4990\n-------------\nRow 2112:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2103.00823\narxiv_title: M6: A Chinese Multimodal Pretrainer\narxiv_abstract: In this work, we construct the largest dataset for multimoda...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: M6_A_Chinese_Multimodal_Pretrainer.pdf\narxiv_paper_markdown: None\ncitations: 122\nversions: 2\nid: 4884\n-------------\nRow 2113:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2401.06066\narxiv_title: DeepSeekMoE: Towards Ultimate Expert Specialization in\n  Mix...\narxiv_abstract: In the era of large language models, Mixture-of-Experts (MoE...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: DeepSeekMoE_Towards_Ultimate_Expert_Specialization_in\n__Mixt...\narxiv_paper_markdown: None\ncitations: 8\nversions: 2\nid: 4887\n-------------\nRow 2114:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2301.12652\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 33\nversions: 2\nid: 4919\n-------------\nRow 2115:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.04757\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 20\nversions: 2\nid: 4922\n-------------\nRow 2116:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.02437\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 11\nversions: 4\nid: 4968\n-------------\nRow 2117:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.14322\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 13\nversions: 2\nid: 4970\n-------------\nRow 2118:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08518\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 19\nversions: 4\nid: 4920\n-------------\nRow 2119:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.14283\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 21\nversions: 4\nid: 4967\n-------------\nRow 2120:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.13269\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 23\nversions: 2\nid: 4965\n-------------\nRow 2121:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.08559\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 46\nversions: 5\nid: 4973\n-------------\nRow 2122:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.07678\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 52\nversions: 4\nid: 4916\n-------------\nRow 2123:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.15294\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: 27\nversions: 4\nid: 4917\n-------------\nRow 2124:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2005.11401\narxiv_title: Retrieval-Augmented Generation for Knowledge-Intensive NLP T...\narxiv_abstract: Large pre-trained language models have been shown to store f...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_T...\narxiv_paper_markdown: None\ncitations: 1667\nversions: 13\nid: 4598\n-------------\nRow 2125:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2112.04426\narxiv_title: Improving language models by retrieving from trillions of to...\narxiv_abstract: We enhance auto-regressive language models by conditioning o...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Improving_language_models_by_retrieving_from_trillions_of_to...\narxiv_paper_markdown: None\ncitations: 598\nversions: 5\nid: 4923\n-------------\nRow 2126:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2402.10200\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 5020\n-------------\nRow 2127:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2402.06664\narxiv_title: LLM Agents can Autonomously Hack Websites\narxiv_abstract: In recent years, large language models (LLMs) have become in...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: LLM_Agents_can_Autonomously_Hack_Websites.pdf\narxiv_paper_markdown: None\ncitations: 1\nversions: 2\nid: 4998\n-------------\nRow 2128:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.08411\narxiv_title: Large Language Models Struggle to Learn Long-Tail Knowledge\narxiv_abstract: The Internet contains a wealth of knowledge -- from the birt...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Large_Language_Models_Struggle_to_Learn_Long-Tail_Knowledge....\narxiv_paper_markdown: None\ncitations: 131\nversions: 9\nid: 4901\n-------------\nRow 2129:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.10496\narxiv_title: Precise Zero-Shot Dense Retrieval without Relevance Labels\narxiv_abstract: While dense retrieval has been shown effective and efficient...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.p...\narxiv_paper_markdown: None\ncitations: 65\nversions: 10\nid: 4915\n-------------\nRow 2130:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.10511\narxiv_title: When Not to Trust Language Models: Investigating Effectivene...\narxiv_abstract: Despite their impressive performance on diverse tasks, large...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: When_Not_to_Trust_Language_Models_Investigating_Effectivenes...\narxiv_paper_markdown: None\ncitations: 94\nversions: 6\nid: 4903\n-------------\nRow 2131:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.11755\narxiv_title: Promptagator: Few-shot Dense Retrieval From 8 Examples\narxiv_abstract: Much recent research on information retrieval has focused on...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Promptagator_Few-shot_Dense_Retrieval_From_8_Examples.pdf\narxiv_paper_markdown: None\ncitations: 103\nversions: 5\nid: 4979\n-------------\nRow 2132:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2305.06983\narxiv_title: Active Retrieval Augmented Generation\narxiv_abstract: Despite the remarkable ability of large language models (LMs...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Active_Retrieval_Augmented_Generation.pdf\narxiv_paper_markdown: None\ncitations: 94\nversions: 6\nid: 4928\n-------------\nRow 2133:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.03493\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 5027\n-------------\nRow 2134:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2209.10063\narxiv_title: Generate rather than Retrieve: Large Language Models are Str...\narxiv_abstract: Knowledge-intensive tasks, such as open-domain question answ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Generate_rather_than_Retrieve_Large_Language_Models_are_Stro...\narxiv_paper_markdown: None\ncitations: 138\nversions: 4\nid: 4956\n-------------\nRow 2135:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.02627\narxiv_title: Improving the Domain Adaptation of Retrieval Augmented Gener...\narxiv_abstract: Retrieval Augment Generation (RAG) is a recent advancement i...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Improving_the_Domain_Adaptation_of_Retrieval_Augmented_Gener...\narxiv_paper_markdown: None\ncitations: 31\nversions: 9\nid: 4891\n-------------\nRow 2136:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2203.08773\narxiv_title: Training Data is More Valuable than You Think: A Simple and ...\narxiv_abstract: Retrieval-based methods have been shown to be effective in N...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Training_Data_is_More_Valuable_than_You_Think_A_Simple_and_E...\narxiv_paper_markdown: None\ncitations: 79\nversions: 5\nid: 4981\n-------------\nRow 2137:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2211.12561\narxiv_title: Retrieval-Augmented Multimodal Language Modeling\narxiv_abstract: Recent multimodal models such as DALL-E and CM3 have achieve...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Retrieval-Augmented_Multimodal_Language_Modeling.pdf\narxiv_paper_markdown: None\ncitations: 42\nversions: 6\nid: 4989\n-------------\nRow 2138:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2208.03299\narxiv_title: Atlas: Few-shot Learning with Retrieval Augmented Language M...\narxiv_abstract: Large language models have shown impressive few-shot results...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Atlas_Few-shot_Learning_with_Retrieval_Augmented_Language_Mo...\narxiv_paper_markdown: None\ncitations: 309\nversions: 3\nid: 4980\n-------------\nRow 2139:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/12457457234623434.56789\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 5034\n-------------\nRow 2140:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/98724723463246234623466.54321\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 5035\n-------------\nRow 2141:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/11223472347234722.3344\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 5036\n-------------\nRow 2142:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.09687\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 5040\n-------------\nRow 2143:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2402.99200\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 5042\n-------------\nRow 2144:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2010.02903\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 5043\n-------------\nRow 2145:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2310.06147\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 5047\n-------------\nRow 2146:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.10509\narxiv_title: Interleaving Retrieval with Chain-of-Thought Reasoning for\n ...\narxiv_abstract: Prompting-based large language models (LLMs) are surprisingl...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Interleaving_Retrieval_with_Chain-of-Thought_Reasoning_for\n_...\narxiv_paper_markdown: None\ncitations: 87\nversions: 5\nid: 4926\n-------------\nRow 2147:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2212.14024\narxiv_title: Demonstrate-Search-Predict: Composing retrieval and language...\narxiv_abstract: Retrieval-augmented in-context learning has emerged as a pow...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Demonstrate-Search-Predict_Composing_retrieval_and_language_...\narxiv_paper_markdown: None\ncitations: 95\nversions: 4\nid: 4975\n-------------\nRow 2148:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2108.13934\narxiv_title: Robust Retrieval Augmented Generation for Zero-shot Slot Fil...\narxiv_abstract: Automatically inducing high quality knowledge graphs from a ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Robust_Retrieval_Augmented_Generation_for_Zero-shot_Slot_Fil...\narxiv_paper_markdown: None\ncitations: 24\nversions: 8\nid: 4984\n-------------\nRow 2149:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2210.01296\narxiv_title: Recitation-Augmented Language Models\narxiv_abstract: We propose a new paradigm to help Large Language Models (LLM...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Recitation-Augmented_Language_Models.pdf\narxiv_paper_markdown: None\ncitations: 59\nversions: 5\nid: 4978\n-------------\nRow 2150:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2201.12431\narxiv_title: Neuro-Symbolic Language Modeling with Automaton-augmented Re...\narxiv_abstract: Retrieval-based language models (R-LM) model the probability...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Neuro-Symbolic_Language_Modeling_with_Automaton-augmented_Re...\narxiv_paper_markdown: None\ncitations: 38\nversions: 5\nid: 4982\n-------------\nRow 2151:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2312.00752\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 5053\n-------------\nRow 2152:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2005.14165\narxiv_title: Language Models are Few-Shot Learners\narxiv_abstract: Recent work has demonstrated substantial gains on many NLP t...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Language_Models_are_Few-Shot_Learners.pdf\narxiv_paper_markdown: None\ncitations: 22336\nversions: 33\nid: 110\n-------------\nRow 2153:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1909.11942\narxiv_title: ALBERT: A Lite BERT for Self-supervised Learning of Language...\narxiv_abstract: Increasing model size when pretraining natural language repr...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: ALBERT_A_Lite_BERT_for_Self-supervised_Learning_of_Language\n...\narxiv_paper_markdown: None\ncitations: 6517\nversions: 10\nid: 5016\n-------------\nRow 2154:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2202.03629\narxiv_title: Survey of Hallucination in Natural Language Generation\narxiv_abstract: Natural Language Generation (NLG) has improved exponentially...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Survey_of_Hallucination_in_Natural_Language_Generation.pdf\narxiv_paper_markdown: None\ncitations: 1281\nversions: 11\nid: 450\n-------------\nRow 2155:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1906.05317\narxiv_title: COMET: Commonsense Transformers for Automatic Knowledge Grap...\narxiv_abstract: We present the first comprehensive study on automatic knowle...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: COMET_Commonsense_Transformers_for_Automatic_Knowledge_Graph...\narxiv_paper_markdown: None\ncitations: 903\nversions: 12\nid: 5010\n-------------\nRow 2156:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2001.08361\narxiv_title: Scaling Laws for Neural Language Models\narxiv_abstract: We study empirical scaling laws for language model performan...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Scaling_Laws_for_Neural_Language_Models.pdf\narxiv_paper_markdown: None\ncitations: 1017\nversions: 7\nid: 29\n-------------\nRow 2157:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/1907.07355\narxiv_title: Probing Neural Network Comprehension of Natural Language Arg...\narxiv_abstract: We are surprised to find that BERT's peak performance of 77%...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Probing_Neural_Network_Comprehension_of_Natural_Language_Arg...\narxiv_paper_markdown: None\ncitations: 488\nversions: 6\nid: 5013\n-------------\nRow 2158:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2303.12712\narxiv_title: Sparks of Artificial General Intelligence: Early experiments...\narxiv_abstract: Artificial intelligence (AI) researchers have been developin...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Sparks_of_Artificial_General_Intelligence_Early_experiments_...\narxiv_paper_markdown: None\ncitations: 1626\nversions: 2\nid: 8\n-------------\nRow 2159:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2109.01247\narxiv_title: Do Prompt-Based Models Really Understand the Meaning of thei...\narxiv_abstract: Recently, a boom of papers has shown extraordinary progress ...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Do_Prompt-Based_Models_Really_Understand_the_Meaning_of_thei...\narxiv_paper_markdown: None\ncitations: 237\nversions: 6\nid: 5009\n-------------\nRow 2160:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2302.00093\narxiv_title: Large Language Models Can Be Easily Distracted by Irrelevant...\narxiv_abstract: Large language models have achieved impressive performance o...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Large_Language_Models_Can_Be_Easily_Distracted_by_Irrelevant...\narxiv_paper_markdown: None\ncitations: 105\nversions: 6\nid: 5014\n-------------\nRow 2161:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.03439\narxiv_title: Evaluating the Logical Reasoning Ability of ChatGPT and GPT-...\narxiv_abstract: Harnessing logical reasoning ability is a comprehensive natu...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Evaluating_the_Logical_Reasoning_Ability_of_ChatGPT_and_GPT-...\narxiv_paper_markdown: None\ncitations: 120\nversions: 2\nid: 5004\n-------------\nRow 2162:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2201.07614\narxiv_title: Uncovering More Shallow Heuristics: Probing the Natural Lang...\narxiv_abstract: In this article, we explore the shallow heuristics used by t...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: Uncovering_More_Shallow_Heuristics_Probing_the_Natural_Langu...\narxiv_paper_markdown: None\ncitations: 3\nversions: 2\nid: 5012\n-------------\nRow 2163:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2304.05376\narxiv_title: ChemCrow: Augmenting large-language models with chemistry to...\narxiv_abstract: Over the last decades, excellent computational chemistry too...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: ChemCrow_Augmenting_large-language_models_with_chemistry_too...\narxiv_paper_markdown: None\ncitations: 113\nversions: 2\nid: 4997\n-------------\nRow 2164:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/2308.02828\narxiv_title: LLM is Like a Box of Chocolates: the Non-determinism of Chat...\narxiv_abstract: There has been a recent explosion of research on Large Langu...\narxiv_metadata: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://w...\narxiv_filename: LLM_is_Like_a_Box_of_Chocolates_the_Non-determinism_of_ChatG...\narxiv_paper_markdown: None\ncitations: 34\nversions: 2\nid: 5005\n-------------\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/06a75244-1893-4fa0-9f6d-029bfaec7984"},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"527df4672114483c8e3b15334d550cc0","deepnote_cell_type":"markdown"},"source":"Define database function to insert scraping results","block_group":"4636c78c977f43b7b48896a49fc9bb5f"},{"cell_type":"code","metadata":{"id":"WT_D0XfPXrYR","source_hash":"e913ff4a","execution_start":1710482893928,"execution_millis":591,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"3f7ccc20e42e42229ce39af07759da6b","deepnote_cell_type":"code"},"source":"def insert_scraping_results(url, html, status, query, title, snippet, job_id):    \n    conn = connection()\n    c = conn.cursor()\n    try:\n        # Check if the URL already exists in the table\n        c.execute('SELECT COUNT(*) FROM google_search_results WHERE url = %s', (url,))\n        count = c.fetchone()[0]\n\n        if count == 0:\n            # URL does not exist, insert new row with all fields including title and snippet\n            c.execute('''\n                INSERT INTO google_search_results (url, html, scraping_status, query, title, snippet, job_id)\n                VALUES (%s, %s, %s, %s, %s, %s, %s)\n            ''', (url, html, status, query, title, snippet, job_id))\n        else:\n            # URL exists, skip inserting\n            print(\"URL already exists in google_search_results. Skipping insert.\")\n\n        # Commit the transaction\n        conn.commit()\n\n    except Exception as e:\n        # Rollback the transaction if an error occurs\n        conn.rollback()\n        print(f\"An error occurred: {e}. Transaction was rolled back.\")\n    finally:\n        if conn:\n            conn.close()\n\n# Example usage\ninsert_scraping_results('https://www.example.com/top-llm-research-papers-2023/', '<html lang=\"en-US\"><head>..</html>', '200', 'example query', 'Example Title', 'This is an example snippet', '1')","block_group":"c4c4624363304a04932801a192ba3f05","execution_count":256,"outputs":[{"name":"stdout","text":"URL already exists in google_search_results. Skipping insert.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/99bf83cd-dc92-4a8a-a8e9-aa8d7d397720"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"eaaf737f5c4048258a60100fe87dde31","deepnote_cell_type":"text-cell-p"},"source":"define a database function to check for processed markdown","block_group":"5c894e5199a94f29a3984861b3de9b24"},{"cell_type":"code","metadata":{"source_hash":"1bb61aa3","execution_start":1710468032054,"execution_millis":628,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"8488807ff1454b9ab7065334b52b645d","deepnote_cell_type":"code"},"source":"def check_processed_markdown(url: str) -> bool:\n    conn = connection()\n    \"\"\"Check if the markdown for a given URL has already been processed.\"\"\"\n    c = conn.cursor()\n    c.execute(\"SELECT processed_markdown FROM google_search_results WHERE url = %s\", (url,))\n    result = c.fetchone()\n    if result and result[0]:\n        # If there's processed markdown, return True\n        return True\n    return False\n    if conn:\n        conn.close()\n\n# Example usage:\nurl = 'https://www.topbots.com/top-llm-research-papers-2023/'\ncheck_processed_markdown(url) ","block_group":"615a3e57d9ff43e6b6bcf28399282cac","execution_count":227,"outputs":[{"output_type":"execute_result","execution_count":227,"data":{"text/plain":"False"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/b6ab056d-8f3d-4e95-a95e-41b639e86234"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"c50c7d0f062044749a10a0ce12f60140","deepnote_cell_type":"text-cell-p"},"source":"define a database function to insert processed markdown","block_group":"97d4ceb2ae3f431199e3f7c3c91da9f3"},{"cell_type":"code","metadata":{"source_hash":"1ef6728b","execution_start":1710468032691,"execution_millis":591,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"8004aacb12f74d1cac30c562d5857c0b","deepnote_cell_type":"code"},"source":"def insert_processed_markdown(url: str, processed_markdown: dict):  # processed_markdown should be a dict based on usage\n    try:\n        conn = connection()\n        c = conn.cursor()\n        \n        # First, check if the URL exists in the database\n        c.execute('SELECT COUNT(*) FROM google_search_results WHERE url = %s', (url,))\n        url_exists = c.fetchone()[0]\n        \n        if url_exists:\n            # Convert processed_markdown to a JSON string\n            processed_markdown_str = json.dumps(processed_markdown, indent=4)  # Assuming processed_markdown is always a dict based on your usage\n            \n            # Update the row where the URL matches, setting the processed_markdown column\n            c.execute('''\n                UPDATE google_search_results \n                SET processed_markdown = %s \n                WHERE url = %s;\n            ''', (processed_markdown_str, url))\n            conn.commit()\n            print(f\"Processed markdown inserted successfully for URL: {url}\")\n        else:\n            print(f\"No entry found in the database for URL: {url}. Update skipped.\")\n        \n    except Exception as e:\n        print(f\"An error occurred while inserting processed markdown: {e}\")\n    finally:\n        if conn:\n            conn.close()\n            \nurl = 'https://test.url'\nprocessed_markdown = {\n    \"Function call\": \"Page\",\n    \"args\": {\n        \"title\": \"Top academic papers on LLMs\",\n        \"summary\": \"A list of academic papers and resources related to Large Language Models (LLMs) and their applications.\",\n        \"paragraphs\": [\n            {\n                \"paper_title\": \"Awesome-LLM-hallucination\",\n                \"content\": \"LLM hallucination paper list.\",\n                \"links\": [\n                    {\"url\": \"https://github.com/LuckyyySTA/Awesome-LLM-hallucination\"}\n                ]\n            },\n            {\n                \"paper_title\": \"awesome-hallucination-detection\",\n                \"content\": \"List of papers on hallucination detection in LLMs.\",\n                \"links\": [\n                    {\"url\": \"https://github.com/EdinburghNLP/awesome-hallucination-detection\"}\n                ]\n            },\n            {\n                \"paper_title\": \"LLMsPracticalGuide\",\n                \"content\": \"A curated (still actively updated) list of practical guide resources of LLMs\",\n                \"links\": [\n                    {\"url\": \"https://github.com/Mooler0410/LLMsPracticalGuide\"}\n                ]\n            },\n            # Add other papers here in the same format\n        ]\n    }\n}\n\ninsert_processed_markdown(url, processed_markdown)","block_group":"65dc6a72009e4a5c94b853fdac5b0d0d","execution_count":228,"outputs":[{"name":"stdout","text":"No entry found in the database for URL: https://test.url. Update skipped.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/6068eb5c-1e67-4a16-b1ab-185f991289dd"},{"cell_type":"markdown","metadata":{"id":"91Izl-tUsvYs","deepnote_app_block_visible":true,"cell_id":"289b7bd7049649fbad72fcaf3408d521","deepnote_cell_type":"markdown"},"source":"### Get metadata from arxiv for the paper","block_group":"a4e1182f717243be889675cfa4369496"},{"cell_type":"code","metadata":{"id":"QShBS-KWqTqD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"127f03ce-502d-4912-c123-b7b1129df144","source_hash":"356c3982","execution_start":1710468033290,"execution_millis":251,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"65fab026a41b438180be639a7ce83dee","deepnote_cell_type":"code"},"source":"import xml.etree.ElementTree as ET\n\ndef fetch_arxiv_paper_from_url(arxiv_url):\n    # Extract the arXiv ID from the provided URL\n    arxiv_id = arxiv_url.split('/')[-1]\n    # Ensure that .pdf is not part of the arXiv ID\n    arxiv_id = arxiv_id.replace('.pdf', '')  # Remove '.pdf' if it's part of the ID\n\n    print(\"Fetching information for arXiv ID:\", arxiv_id)\n\n    # Define the base URL for the arXiv API\n    base_url = 'http://export.arxiv.org/api/query?'\n    query_params = 'id_list={}&max_results=1'.format(arxiv_id)\n    final_url = base_url + query_params  # Construct the final URL\n    print(\"Final API Request URL:\", final_url)  # Debug: print the URL to be requested\n\n    # Make the request\n    response = requests.get(final_url)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        print(\"Raw XML response received\")\n        xml_data = response.text\n        root = ET.fromstring(xml_data)\n        ns = {'atom': 'http://www.w3.org/2005/Atom'}  # Namespace for parsing\n\n        # Extract paper details\n        link_element = root.find('.//atom:entry/atom:link[@rel=\"related\"]', ns)\n        if link_element is not None:\n            pdf_url = link_element.attrib['href']\n        else:\n            pdf_url = None\n        title = root.find('.//atom:entry/atom:title', ns).text.strip()\n        abstract = root.find('.//atom:entry/atom:summary', ns).text.strip()\n        published_date = root.find('.//atom:entry/atom:published', ns).text.strip()\n\n        # Extract authors\n        authors = [author.find('atom:name', ns).text for author in root.findall('.//atom:entry/atom:author', ns)]\n\n        # Generate a sanitized file name from the title\n        file_name = title.replace(':', '').replace(' ', '_') + '.pdf'\n\n        # Print extracted information for debugging\n        print(f\"PDF URL: {pdf_url}\")\n        print(f\"Title: {title}\")\n        print(f\"File Name: {file_name}\")\n        print(f\"Abstract: {abstract[:100]}...\" if len(abstract) > 100 else abstract)\n        print(f\"Published Date: {published_date}\")\n        print(f\"Authors: {', '.join(authors)}\")\n\n        # Return the collected information\n        return xml_data, pdf_url, title, file_name, abstract, published_date, authors\n    else:\n        print(\"Failed to fetch data from arXiv API. Status code:\", response.status_code)\n        return None, None, None, None, None, None, None\n\n# Example usage\narxiv_url = 'https://arxiv.org/abs/2302.13971'\nfetch_arxiv_paper_from_url(arxiv_url)","block_group":"3c4bf3e6017046bca5920f5fb49096e9","execution_count":229,"outputs":[{"name":"stdout","text":"Fetching information for arXiv ID: 2302.13971\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2302.13971&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2302.13971v1\nTitle: LLaMA: Open and Efficient Foundation Language Models\nFile Name: LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf\nAbstract: We introduce LLaMA, a collection of foundation language models ranging from\n7B to 65B parameters. We...\nPublished Date: 2023-02-27T17:11:15Z\nAuthors: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample\n","output_type":"stream"},{"output_type":"execute_result","execution_count":229,"data":{"text/plain":"('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2302.13971%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2302.13971&amp;start=0&amp;max_results=1</title>\\n  <id>http://arxiv.org/api/qJuhZNxbRqWajNrNkNtkRSmyBuQ</id>\\n  <updated>2024-03-14T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2302.13971v1</id>\\n    <updated>2023-02-27T17:11:15Z</updated>\\n    <published>2023-02-27T17:11:15Z</published>\\n    <title>LLaMA: Open and Efficient Foundation Language Models</title>\\n    <summary>  We introduce LLaMA, a collection of foundation language models ranging from\\n7B to 65B parameters. We train our models on trillions of tokens, and show that\\nit is possible to train state-of-the-art models using publicly available\\ndatasets exclusively, without resorting to proprietary and inaccessible\\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\\nPaLM-540B. We release all our models to the research community.\\n</summary>\\n    <author>\\n      <name>Hugo Touvron</name>\\n    </author>\\n    <author>\\n      <name>Thibaut Lavril</name>\\n    </author>\\n    <author>\\n      <name>Gautier Izacard</name>\\n    </author>\\n    <author>\\n      <name>Xavier Martinet</name>\\n    </author>\\n    <author>\\n      <name>Marie-Anne Lachaux</name>\\n    </author>\\n    <author>\\n      <name>Timothée Lacroix</name>\\n    </author>\\n    <author>\\n      <name>Baptiste Rozière</name>\\n    </author>\\n    <author>\\n      <name>Naman Goyal</name>\\n    </author>\\n    <author>\\n      <name>Eric Hambro</name>\\n    </author>\\n    <author>\\n      <name>Faisal Azhar</name>\\n    </author>\\n    <author>\\n      <name>Aurelien Rodriguez</name>\\n    </author>\\n    <author>\\n      <name>Armand Joulin</name>\\n    </author>\\n    <author>\\n      <name>Edouard Grave</name>\\n    </author>\\n    <author>\\n      <name>Guillaume Lample</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2302.13971v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2302.13971v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n',\n 'http://arxiv.org/pdf/2302.13971v1',\n 'LLaMA: Open and Efficient Foundation Language Models',\n 'LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf',\n 'We introduce LLaMA, a collection of foundation language models ranging from\\n7B to 65B parameters. We train our models on trillions of tokens, and show that\\nit is possible to train state-of-the-art models using publicly available\\ndatasets exclusively, without resorting to proprietary and inaccessible\\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\\nPaLM-540B. We release all our models to the research community.',\n '2023-02-27T17:11:15Z',\n ['Hugo Touvron',\n  'Thibaut Lavril',\n  'Gautier Izacard',\n  'Xavier Martinet',\n  'Marie-Anne Lachaux',\n  'Timothée Lacroix',\n  'Baptiste Rozière',\n  'Naman Goyal',\n  'Eric Hambro',\n  'Faisal Azhar',\n  'Aurelien Rodriguez',\n  'Armand Joulin',\n  'Edouard Grave',\n  'Guillaume Lample'])"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/287d1deb-5f36-4c80-bbea-b8cdd104d891"},{"cell_type":"markdown","metadata":{"id":"i9A5UfGBstA0","deepnote_app_block_visible":true,"cell_id":"82738a7559f34ccdad8b031ba05fd7d5","deepnote_cell_type":"markdown"},"source":"### Download pdf of the paper","block_group":"9329080b7fb34539a821e604c0875e0c"},{"cell_type":"code","metadata":{"id":"fZwslypIsf-4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"01af4fd0-2b43-4886-e0ef-eac8ed71e1cf","source_hash":"de2044b4","execution_start":1710468033542,"execution_millis":177,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"2f493d40fd38496d90c90a57c8064805","deepnote_cell_type":"code"},"source":"def download_pdf(pdf_url, file_name):\n    # Create the \"papers\" directory if it doesn't exist\n    papers_dir = \"papers\"\n    if not os.path.exists(papers_dir):\n        os.makedirs(papers_dir)\n\n    # Construct the full file path\n    file_path = os.path.join(papers_dir, file_name)\n\n    # Check if the file already exists\n    if os.path.exists(file_path):\n        print(\"The paper already exists.\")\n        return file_path  # Return the file path\n\n    # Send a GET request to download the PDF\n    response = requests.get(pdf_url)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        # Write the PDF content to the file\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n        print(\"The paper has been downloaded successfully.\")\n        return file_path  # Return the file path\n    else:\n        # Return a status error message\n        error_message = f\"Failed to download the paper. Status code: {response.status_code}\"\n        return error_message\n\n# Example usage\ndownload_pdf('http://arxiv.org/pdf/2302.13971v1', 'LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf')\n\n","block_group":"221abd5c1c544a869145d9f714468673","execution_count":230,"outputs":[{"name":"stdout","text":"The paper already exists.\n","output_type":"stream"},{"output_type":"execute_result","execution_count":230,"data":{"text/plain":"'papers/LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf'"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/4c8965c8-5a81-4fac-abe2-cb130855d325"},{"cell_type":"markdown","metadata":{"id":"NlHg6Z20sqKH","deepnote_app_block_visible":true,"cell_id":"b7a1786b345c43c99f2b71a25a97c965","deepnote_cell_type":"markdown"},"source":"### Convert pdf into markdown","block_group":"a23ae86f5c844865af4511949a641b93"},{"cell_type":"code","metadata":{"id":"PCakTs60tTH1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ba2f97f5-42ce-4182-ea5e-4ebb15f13246","source_hash":"55b486db","execution_start":1710468033743,"execution_millis":56440,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"f53abd56a6894ade81db76791227ce9f","deepnote_cell_type":"code"},"source":"import nest_asyncio\nfrom llama_parse import LlamaParse\n\n# This function will convert a given PDF file to Markdown format using LlamaParse\ndef convert_pdf_to_markdown(file_name):\n    # Necessary for running async code in notebooks or scripts\n    nest_asyncio.apply()\n\n    # Initialize the LlamaParse parser\n    parser = LlamaParse(\n        api_key=llamaindex_api_key,\n        result_type=\"markdown\",  # Choose \"markdown\" as the output format\n        verbose=True,  # Enable verbose output to see detailed logs\n    )\n    \n    # Define the path to your PDF file\n    pdf_file_path = os.path.join(\"./papers/\", file_name)\n    print(pdf_file_path, \"type:\", type(pdf_file_path))\n    # Convert the PDF to Markdown\n    # This is a synchronous call, you can also use asynchronous calls as shown in the documentation\n    documents = parser.load_data(pdf_file_path)\n\n    # Return the converted documents\n    return documents\n\n# Define the path to your PDF file\nfile_name = \"Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks.pdf\"\ndocuments = convert_pdf_to_markdown(file_name)","block_group":"aa9b48dc38d0416486e6dd513632f311","execution_count":231,"outputs":[{"name":"stdout","text":"./papers/Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks.pdf type: <class 'str'>\nStarted parsing the file under job_id 9fafeba8-b5e0-49ca-9bc2-adc90b1c5c76\n....","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/978982b5-87c9-4e58-ae2b-64220ba64c86"},{"cell_type":"code","metadata":{"id":"nTLmIr8AwpmG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"af063990-1cfb-4b52-c74a-9294984976fd","source_hash":"8d5f8090","execution_start":1710468090185,"execution_millis":331,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"c0e822e8b52e437c941781cb6af4050d","deepnote_cell_type":"code"},"source":"markdown_content = None\nif documents:\n    # Assuming the first document contains the content\n    # Use the get_text() method to retrieve the Markdown content\n    markdown_content = documents[0].get_text()\n    print(markdown_content)\n\n    # Optionally, write the markdown content to a file\n    with open('converted_markdown.md', 'w', encoding='utf-8') as markdown_file:\n        markdown_file.write(markdown_content)","block_group":"3d6ca6c2ca164b4c92462bdbaaded69e","execution_count":232,"outputs":[{"name":"stdout","text":"## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\n|Authors|Patrick Lewis†‡, Ethan Perez⋆, Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†, Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†|\n|---|---|\n|Affiliations|†Facebook AI Research; ‡University College London; ⋆New York University;|\n|Email|plewis@fb.com|\n\nAbstract\n\nLarge pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\n\n### Introduction\n\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data [47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52]. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results.\n---\n## Define \"middle ear\"(x)\n\nThe middle ear includes the tympanic cavity and the three ossicles.\n\n## Question Answering:\n\n|Question Query|Query|Retriever p|η|Document|Generator p|θ|\n|---|---|---|---|---|---|---|\n|Barack Obama was born in Hawaii.(x)|Encoder|(Non-Parametric)| |Index|(Parametric)|Answer Generation|\n\nSupports (y)\n\n## Fact Verification: Fact Query\n\nThe Divine Comedy (x)\n\n## Jeopardy Question Generation:\n\nAnswer Query\n\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to find the top-K documents zi. For final prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents. but have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned. There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stack-augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training. Our results highlight the benefits of combining parametric and non-parametric memory with generation for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER [56] fact verification, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models’ knowledge as the world changes.\n\n## Methods\n\nWe explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pη(z|x) with parameters η that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pθ(yi|x, z, y1:i−1) parametrized\n\n1 Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transformers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\n---\n## by θ that generates a current token based on a context of the previous i − 1 tokens y1:i−1, the original input x and a retrieved passage z.\n\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pη and pθ components, as well as the training and decoding procedure.\n\n## Models\n\n|RAG-Sequence Model|The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,|\n|---|---|\n| |pRAG-Sequence(y|x) ≈ pη(z|x)pθ(y|x, z) = pη(z|x) Σ pθ(yi|x, z, y1:i−1) z∈top-k(p(·|x)) z∈top-k(p(·|x)) i|\n\n## RAG-Token Model\n\nIn the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we define:\n\npRAG-Token(y|x) ≈ Σ z∈top-k(p(·|x)) pη(z|x)pθ(yi|x, z, y1:i−1)\n\nFinally, we note that RAG can be used for sequence classification tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\n\n## Retriever: DPR\n\nThe retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:\n\npη(z|x) ∝ exp d(z)⊤q(x) d(z) = BERTd(z), q(x) = BERTq(x)\n\nwhere d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pη(·|x)), the list of k documents z with highest prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory.\n\n## Generator: BART\n\nThe generator component pθ(yi|x, z, y1:i−1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters θ as the parametric memory henceforth.\n\n## Training\n\nWe jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a fine-tuning training corpus of input/output pairs (xj, yj), we\n---\nminimize the negative marginal log-likelihood of each target, j −log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder BERTq and the BART generator.\n\n## Decoding\n\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate arg max y p(y|x).\n\n|RAG-Token|The RAG-Token model can be seen as a standard, autoregressive seq2seq generator with transition probability: p′ θ(yi|x, y1:i−1) = z∈top-k(p(·|x)) pη(zi|x)pθ(yi|x, zi, y1:i−1) To decode, we can plug p′ θ(yi|x, y1:i−1) into a standard beam decoder.|\n|---|---|\n|RAG-Sequence|For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pθ(yi|x, z, y1:i−1). This yields a set of hypotheses Y, some of which may not have appeared in the beams of all documents. To estimate the probability of a hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer output sequences, |Y| can become large, requiring many forward passes. For more efficient decoding, we can make a further approximation that pθ(y|x, zi) ≈ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as “Fast Decoding.”|\n\n## Experiments\n\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k ∈ {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task.\n\n### Open-domain Question Answering\n\nOpen-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book QA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\n\n### Abstractive Question Answering\n\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat\n---\nMSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as \"What is the weather in Volcano, CA?\" so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses.\n\nJeopardy Question Generation\n\nTo evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question generation. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, \"The World Cup\" is the answer to the question \"In 1986 Mexico scored as the first country to host this international sports competition twice.\" As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task.\n\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for specificity. We define factuality as whether a statement can be corroborated by trusted external sources, and specificity as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options—question A is better, question B is better, both are good, or neither is good.\n\nFact Verification\n\nFEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unverifiable from Wikipedia alone. FEVER is a retrieval problem coupled with a challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models’ ability to handle classification rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\n\n## Results\n\n### Open-domain Question Answering\n\n|Task|RAG|State-of-the-art models|\n|---|---|---|\n|All four open-domain QA tasks|RAG sets a new state of the art (only on the T5-comparable split for TQA)|RAG combines the generation flexibility of the “closed-book” (parametric only) approaches and the performance of \"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance.|\n\nThere are several advantages to generating answers even when it is possible to extract them. Documents with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading\n---\n|Content|Page Number|\n|---|---|\n|Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open-Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details.| |\n|Table 2: Generation and classification Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined.| |\n\n## Model NQ TQA WQ CT\n\n|Model|NQ|TQA|WQ|CT|\n|---|---|---|---|---|\n|Closed T5-11B [52]|34.5|- /50.1|37.4|-|\n|Book T5-11B+SSM[52]|36.6|- /60.5|44.7|-|\n|Open REALM [20]|40.4|- / -|40.7|46.8|\n|Book DPR [26]|41.5|57.9/ -|41.1|50.6|\n|RAG-Token|44.1|55.2/66.1|45.5|50.0|\n|RAG-Seq.|44.5|56.8/68.0|45.2|52.2|\n\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%.\n\n## Abstractive Question Answering\n\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with specific information required to generate the reference answer, (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we find that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5).\n\n## Jeopardy Question Generation\n\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also find RAG generations to be more specific by a large margin. Table 3 shows typical generations from each model.\n---\n## Document 1\n\nHis works are considered classics of American literature ... His wartime experiences formed the basis for his novel \"A Farewell to Arms\" (1929) ...\n\n## Document 2\n\n... artists of the 1920s \"Lost Generation\" expatriate community. His debut novel, \"The Sun Also Rises\", was published in 1926.\n\nFigure 2: RAG-Token document posterior p(zi|x, yi, y−i) for each generated token for input “Hemingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\n\n|Task|Input|Model|Generation|\n|---|---|---|---|\n|define middle ear|BART|?|The middle ear is the part of the ear between the middle ear and the nose.|\n| |RAG-T| |The middle ear is the portion of the ear internal to the eardrum.|\n|MS-MARCO|what currency needed in Scotland|BART|The currency needed in Scotland is Pound sterling.|\n| |RAG-T| |Pound is the currency needed in Scotland.|\n| |RAG-S| |The currency needed in Scotland is the pound sterling.|\n| |BART|?|This state has the largest number of counties in the U.S.|\n|Jeopardy Question Generation|Washington|RAG-T|It’s the only U.S. state named for a U.S. president|\n| |RAG-S| |It’s the state where you’ll find Mount Rainier National Park|\n| |BART|*|This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio|\n|Generation|The Divine Comedy|RAG-T|Dante’s \"Inferno\" is the first part of this epic poem|\n| |RAG-S| |This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"|\n\nFor 2-way classification, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved by RAG and gold evidence annotations. We find that the top retrieved document is from a gold article in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n\n## Additional Results\n\nGeneration Diversity: Section 4.3 shows that RAG models are more factual and specific than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding, we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are more diverse than RAG-Token’s, and both are significantly more diverse than BART without needing any diversity-promoting decoding.\n\nRetrieval Ablations: A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks.\n\nWe compare RAG’s dense retriever to a word overlap-based BM25 retriever. Here, we replace RAG’s retriever with a fixed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.\n\nIndex hot-swapping: An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed\n---\n## Table 4: Human assessments for the Jeopardy Question Generation Task\n\n| |Factuality|Specificity|\n|---|---|---|\n|BART better|7.1%|16.8%|\n|RAG better|42.7%|37.4%|\n|Both good|11.7%|11.8%|\n|Both poor|17.7%|6.9%|\n|No majority|20.8%|20.1%|\n\n## Table 5: Ratio of distinct to total tri-grams for generation tasks\n\n| |MSMARCO|Jeopardy QGen|\n|---|---|---|\n|Gold|89.6%|90.0%|\n|BART|70.7%|32.4%|\n|RAG-Token|77.8%|46.8%|\n|RAG-Seq.|83.5%|53.8%|\n\n## Table 6: Ablations on the dev set\n\n|Model|NQ|TQA|WQ|CT|Jeopardy-QGen|MSMarco|FVR-3|FVR-2|\n|---|---|---|---|---|---|---|---|---|\n|RAG-Token-BM25|29.7|41.5|32.1|33.1|17.5|22.3|55.5|48.4|75.1|91.6|\n|RAG-Sequence-BM25|31.8|44.1|36.6|33.8|11.1|19.5|56.5|46.9|\n|RAG-Token-Frozen|37.8|50.1|37.1|51.1|16.7|21.7|55.9|49.4|72.9|89.4|\n|RAG-Sequence-Frozen|41.2|52.1|41.8|52.6|11.8|19.6|56.7|47.3|\n|RAG-Token|43.5|54.8|46.5|51.9|17.9|22.6|56.2|49.4|74.5|90.6|\n|RAG-Sequence|44.0|55.8|44.9|53.4|15.3|21.5|57.2|47.5|\n\nBetween these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”) to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for 2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders). This shows we can update RAG’s world knowledge by simply replacing its non-parametric memory.\n\nEffect of Retrieving more documents: Models are trained with either 5 or 10 retrieved latent documents, and we do not observe significant differences in performance between them. We have the flexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\n\n|44|80|Bleu-1 / Rouge-L score|\n|---|---|---|\n|NQ Answer Recall @ K| | |\n\nNQ Exact Match\n\n| |10|20|30|40|50|10|20|30|40|50|10|20|30|40|50|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|K Retrieved Docs| | | | | | | | | | | | | | | |\n\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\n\n## Related Work\n\nSingle-Task Retrieval: Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering, fact checking, fact completion, long-form question answering, Wikipedia article generation, dialogue, translation, and language modeling. Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.\n---\n## General-Purpose Architectures for NLP\n\nPrior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks [60, 61] after fine-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models.\n\n## Learned Retrieval\n\nThere is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.\n\n## Memory-based Architectures\n\nOur document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval [9].\n\n## Retrieve-and-Edit approaches\n\nOur method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a final output. These approaches have proved successful in a number of domains including Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.\n\n## Discussion\n\nIn this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG’s generation over purely parametric BART, finding RAG more factual and specific. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.\n---\n## Broader Impact\n\nThis work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.\n\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [54]. Advanced language models may also lead to the automation of various jobs in the coming decades [16]. In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing.\n\n## Acknowledgments\n\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.\n\n## References\n\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http://arxiv.org/abs/1611.09268. arXiv: 1611.09268.\n[2] Petr Baudiš and Jan Šediv` y. Modeling of pe question answering task in pe yodaqa system. In International Conference of pe Cross-Language Evaluation Forum for European Languages, pages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20.\n[3] Jonapan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of pe 2013 Conference on Empirical Mepods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anpology/D13-1160.\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-ing&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159, 2020. URL https://arxiv.org/abs/2004.07159.\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anpology/P17-1171.\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonapan Berant. Coarse-to-fine question answering for long documents. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anpology/P17-1020.\n---\nChristopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Comprehension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723. arXiv: 1710.10723.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of pe 2019 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anpology/N19-1423.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\nMatpew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A New Q&A Dataset Augmented wip Context from a Search Engine. arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv: 1704.05179.\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of pe 56p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anpology/P18-1082.\nAngela Fan, Yacine Jernite, Epan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of pe 57p Annual Meeting of pe Association for Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/anpology/P19-1346.\nAngela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers wip KNN-based composite memory, 2020. URL https://openreview.net/forum?id=H1gx1CNKPH.\nThibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access wip entity supervision. ArXiv, abs/2004.07202, 2020. URL https://arxiv.org/abs/2004.07202.\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wentau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710.\nKatja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL http://arxiv.org/abs/1705.08807.\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018. 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018.\nKelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of pe Association for Computational Linguistics, 6:437–450, 2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anpology/Q18-1031.\n---\n## References\n\n|[20]|Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https://arxiv.org/abs/2002.08909.|\n|---|---|\n|[21]|Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 10052–10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf.|\n|[22]|Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-edit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2532–2538, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/anthology/2020.acl-main.228.|\n|[23]|Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.|\n|[24]|Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.|\n|[25]|Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, page 190–198, Cambridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.|\n|[26]|Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.|\n|[27]|Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.|\n|[28]|Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.|\n|[29]|Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. Transactions of the Association of Computational Linguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf.|\n|[30]|Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf.|\n|[31]|Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association|\n---\n## References\n\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\n\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110–119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/N16-1014.\n\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL https://arxiv.org/abs/1909.03087.\n\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation with joint textual and phonetic embedding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3044–3049, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL https://www.aclweb.org/anthology/P19-1291.\n\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hyg0vbWC-.\n\n[37] Yury A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320.\n\n[38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\n\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the verifiability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https://arxiv.org/abs/1911.03587.\n\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\n\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploiting background knowledge for building conversation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.\n\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/anthology/D18-1429.\n\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic. 13\n---\napproaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf.\n\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\n\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.org/anthology/N19-4009.\n\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\n\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://www.aclweb.org/anthology/D19-1250.\n\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models’ factual predictions. In Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum?id=025X0zPfn.\n\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training, 2018. URL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf.\n\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\n\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\n\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/2002.08910.\n\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL https://doi.org/10.1561/1500000019.\n\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019.\n\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\n---\n# References\n\n[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of pe 2018 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://www.aclweb.org/anpology/N18-1074.\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classification wip elastic weight consolidation. ArXiv, abs/2004.14366, 2020. URL https://arxiv.org/abs/2004.14366.\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanapan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaap Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329.\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of pe 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anpology/W18-5446.\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https://arxiv.org/abs/1905.00537.\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain question answering. In Sheila A. McIlraip and Kilian Q. Weinberger, editors, Proceedings of pe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), pe 30p innovative Applications of Artificial Intelligence (IAAI-18), and pe 8p AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712.\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in open-domain question answering. In ICLR, 2018. URL https://openreview.net/forum?id=rJl3yM-Ab.\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and refine: Improved sequence generation models for dialogue. In Proceedings of pe 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anpology/W18-5713.\n---\n## References\n\n[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anpony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers: State-of-pe-art natural language processing. ArXiv, abs/1910.03771, 2019.\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-supervised question answering. In Proceedings of pe 2019 Conference on Empirical Mepods in Natural Language Processing and pe 9p International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2495–2509, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anpology/D19-1253.\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019. URL https://arxiv.org/abs/1909.03745.\n---\n## Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\n### Implementation Details\n\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not find beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\n\n### Human Evaluation\n\n|Which sentence is more factually true?|Select option|\n|---|---|\n|Noje: Scna Guesucn?|snterzt \"The8r Nuso Rists|\n|IncicateFich|Farcncllic AM; on Im|\n|Iclbwng sentarces Is Mca luclualy Injb[ealecllo|ZLbko Uaino|\n| |cnoc urdoco|\n\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \"view tool guide\".\n\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instructions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.\n\n### Training Setup Details\n\nWe train all RAG models and BART baselines using Fairseq. We train with mixed precision floating point arithmetic, distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We find that doing Maximum Inner Product Search with FAISS is sufficiently fast on CPU, so we store document index vectors on CPU, requiring ∼ 100 GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers, which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS’s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/\n\n2. https://github.com/pytorch/fairseq\n\n3. https://github.com/huggingface/transformers\n---\n## Further Details on Open-Domain QA\n\nFor open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to find matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we filter out answer candidates if they do not occur in top 1000 documents for the query.\n\n## CuratedTrec preprocessing\n\nThe answers for CuratedTrec are given in the form of regular expressions, which has been suggested as a reason why it is unsuitable for answer-generation models [20]. To overcome this, we use a pre-processing step where we first retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\n\n## TriviaQA Evaluation setups\n\nThe open-domain QA community customarily uses public development datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading comprehension purposes. We report our results using the datasets splits used in DPR [26], which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA official Wikipedia test set instead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See appendix of [14]). We report results on both test sets to enable fair comparison to both approaches. We find that our performance is much higher using the official Wiki test set, rather than the more conventional open-domain test set, which we attribute to the official Wiki test set questions being simpler to answer from Wikipedia.\n\n## Further Details on FEVER\n\nFor FEVER classification, we follow the practice from [32], and first re-generate the claim, and then classify using the representation of the final hidden state, before finally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The first is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task we explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classification prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work.\n\n## Null Document Probabilities\n\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order to model cases where no useful information could be retrieved for a given input. Here, if k documents were retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null document, before marginalizing over k + 1 predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not find that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to benefit from retrieval, suggesting that null document mechanisms may not be necessary for RAG.\n\n## Parameters\n\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable.\n---\n|Task|Train|Development|Test|\n|---|---|---|---|\n|Natural Questions|79169|8758|3611|\n|TriviaQA|78786|8838|11314|\n|WebQuestions|3418|362|2033|\n|CuratedTrec|635|134|635|\n|Jeopardy Question Generation|97392|13714|26849|\n|MS-MARCO|153726|12468|101093*|\n|FEVER-3-way|145450|10000|10000|\n|FEVER-2-way|96966|6666|6666|\n\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit floating\npoint precision to manage memory and disk footprints.\n\n## Retrieval Collapse\n\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the\nretrieval component would “collapse” and learn to retrieve the same documents regardless of the\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\nin less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\n\n## Number of instances per dataset\n\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/1dca681f-d8ed-4011-a573-037ebe24705c"},{"cell_type":"markdown","metadata":{"id":"3tM61elq3hrj","deepnote_app_block_visible":true,"cell_id":"17ffcf9815084e5fa2002a2270ba5de0","deepnote_cell_type":"markdown"},"source":"### Get citations and number of versions from Google Scholar","block_group":"b8e3e9beee96482c8a18da4d133a0393"},{"cell_type":"code","metadata":{"id":"mwuv2gRw3mv-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bdfaf190-a448-4bc6-9bd7-5ae42a5e41dd","source_hash":"6bb279e6","execution_start":1710468090523,"execution_millis":1226,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"3cbeef9dc096403499243ad238f9194e","deepnote_cell_type":"code"},"source":"from serpapi import GoogleSearch\n\ndef get_scholar_citations_versions(query_url):\n    params = {\n        \"api_key\": serp_api_key,  # Ensure serp_api_key is defined elsewhere\n        \"engine\": \"google_scholar\",\n        \"q\": query_url,\n        \"hl\": \"en\"\n    }\n\n    search = GoogleSearch(params)\n    results = search.get_dict()\n\n    # Initialize the return values\n    number_of_citations = None\n    number_of_versions = None\n\n    # Extracting number of citations and versions\n    if 'organic_results' in results:\n        if 'inline_links' in results['organic_results'][0]:\n            if 'cited_by' in results['organic_results'][0]['inline_links']:\n                number_of_citations = results[\"organic_results\"][0][\"inline_links\"][\"cited_by\"][\"total\"]\n\n            if 'versions' in results['organic_results'][0]['inline_links']:\n                number_of_versions = results[\"organic_results\"][0][\"inline_links\"][\"versions\"][\"total\"]\n\n    return number_of_citations, number_of_versions\n\nquery_url = 'https://arxiv.org/abs/2302.13971'\ncitations, versions = get_scholar_citations_versions(query_url)\nprint(\"Number of citations:\", citations)\nprint(\"Number of versions:\", versions)","block_group":"64707be5b9684185b05d52d700f0ecc3","execution_count":233,"outputs":[{"name":"stdout","text":"Number of citations: 4477\nNumber of versions: 13\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/b45aad8b-3f92-4462-949d-ecd0693fec90"},{"cell_type":"markdown","metadata":{"id":"u9eHATZLi5ZV","deepnote_app_block_visible":true,"cell_id":"7db90d63c5ee477fadc3fcaefa116c5e","deepnote_cell_type":"markdown"},"source":"### Gemini summary and relevance score","block_group":"303d9cd81b044716802ee7b71c614f28"},{"cell_type":"markdown","metadata":{"id":"-gQldNXOVd68","deepnote_app_block_visible":true,"cell_id":"8bd63746631d4fd888104de5728f7af1","deepnote_cell_type":"markdown"},"source":"Gemini Set up","block_group":"faa953eb4647427db8a2f9cea784cc67"},{"cell_type":"code","metadata":{"id":"GWLjB54KVgGb","colab":{"height":332,"base_uri":"https://localhost:8080/"},"outputId":"8b5b4290-7941-4489-9bab-6149fac379e6","source_hash":"c3b85edd","execution_start":1710468091753,"execution_millis":316,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"40c54655224746b7a4fd57cdcbaaa818","deepnote_cell_type":"code"},"source":"import google.generativeai as genai\ngenai.configure(api_key=gemini_api_key)\nmodel = genai.GenerativeModel('gemini-1.0-pro')","block_group":"8b3b32ec98e14047b104f2506295ba54","execution_count":234,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"d991cab6","execution_start":1710468091755,"execution_millis":314,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"4241ee38055245fab054980baf5462bc","deepnote_cell_type":"code"},"source":"model","block_group":"4a0d5e23b54a431a82ccbb0d4aca39b9","execution_count":235,"outputs":[{"output_type":"execute_result","execution_count":235,"data":{"text/plain":"genai.GenerativeModel(\n    model_name='models/gemini-1.0-pro',\n    generation_config={},\n    safety_settings={},\n    tools=None,\n)"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/d13c5897-4f52-4530-bb8a-8a0c20fdbd82"},{"cell_type":"markdown","metadata":{"id":"nvyJkedJU2Pv","deepnote_app_block_visible":true,"cell_id":"e94a45f53692417d941389ae227e4b3e","deepnote_cell_type":"markdown"},"source":"Given `arxiv` structure, summarize and evaluate against user prompt. Give a heuritic score.","block_group":"fa2a223bf8034417a6d1fa9e5f8c8f52"},{"cell_type":"code","metadata":{"id":"SoqeHjBAihAD","source_hash":"389c1e7","execution_start":1710468091772,"execution_millis":297,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"bbdda948ae7447d59b2ab4dcbda5affb","deepnote_cell_type":"code"},"source":"import re\nimport json\n\ndef process_arxiv(mkdn, metdata, query):\n  # 1 - `arxiv` dict\n  def extract_markdown(markdown_text, pattern):\n    # Use re.findall to find all matches of the pattern in the markdown text\n    matches = re.findall(pattern, markdown_text, re.MULTILINE)\n\n    # Return the first match (if any)\n    if matches:\n        return matches[0]\n    else:\n        return None\n\n  paper_title = extract_markdown(mkdn, r'^##\\s+(.*)$')\n  if paper_title is None:\n    print(\"extract_markdown for paper_title isn't working\")\n\n  abstract = extract_markdown(mkdn, r'^Abstract(.*)#')\n  if abstract is None:\n    print(\"extract_markdown for abstract isn't working...hardcoding the abstract instead\")\n    abstract = '''We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.'''\n\n  arxiv = {'paper_title': paper_title, 'abstract': abstract, 'metadata': metadata, 'paper': mkdn}\n\n  # 2 - Summarizer\n  prompt = '''Please summarize the following paper in one sentence given the user query \"{query}\". The paper is provided in a structured format {paper_format} \\n\\nDocument: {document}'''.format(query=query, document=arxiv, paper_format={key: \"\" for key in arxiv.keys()})\n  print(prompt, \"\\nGenerating summarization............\")\n\n  if model.count_tokens(prompt).total_tokens > 28_000:\n    print(\"The prompt is too long, visiting https://aistudio.google.com/app/prompts/new_freeform to manually use Gemini 1.5 pro instead with the prompt above.\")\n  relevant_answer = model.generate_content(prompt).text\n\n  print(relevant_answer)\n\n  # 3 - Relevance scorer\n  prompt = '''From a scale of 1 to 5, rate how relevant the following paper is with the user query \"{query}\". The paper is provided in a structured format {paper_format}. Please provide the score in the format of a json object with one key, 'score'. Example: {{\"score\": 5}}. Also please provide reasoning why it doesn't have a higher or lower relevance score. \\n\\nDocument: {document}'''.format(query=query, document=arxiv, paper_format={key: \"\" for key in arxiv.keys()})\n  print(prompt, \"\\nGenerating............\")\n\n  if model.count_tokens(prompt).total_tokens > 28_000:\n    print(\"The prompt is too long, visiting https://aistudio.google.com/app/prompts/new_freeform to manually use Gemini 1.5 pro instead with the prompt above.\")\n\n  model_response = model.generate_content(prompt).text\n\n  re_match = re.search(r'\"score\": (\\d+)', model_response)\n  relevance_score = re_match.group(1)\n\n  print(\"relevance score: \" + relevance_score)\n\n  return {\n      'relevance_score': relevance_score,\n      'relevant_answer': relevant_answer\n  }\n\n  query\n\n  #@title `mkdn` and `metadata`\nmetadata = markdown_content #right now it's just the entire paper pdf\n\n#@title Extractors to process `mkdn` and `metadata` into `arxiv` dict\n\nimport re\n\ndef extract_markdown(markdown_text, pattern):\n  # Use re.findall to find all matches of the pattern in the markdown text\n  matches = re.findall(pattern, markdown_text, re.MULTILINE)\n\n  # Return the first match (if any)\n  if matches:\n      return matches[0]\n  else:\n      return None\n\npaper_title = extract_markdown(metadata, r'^##\\s+(.*)$')\nif paper_title is None:\n  print(\"extract_markdown for paper_title isn't working\")\n\nabstract = extract_markdown(metadata, r'^Abstract(.*)#')\nif abstract is None:\n  print(\"extract_markdown for abstract isn't working...hardcoding the abstract instead\")\n  abstract = '''We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.'''\n\narxiv = {'paper_title': paper_title, 'abstract': abstract, 'metadata': metadata, 'paper': metadata}\n\n\n\nimport json\n\n# Convert to JSON string with indentation for readability\npretty_arxiv_output = json.dumps(arxiv, indent=4, default=str)\n\n# Print with added line breaks\nprint(\"\\narxiv=\",)\nprint(pretty_arxiv_output)","block_group":"e3a2b46fdc674e5aa372ece84f9f2082","execution_count":236,"outputs":[{"name":"stdout","text":"extract_markdown for abstract isn't working...hardcoding the abstract instead\n\narxiv=\n{\n    \"paper_title\": \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\",\n    \"abstract\": \"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.\",\n    \"metadata\": \"## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\n|Authors|Patrick Lewis\\u2020\\u2021, Ethan Perez\\u22c6, Aleksandra Piktus\\u2020, Fabio Petroni\\u2020, Vladimir Karpukhin\\u2020, Naman Goyal\\u2020, Heinrich K\\u00fcttler\\u2020, Mike Lewis\\u2020, Wen-tau Yih\\u2020, Tim Rockt\\u00e4schel\\u2020\\u2021, Sebastian Riedel\\u2020\\u2021, Douwe Kiela\\u2020|\\n|---|---|\\n|Affiliations|\\u2020Facebook AI Research; \\u2021University College London; \\u22c6New York University;|\\n|Email|plewis@fb.com|\\n\\nAbstract\\n\\nLarge pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \\u2014 models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\\n\\n### Introduction\\n\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data [47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52]. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can\\u2019t straightforwardly provide insight into their predictions, and may produce \\u201challucinations\\u201d [38]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results.\\n---\\n## Define \\\"middle ear\\\"(x)\\n\\nThe middle ear includes the tympanic cavity and the three ossicles.\\n\\n## Question Answering:\\n\\n|Question Query|Query|Retriever p|\\u03b7|Document|Generator p|\\u03b8|\\n|---|---|---|---|---|---|---|\\n|Barack Obama was born in Hawaii.(x)|Encoder|(Non-Parametric)| |Index|(Parametric)|Answer Generation|\\n\\nSupports (y)\\n\\n## Fact Verification: Fact Query\\n\\nThe Divine Comedy (x)\\n\\n## Jeopardy Question Generation:\\n\\nAnswer Query\\n\\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to find the top-K documents zi. For final prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents. but have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the \\u201cworkhorse of NLP,\\u201d i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned. There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stack-augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training. Our results highlight the benefits of combining parametric and non-parametric memory with generation for knowledge-intensive tasks\\u2014tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER [56] fact verification, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models\\u2019 knowledge as the world changes.\\n\\n## Methods\\n\\nWe explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever p\\u03b7(z|x) with parameters \\u03b7 that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator p\\u03b8(yi|x, z, y1:i\\u22121) parametrized\\n\\n1 Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transformers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n---\\n## by \\u03b8 that generates a current token based on a context of the previous i \\u2212 1 tokens y1:i\\u22121, the original input x and a retrieved passage z.\\n\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the p\\u03b7 and p\\u03b8 components, as well as the training and decoding procedure.\\n\\n## Models\\n\\n|RAG-Sequence Model|The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,|\\n|---|---|\\n| |pRAG-Sequence(y|x) \\u2248 p\\u03b7(z|x)p\\u03b8(y|x, z) = p\\u03b7(z|x) \\u03a3 p\\u03b8(yi|x, z, y1:i\\u22121) z\\u2208top-k(p(\\u00b7|x)) z\\u2208top-k(p(\\u00b7|x)) i|\\n\\n## RAG-Token Model\\n\\nIn the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we define:\\n\\npRAG-Token(y|x) \\u2248 \\u03a3 z\\u2208top-k(p(\\u00b7|x)) p\\u03b7(z|x)p\\u03b8(yi|x, z, y1:i\\u22121)\\n\\nFinally, we note that RAG can be used for sequence classification tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n\\n## Retriever: DPR\\n\\nThe retrieval component p\\u03b7(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:\\n\\np\\u03b7(z|x) \\u221d exp d(z)\\u22a4q(x) d(z) = BERTd(z), q(x) = BERTq(x)\\n\\nwhere d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(p\\u03b7(\\u00b7|x)), the list of k documents z with highest prior probability p\\u03b7(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory.\\n\\n## Generator: BART\\n\\nThe generator component p\\u03b8(yi|x, z, y1:i\\u22121) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters \\u03b8 as the parametric memory henceforth.\\n\\n## Training\\n\\nWe jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a fine-tuning training corpus of input/output pairs (xj, yj), we\\n---\\nminimize the negative marginal log-likelihood of each target, j \\u2212log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder BERTq and the BART generator.\\n\\n## Decoding\\n\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate arg max y p(y|x).\\n\\n|RAG-Token|The RAG-Token model can be seen as a standard, autoregressive seq2seq generator with transition probability: p\\u2032 \\u03b8(yi|x, y1:i\\u22121) = z\\u2208top-k(p(\\u00b7|x)) p\\u03b7(zi|x)p\\u03b8(yi|x, zi, y1:i\\u22121) To decode, we can plug p\\u2032 \\u03b8(yi|x, y1:i\\u22121) into a standard beam decoder.|\\n|---|---|\\n|RAG-Sequence|For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using p\\u03b8(yi|x, z, y1:i\\u22121). This yields a set of hypotheses Y, some of which may not have appeared in the beams of all documents. To estimate the probability of a hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with p\\u03b7(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as \\u201cThorough Decoding.\\u201d For longer output sequences, |Y| can become large, requiring many forward passes. For more efficient decoding, we can make a further approximation that p\\u03b8(y|x, zi) \\u2248 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as \\u201cFast Decoding.\\u201d|\\n\\n## Experiments\\n\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k \\u2208 {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task.\\n\\n### Open-domain Question Answering\\n\\nOpen-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to \\u201cClosed-Book QA\\u201d approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\\n\\n### Abstractive Question Answering\\n\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG\\u2019s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n---\\nMSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as \\\"What is the weather in Volcano, CA?\\\" so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses.\\n\\nJeopardy Question Generation\\n\\nTo evaluate RAG\\u2019s generation abilities in a non-QA setting, we study open-domain question generation. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, \\\"The World Cup\\\" is the answer to the question \\\"In 1986 Mexico scored as the first country to host this international sports competition twice.\\\" As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task.\\n\\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for specificity. We define factuality as whether a statement can be corroborated by trusted external sources, and specificity as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options\\u2014question A is better, question B is better, both are good, or neither is good.\\n\\nFact Verification\\n\\nFEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unverifiable from Wikipedia alone. FEVER is a retrieval problem coupled with a challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models\\u2019 ability to handle classification rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren\\u2019t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n\\n## Results\\n\\n### Open-domain Question Answering\\n\\n|Task|RAG|State-of-the-art models|\\n|---|---|---|\\n|All four open-domain QA tasks|RAG sets a new state of the art (only on the T5-comparable split for TQA)|RAG combines the generation flexibility of the \\u201cclosed-book\\u201d (parametric only) approaches and the performance of \\\"open-book\\\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized \\u201csalient span masking\\u201d pre-training [20]. It is worth noting that RAG\\u2019s retriever is initialized using DPR\\u2019s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based \\u201ccross-encoder\\u201d to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance.|\\n\\nThere are several advantages to generating answers even when it is possible to extract them. Documents with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading\\n---\\n|Content|Page Number|\\n|---|---|\\n|Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open-Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details.| |\\n|Table 2: Generation and classification Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined.| |\\n\\n## Model NQ TQA WQ CT\\n\\n|Model|NQ|TQA|WQ|CT|\\n|---|---|---|---|---|\\n|Closed T5-11B [52]|34.5|- /50.1|37.4|-|\\n|Book T5-11B+SSM[52]|36.6|- /60.5|44.7|-|\\n|Open REALM [20]|40.4|- / -|40.7|46.8|\\n|Book DPR [26]|41.5|57.9/ -|41.1|50.6|\\n|RAG-Token|44.1|55.2/66.1|45.5|50.0|\\n|RAG-Seq.|44.5|56.8/68.0|45.2|52.2|\\n\\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%.\\n\\n## Abstractive Question Answering\\n\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with specific information required to generate the reference answer, (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we find that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see \\u00a74.5).\\n\\n## Jeopardy Question Generation\\n\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also find RAG generations to be more specific by a large margin. Table 3 shows typical generations from each model.\\n---\\n## Document 1\\n\\nHis works are considered classics of American literature ... His wartime experiences formed the basis for his novel \\\"A Farewell to Arms\\\" (1929) ...\\n\\n## Document 2\\n\\n... artists of the 1920s \\\"Lost Generation\\\" expatriate community. His debut novel, \\\"The Sun Also Rises\\\", was published in 1926.\\n\\nFigure 2: RAG-Token document posterior p(zi|x, yi, y\\u2212i) for each generated token for input \\u201cHemingway\\\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating \\u201cA Farewell to Arms\\\" and for document 2 when generating \\u201cThe Sun Also Rises\\\".\\n\\n|Task|Input|Model|Generation|\\n|---|---|---|---|\\n|define middle ear|BART|?|The middle ear is the part of the ear between the middle ear and the nose.|\\n| |RAG-T| |The middle ear is the portion of the ear internal to the eardrum.|\\n|MS-MARCO|what currency needed in Scotland|BART|The currency needed in Scotland is Pound sterling.|\\n| |RAG-T| |Pound is the currency needed in Scotland.|\\n| |RAG-S| |The currency needed in Scotland is the pound sterling.|\\n| |BART|?|This state has the largest number of counties in the U.S.|\\n|Jeopardy Question Generation|Washington|RAG-T|It\\u2019s the only U.S. state named for a U.S. president|\\n| |RAG-S| |It\\u2019s the state where you\\u2019ll find Mount Rainier National Park|\\n| |BART|*|This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio|\\n|Generation|The Divine Comedy|RAG-T|Dante\\u2019s \\\"Inferno\\\" is the first part of this epic poem|\\n| |RAG-S| |This 14th century work is divided into 3 sections: \\\"Inferno\\\", \\\"Purgatorio\\\" & \\\"Paradiso\\\"|\\n\\nFor 2-way classification, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved by RAG and gold evidence annotations. We find that the top retrieved document is from a gold article in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\\n\\n## Additional Results\\n\\nGeneration Diversity: Section 4.3 shows that RAG models are more factual and specific than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding, we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence\\u2019s generations are more diverse than RAG-Token\\u2019s, and both are significantly more diverse than BART without needing any diversity-promoting decoding.\\n\\nRetrieval Ablations: A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks.\\n\\nWe compare RAG\\u2019s dense retriever to a word overlap-based BM25 retriever. Here, we replace RAG\\u2019s retriever with a fixed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\n\\nIndex hot-swapping: An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n---\\n## Table 4: Human assessments for the Jeopardy Question Generation Task\\n\\n| |Factuality|Specificity|\\n|---|---|---|\\n|BART better|7.1%|16.8%|\\n|RAG better|42.7%|37.4%|\\n|Both good|11.7%|11.8%|\\n|Both poor|17.7%|6.9%|\\n|No majority|20.8%|20.1%|\\n\\n## Table 5: Ratio of distinct to total tri-grams for generation tasks\\n\\n| |MSMARCO|Jeopardy QGen|\\n|---|---|---|\\n|Gold|89.6%|90.0%|\\n|BART|70.7%|32.4%|\\n|RAG-Token|77.8%|46.8%|\\n|RAG-Seq.|83.5%|53.8%|\\n\\n## Table 6: Ablations on the dev set\\n\\n|Model|NQ|TQA|WQ|CT|Jeopardy-QGen|MSMarco|FVR-3|FVR-2|\\n|---|---|---|---|---|---|---|---|---|\\n|RAG-Token-BM25|29.7|41.5|32.1|33.1|17.5|22.3|55.5|48.4|75.1|91.6|\\n|RAG-Sequence-BM25|31.8|44.1|36.6|33.8|11.1|19.5|56.5|46.9|\\n|RAG-Token-Frozen|37.8|50.1|37.1|51.1|16.7|21.7|55.9|49.4|72.9|89.4|\\n|RAG-Sequence-Frozen|41.2|52.1|41.8|52.6|11.8|19.6|56.7|47.3|\\n|RAG-Token|43.5|54.8|46.5|51.9|17.9|22.6|56.2|49.4|74.5|90.6|\\n|RAG-Sequence|44.0|55.8|44.9|53.4|15.3|21.5|57.2|47.5|\\n\\nBetween these dates and use a template \\u201cWho is {position}?\\u201d (e.g. \\u201cWho is the President of Peru?\\u201d) to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for 2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders). This shows we can update RAG\\u2019s world knowledge by simply replacing its non-parametric memory.\\n\\nEffect of Retrieving more documents: Models are trained with either 5 or 10 retrieved latent documents, and we do not observe significant differences in performance between them. We have the flexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n\\n|44|80|Bleu-1 / Rouge-L score|\\n|---|---|---|\\n|NQ Answer Recall @ K| | |\\n\\nNQ Exact Match\\n\\n| |10|20|30|40|50|10|20|30|40|50|10|20|30|40|50|\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|K Retrieved Docs| | | | | | | | | | | | | | | |\\n\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n\\n## Related Work\\n\\nSingle-Task Retrieval: Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering, fact checking, fact completion, long-form question answering, Wikipedia article generation, dialogue, translation, and language modeling. Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.\\n---\\n## General-Purpose Architectures for NLP\\n\\nPrior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks [60, 61] after fine-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models.\\n\\n## Learned Retrieval\\n\\nThere is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.\\n\\n## Memory-based Architectures\\n\\nOur document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model\\u2019s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval [9].\\n\\n## Retrieve-and-Edit approaches\\n\\nOur method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a final output. These approaches have proved successful in a number of domains including Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.\\n\\n## Discussion\\n\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG\\u2019s generation over purely parametric BART, finding RAG more factual and specific. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.\\n---\\n## Broader Impact\\n\\nThis work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it \\u201challucinate\\u201d less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.\\n\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [54]. Advanced language models may also lead to the automation of various jobs in the coming decades [16]. In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing.\\n\\n## Acknowledgments\\n\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.\\n\\n## References\\n\\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http://arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n[2] Petr Baudi\\u0161 and Jan \\u0160ediv` y. Modeling of pe question answering task in pe yodaqa system. In International Conference of pe Cross-Language Evaluation Forum for European Languages, pages 222\\u2013228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20.\\n[3] Jonapan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of pe 2013 Conference on Empirical Mepods in Natural Language Processing, pages 1533\\u20131544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anpology/D13-1160.\\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-ing&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159, 2020. URL https://arxiv.org/abs/2004.07159.\\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 1870\\u20131879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anpology/P17-1171.\\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonapan Berant. Coarse-to-fine question answering for long documents. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 209\\u2013220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anpology/P17-1020.\\n---\\nChristopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Comprehension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723. arXiv: 1710.10723.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of pe 2019 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anpology/N19-1423.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\\nMatpew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A New Q&A Dataset Augmented wip Context from a Search Engine. arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv: 1704.05179.\\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of pe 56p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 889\\u2013898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anpology/P18-1082.\\nAngela Fan, Yacine Jernite, Epan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of pe 57p Annual Meeting of pe Association for Computational Linguistics, pages 3558\\u20133567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/anpology/P19-1346.\\nAngela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers wip KNN-based composite memory, 2020. URL https://openreview.net/forum?id=H1gx1CNKPH.\\nThibault F\\u00e9vry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access wip entity supervision. ArXiv, abs/2004.07202, 2020. URL https://arxiv.org/abs/2004.07202.\\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wentau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710.\\nKatja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL http://arxiv.org/abs/1705.08807.\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pages 5133\\u20135140. AAAI press, 2018. 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018.\\nKelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of pe Association for Computational Linguistics, 6:437\\u2013450, 2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anpology/Q18-1031.\\n---\\n## References\\n\\n|[20]|Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https://arxiv.org/abs/2002.08909.|\\n|---|---|\\n|[21]|Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 10052\\u201310062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf.|\\n|[22]|Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-edit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2532\\u20132538, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/anthology/2020.acl-main.228.|\\n|[23]|Jeff Johnson, Matthijs Douze, and Herv\\u00e9 J\\u00e9gou. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.|\\n|[24]|Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\\u20131611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.|\\n|[25]|Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS\\u201915, page 190\\u2013198, Cambridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.|\\n|[26]|Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.|\\n|[27]|Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.|\\n|[28]|Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.|\\n|[29]|Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. Transactions of the Association of Computational Linguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf.|\\n|[30]|Guillaume Lample, Alexandre Sablayrolles, Marc\\u2019 Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\u2019 Alch\\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8548\\u20138559. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf.|\\n|[31]|Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association|\\n---\\n## References\\n\\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\\n\\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110\\u2013119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/N16-1014.\\n\\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL https://arxiv.org/abs/1909.03087.\\n\\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation with joint textual and phonetic embedding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3044\\u20133049, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL https://www.aclweb.org/anthology/P19-1291.\\n\\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hyg0vbWC-.\\n\\n[37] Yury A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824\\u2013836, 2016. URL https://arxiv.org/abs/1603.09320.\\n\\n[38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\\n\\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rockt\\u00e4schel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the verifiability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https://arxiv.org/abs/1911.03587.\\n\\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\n\\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploiting background knowledge for building conversation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2322\\u20132332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.\\n\\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3950\\u20133959, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/anthology/D18-1429.\\n\\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d\\u2019Avila Garcez, and Greg Wayne, editors, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic. 13\\n---\\napproaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf.\\n\\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\\n\\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48\\u201353, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.org/anthology/N19-4009.\\n\\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402\\u20132411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\\n\\n[47] Fabio Petroni, Tim Rockt\\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\\u20132473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://www.aclweb.org/anthology/D19-1250.\\n\\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\\u00e4schel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models\\u2019 factual predictions. In Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum?id=025X0zPfn.\\n\\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training, 2018. URL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf.\\n\\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\\n\\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\\n\\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/2002.08910.\\n\\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333\\u2013389, April 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL https://doi.org/10.1561/1500000019.\\n\\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019.\\n\\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440\\u20132448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\\n---\\n# References\\n\\n[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of pe 2018 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809\\u2013819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://www.aclweb.org/anpology/N18-1074.\\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classification wip elastic weight consolidation. ArXiv, abs/2004.14366, 2020. URL https://arxiv.org/abs/2004.14366.\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanapan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998\\u20136008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaap Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329.\\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of pe 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\\u2013355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anpology/W18-5446.\\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3261\\u20133275. Curran Associates, Inc., 2019. URL https://arxiv.org/abs/1905.00537.\\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain question answering. In Sheila A. McIlraip and Kilian Q. Weinberger, editors, Proceedings of pe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), pe 30p innovative Applications of Artificial Intelligence (IAAI-18), and pe 8p AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5981\\u20135988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712.\\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in open-domain question answering. In ICLR, 2018. URL https://openreview.net/forum?id=rJl3yM-Ab.\\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.\\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and refine: Improved sequence generation models for dialogue. In Proceedings of pe 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 87\\u201392, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anpology/W18-5713.\\n---\\n## References\\n\\n[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anpony Moi, Pierric Cistac, Tim Rault, R\\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface\\u2019s transformers: State-of-pe-art natural language processing. ArXiv, abs/1910.03771, 2019.\\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-supervised question answering. In Proceedings of pe 2019 Conference on Empirical Mepods in Natural Language Processing and pe 9p International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2495\\u20132509, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anpology/D19-1253.\\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019. URL https://arxiv.org/abs/1909.03745.\\n---\\n## Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\n### Implementation Details\\n\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not find beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\n\\n### Human Evaluation\\n\\n|Which sentence is more factually true?|Select option|\\n|---|---|\\n|Noje: Scna Guesucn?|snterzt \\\"The8r Nuso Rists|\\n|IncicateFich|Farcncllic AM; on Im|\\n|Iclbwng sentarces Is Mca luclualy Injb[ealecllo|ZLbko Uaino|\\n| |cnoc urdoco|\\n\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \\\"view tool guide\\\".\\n\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instructions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.\\n\\n### Training Setup Details\\n\\nWe train all RAG models and BART baselines using Fairseq. We train with mixed precision floating point arithmetic, distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We find that doing Maximum Inner Product Search with FAISS is sufficiently fast on CPU, so we store document index vectors on CPU, requiring \\u223c 100 GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers, which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS\\u2019s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/\\n\\n2. https://github.com/pytorch/fairseq\\n\\n3. https://github.com/huggingface/transformers\\n---\\n## Further Details on Open-Domain QA\\n\\nFor open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to find matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we filter out answer candidates if they do not occur in top 1000 documents for the query.\\n\\n## CuratedTrec preprocessing\\n\\nThe answers for CuratedTrec are given in the form of regular expressions, which has been suggested as a reason why it is unsuitable for answer-generation models [20]. To overcome this, we use a pre-processing step where we first retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\n\\n## TriviaQA Evaluation setups\\n\\nThe open-domain QA community customarily uses public development datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading comprehension purposes. We report our results using the datasets splits used in DPR [26], which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA official Wikipedia test set instead. F\\u00e9vry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See appendix of [14]). We report results on both test sets to enable fair comparison to both approaches. We find that our performance is much higher using the official Wiki test set, rather than the more conventional open-domain test set, which we attribute to the official Wiki test set questions being simpler to answer from Wikipedia.\\n\\n## Further Details on FEVER\\n\\nFor FEVER classification, we follow the practice from [32], and first re-generate the claim, and then classify using the representation of the final hidden state, before finally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The first is to classify the claim as either \\\"Supported\\\", \\\"Refuted\\\" or \\\"Not Enough Info\\\", which is the task we explore in the main paper. FEVER\\u2019s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classification prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work.\\n\\n## Null Document Probabilities\\n\\nWe experimented with adding \\\"Null document\\\" mechanism to RAG, similar to REALM [20] in order to model cases where no useful information could be retrieved for a given input. Here, if k documents were retrieved, we would additionally \\\"retrieve\\\" an empty document and predict a logit for the null document, before marginalizing over k + 1 predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not find that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to benefit from retrieval, suggesting that null document mechanisms may not be necessary for RAG.\\n\\n## Parameters\\n\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable.\\n---\\n|Task|Train|Development|Test|\\n|---|---|---|---|\\n|Natural Questions|79169|8758|3611|\\n|TriviaQA|78786|8838|11314|\\n|WebQuestions|3418|362|2033|\\n|CuratedTrec|635|134|635|\\n|Jeopardy Question Generation|97392|13714|26849|\\n|MS-MARCO|153726|12468|101093*|\\n|FEVER-3-way|145450|10000|10000|\\n|FEVER-2-way|96966|6666|6666|\\n\\nparameters. The best performing \\\"closed-book\\\" (parametric only) open-domain QA model is T5-11B\\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit floating\\npoint precision to manage memory and disk footprints.\\n\\n## Retrieval Collapse\\n\\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the\\nretrieval component would \\u201ccollapse\\u201d and learn to retrieve the same documents regardless of the\\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\\nin less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results\\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\\n\\n## Number of instances per dataset\\n\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\",\n    \"paper\": \"## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\n|Authors|Patrick Lewis\\u2020\\u2021, Ethan Perez\\u22c6, Aleksandra Piktus\\u2020, Fabio Petroni\\u2020, Vladimir Karpukhin\\u2020, Naman Goyal\\u2020, Heinrich K\\u00fcttler\\u2020, Mike Lewis\\u2020, Wen-tau Yih\\u2020, Tim Rockt\\u00e4schel\\u2020\\u2021, Sebastian Riedel\\u2020\\u2021, Douwe Kiela\\u2020|\\n|---|---|\\n|Affiliations|\\u2020Facebook AI Research; \\u2021University College London; \\u22c6New York University;|\\n|Email|plewis@fb.com|\\n\\nAbstract\\n\\nLarge pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \\u2014 models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\\n\\n### Introduction\\n\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data [47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52]. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can\\u2019t straightforwardly provide insight into their predictions, and may produce \\u201challucinations\\u201d [38]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results.\\n---\\n## Define \\\"middle ear\\\"(x)\\n\\nThe middle ear includes the tympanic cavity and the three ossicles.\\n\\n## Question Answering:\\n\\n|Question Query|Query|Retriever p|\\u03b7|Document|Generator p|\\u03b8|\\n|---|---|---|---|---|---|---|\\n|Barack Obama was born in Hawaii.(x)|Encoder|(Non-Parametric)| |Index|(Parametric)|Answer Generation|\\n\\nSupports (y)\\n\\n## Fact Verification: Fact Query\\n\\nThe Divine Comedy (x)\\n\\n## Jeopardy Question Generation:\\n\\nAnswer Query\\n\\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to find the top-K documents zi. For final prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents. but have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the \\u201cworkhorse of NLP,\\u201d i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned. There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stack-augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training. Our results highlight the benefits of combining parametric and non-parametric memory with generation for knowledge-intensive tasks\\u2014tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER [56] fact verification, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models\\u2019 knowledge as the world changes.\\n\\n## Methods\\n\\nWe explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever p\\u03b7(z|x) with parameters \\u03b7 that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator p\\u03b8(yi|x, z, y1:i\\u22121) parametrized\\n\\n1 Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transformers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n---\\n## by \\u03b8 that generates a current token based on a context of the previous i \\u2212 1 tokens y1:i\\u22121, the original input x and a retrieved passage z.\\n\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the p\\u03b7 and p\\u03b8 components, as well as the training and decoding procedure.\\n\\n## Models\\n\\n|RAG-Sequence Model|The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,|\\n|---|---|\\n| |pRAG-Sequence(y|x) \\u2248 p\\u03b7(z|x)p\\u03b8(y|x, z) = p\\u03b7(z|x) \\u03a3 p\\u03b8(yi|x, z, y1:i\\u22121) z\\u2208top-k(p(\\u00b7|x)) z\\u2208top-k(p(\\u00b7|x)) i|\\n\\n## RAG-Token Model\\n\\nIn the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we define:\\n\\npRAG-Token(y|x) \\u2248 \\u03a3 z\\u2208top-k(p(\\u00b7|x)) p\\u03b7(z|x)p\\u03b8(yi|x, z, y1:i\\u22121)\\n\\nFinally, we note that RAG can be used for sequence classification tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n\\n## Retriever: DPR\\n\\nThe retrieval component p\\u03b7(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:\\n\\np\\u03b7(z|x) \\u221d exp d(z)\\u22a4q(x) d(z) = BERTd(z), q(x) = BERTq(x)\\n\\nwhere d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(p\\u03b7(\\u00b7|x)), the list of k documents z with highest prior probability p\\u03b7(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory.\\n\\n## Generator: BART\\n\\nThe generator component p\\u03b8(yi|x, z, y1:i\\u22121) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters \\u03b8 as the parametric memory henceforth.\\n\\n## Training\\n\\nWe jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a fine-tuning training corpus of input/output pairs (xj, yj), we\\n---\\nminimize the negative marginal log-likelihood of each target, j \\u2212log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder BERTq and the BART generator.\\n\\n## Decoding\\n\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate arg max y p(y|x).\\n\\n|RAG-Token|The RAG-Token model can be seen as a standard, autoregressive seq2seq generator with transition probability: p\\u2032 \\u03b8(yi|x, y1:i\\u22121) = z\\u2208top-k(p(\\u00b7|x)) p\\u03b7(zi|x)p\\u03b8(yi|x, zi, y1:i\\u22121) To decode, we can plug p\\u2032 \\u03b8(yi|x, y1:i\\u22121) into a standard beam decoder.|\\n|---|---|\\n|RAG-Sequence|For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using p\\u03b8(yi|x, z, y1:i\\u22121). This yields a set of hypotheses Y, some of which may not have appeared in the beams of all documents. To estimate the probability of a hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with p\\u03b7(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as \\u201cThorough Decoding.\\u201d For longer output sequences, |Y| can become large, requiring many forward passes. For more efficient decoding, we can make a further approximation that p\\u03b8(y|x, zi) \\u2248 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as \\u201cFast Decoding.\\u201d|\\n\\n## Experiments\\n\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k \\u2208 {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task.\\n\\n### Open-domain Question Answering\\n\\nOpen-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to \\u201cClosed-Book QA\\u201d approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\\n\\n### Abstractive Question Answering\\n\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG\\u2019s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n---\\nMSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as \\\"What is the weather in Volcano, CA?\\\" so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses.\\n\\nJeopardy Question Generation\\n\\nTo evaluate RAG\\u2019s generation abilities in a non-QA setting, we study open-domain question generation. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, \\\"The World Cup\\\" is the answer to the question \\\"In 1986 Mexico scored as the first country to host this international sports competition twice.\\\" As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task.\\n\\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for specificity. We define factuality as whether a statement can be corroborated by trusted external sources, and specificity as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options\\u2014question A is better, question B is better, both are good, or neither is good.\\n\\nFact Verification\\n\\nFEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unverifiable from Wikipedia alone. FEVER is a retrieval problem coupled with a challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models\\u2019 ability to handle classification rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren\\u2019t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n\\n## Results\\n\\n### Open-domain Question Answering\\n\\n|Task|RAG|State-of-the-art models|\\n|---|---|---|\\n|All four open-domain QA tasks|RAG sets a new state of the art (only on the T5-comparable split for TQA)|RAG combines the generation flexibility of the \\u201cclosed-book\\u201d (parametric only) approaches and the performance of \\\"open-book\\\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized \\u201csalient span masking\\u201d pre-training [20]. It is worth noting that RAG\\u2019s retriever is initialized using DPR\\u2019s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based \\u201ccross-encoder\\u201d to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance.|\\n\\nThere are several advantages to generating answers even when it is possible to extract them. Documents with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading\\n---\\n|Content|Page Number|\\n|---|---|\\n|Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open-Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details.| |\\n|Table 2: Generation and classification Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined.| |\\n\\n## Model NQ TQA WQ CT\\n\\n|Model|NQ|TQA|WQ|CT|\\n|---|---|---|---|---|\\n|Closed T5-11B [52]|34.5|- /50.1|37.4|-|\\n|Book T5-11B+SSM[52]|36.6|- /60.5|44.7|-|\\n|Open REALM [20]|40.4|- / -|40.7|46.8|\\n|Book DPR [26]|41.5|57.9/ -|41.1|50.6|\\n|RAG-Token|44.1|55.2/66.1|45.5|50.0|\\n|RAG-Seq.|44.5|56.8/68.0|45.2|52.2|\\n\\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%.\\n\\n## Abstractive Question Answering\\n\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with specific information required to generate the reference answer, (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we find that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see \\u00a74.5).\\n\\n## Jeopardy Question Generation\\n\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also find RAG generations to be more specific by a large margin. Table 3 shows typical generations from each model.\\n---\\n## Document 1\\n\\nHis works are considered classics of American literature ... His wartime experiences formed the basis for his novel \\\"A Farewell to Arms\\\" (1929) ...\\n\\n## Document 2\\n\\n... artists of the 1920s \\\"Lost Generation\\\" expatriate community. His debut novel, \\\"The Sun Also Rises\\\", was published in 1926.\\n\\nFigure 2: RAG-Token document posterior p(zi|x, yi, y\\u2212i) for each generated token for input \\u201cHemingway\\\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating \\u201cA Farewell to Arms\\\" and for document 2 when generating \\u201cThe Sun Also Rises\\\".\\n\\n|Task|Input|Model|Generation|\\n|---|---|---|---|\\n|define middle ear|BART|?|The middle ear is the part of the ear between the middle ear and the nose.|\\n| |RAG-T| |The middle ear is the portion of the ear internal to the eardrum.|\\n|MS-MARCO|what currency needed in Scotland|BART|The currency needed in Scotland is Pound sterling.|\\n| |RAG-T| |Pound is the currency needed in Scotland.|\\n| |RAG-S| |The currency needed in Scotland is the pound sterling.|\\n| |BART|?|This state has the largest number of counties in the U.S.|\\n|Jeopardy Question Generation|Washington|RAG-T|It\\u2019s the only U.S. state named for a U.S. president|\\n| |RAG-S| |It\\u2019s the state where you\\u2019ll find Mount Rainier National Park|\\n| |BART|*|This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio|\\n|Generation|The Divine Comedy|RAG-T|Dante\\u2019s \\\"Inferno\\\" is the first part of this epic poem|\\n| |RAG-S| |This 14th century work is divided into 3 sections: \\\"Inferno\\\", \\\"Purgatorio\\\" & \\\"Paradiso\\\"|\\n\\nFor 2-way classification, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved by RAG and gold evidence annotations. We find that the top retrieved document is from a gold article in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\\n\\n## Additional Results\\n\\nGeneration Diversity: Section 4.3 shows that RAG models are more factual and specific than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding, we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence\\u2019s generations are more diverse than RAG-Token\\u2019s, and both are significantly more diverse than BART without needing any diversity-promoting decoding.\\n\\nRetrieval Ablations: A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks.\\n\\nWe compare RAG\\u2019s dense retriever to a word overlap-based BM25 retriever. Here, we replace RAG\\u2019s retriever with a fixed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\n\\nIndex hot-swapping: An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n---\\n## Table 4: Human assessments for the Jeopardy Question Generation Task\\n\\n| |Factuality|Specificity|\\n|---|---|---|\\n|BART better|7.1%|16.8%|\\n|RAG better|42.7%|37.4%|\\n|Both good|11.7%|11.8%|\\n|Both poor|17.7%|6.9%|\\n|No majority|20.8%|20.1%|\\n\\n## Table 5: Ratio of distinct to total tri-grams for generation tasks\\n\\n| |MSMARCO|Jeopardy QGen|\\n|---|---|---|\\n|Gold|89.6%|90.0%|\\n|BART|70.7%|32.4%|\\n|RAG-Token|77.8%|46.8%|\\n|RAG-Seq.|83.5%|53.8%|\\n\\n## Table 6: Ablations on the dev set\\n\\n|Model|NQ|TQA|WQ|CT|Jeopardy-QGen|MSMarco|FVR-3|FVR-2|\\n|---|---|---|---|---|---|---|---|---|\\n|RAG-Token-BM25|29.7|41.5|32.1|33.1|17.5|22.3|55.5|48.4|75.1|91.6|\\n|RAG-Sequence-BM25|31.8|44.1|36.6|33.8|11.1|19.5|56.5|46.9|\\n|RAG-Token-Frozen|37.8|50.1|37.1|51.1|16.7|21.7|55.9|49.4|72.9|89.4|\\n|RAG-Sequence-Frozen|41.2|52.1|41.8|52.6|11.8|19.6|56.7|47.3|\\n|RAG-Token|43.5|54.8|46.5|51.9|17.9|22.6|56.2|49.4|74.5|90.6|\\n|RAG-Sequence|44.0|55.8|44.9|53.4|15.3|21.5|57.2|47.5|\\n\\nBetween these dates and use a template \\u201cWho is {position}?\\u201d (e.g. \\u201cWho is the President of Peru?\\u201d) to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for 2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders). This shows we can update RAG\\u2019s world knowledge by simply replacing its non-parametric memory.\\n\\nEffect of Retrieving more documents: Models are trained with either 5 or 10 retrieved latent documents, and we do not observe significant differences in performance between them. We have the flexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n\\n|44|80|Bleu-1 / Rouge-L score|\\n|---|---|---|\\n|NQ Answer Recall @ K| | |\\n\\nNQ Exact Match\\n\\n| |10|20|30|40|50|10|20|30|40|50|10|20|30|40|50|\\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\\n|K Retrieved Docs| | | | | | | | | | | | | | | |\\n\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n\\n## Related Work\\n\\nSingle-Task Retrieval: Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering, fact checking, fact completion, long-form question answering, Wikipedia article generation, dialogue, translation, and language modeling. Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.\\n---\\n## General-Purpose Architectures for NLP\\n\\nPrior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks [60, 61] after fine-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models.\\n\\n## Learned Retrieval\\n\\nThere is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.\\n\\n## Memory-based Architectures\\n\\nOur document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model\\u2019s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval [9].\\n\\n## Retrieve-and-Edit approaches\\n\\nOur method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a final output. These approaches have proved successful in a number of domains including Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.\\n\\n## Discussion\\n\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG\\u2019s generation over purely parametric BART, finding RAG more factual and specific. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.\\n---\\n## Broader Impact\\n\\nThis work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it \\u201challucinate\\u201d less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.\\n\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [54]. Advanced language models may also lead to the automation of various jobs in the coming decades [16]. In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing.\\n\\n## Acknowledgments\\n\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.\\n\\n## References\\n\\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http://arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n[2] Petr Baudi\\u0161 and Jan \\u0160ediv` y. Modeling of pe question answering task in pe yodaqa system. In International Conference of pe Cross-Language Evaluation Forum for European Languages, pages 222\\u2013228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20.\\n[3] Jonapan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of pe 2013 Conference on Empirical Mepods in Natural Language Processing, pages 1533\\u20131544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anpology/D13-1160.\\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-ing&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159, 2020. URL https://arxiv.org/abs/2004.07159.\\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 1870\\u20131879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anpology/P17-1171.\\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonapan Berant. Coarse-to-fine question answering for long documents. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 209\\u2013220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anpology/P17-1020.\\n---\\nChristopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Comprehension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723. arXiv: 1710.10723.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of pe 2019 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anpology/N19-1423.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\\nMatpew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A New Q&A Dataset Augmented wip Context from a Search Engine. arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv: 1704.05179.\\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of pe 56p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 889\\u2013898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anpology/P18-1082.\\nAngela Fan, Yacine Jernite, Epan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of pe 57p Annual Meeting of pe Association for Computational Linguistics, pages 3558\\u20133567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/anpology/P19-1346.\\nAngela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers wip KNN-based composite memory, 2020. URL https://openreview.net/forum?id=H1gx1CNKPH.\\nThibault F\\u00e9vry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access wip entity supervision. ArXiv, abs/2004.07202, 2020. URL https://arxiv.org/abs/2004.07202.\\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wentau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710.\\nKatja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL http://arxiv.org/abs/1705.08807.\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pages 5133\\u20135140. AAAI press, 2018. 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018.\\nKelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of pe Association for Computational Linguistics, 6:437\\u2013450, 2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anpology/Q18-1031.\\n---\\n## References\\n\\n|[20]|Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https://arxiv.org/abs/2002.08909.|\\n|---|---|\\n|[21]|Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 10052\\u201310062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf.|\\n|[22]|Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-edit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2532\\u20132538, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/anthology/2020.acl-main.228.|\\n|[23]|Jeff Johnson, Matthijs Douze, and Herv\\u00e9 J\\u00e9gou. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.|\\n|[24]|Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\\u20131611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.|\\n|[25]|Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS\\u201915, page 190\\u2013198, Cambridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.|\\n|[26]|Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.|\\n|[27]|Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.|\\n|[28]|Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.|\\n|[29]|Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. Transactions of the Association of Computational Linguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf.|\\n|[30]|Guillaume Lample, Alexandre Sablayrolles, Marc\\u2019 Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\u2019 Alch\\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8548\\u20138559. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf.|\\n|[31]|Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association|\\n---\\n## References\\n\\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\\n\\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110\\u2013119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/N16-1014.\\n\\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL https://arxiv.org/abs/1909.03087.\\n\\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation with joint textual and phonetic embedding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3044\\u20133049, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL https://www.aclweb.org/anthology/P19-1291.\\n\\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hyg0vbWC-.\\n\\n[37] Yury A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824\\u2013836, 2016. URL https://arxiv.org/abs/1603.09320.\\n\\n[38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\\n\\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rockt\\u00e4schel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the verifiability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https://arxiv.org/abs/1911.03587.\\n\\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\n\\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploiting background knowledge for building conversation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2322\\u20132332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.\\n\\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3950\\u20133959, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/anthology/D18-1429.\\n\\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d\\u2019Avila Garcez, and Greg Wayne, editors, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic. 13\\n---\\napproaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf.\\n\\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\\n\\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48\\u201353, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.org/anthology/N19-4009.\\n\\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402\\u20132411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\\n\\n[47] Fabio Petroni, Tim Rockt\\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\\u20132473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://www.aclweb.org/anthology/D19-1250.\\n\\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\\u00e4schel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models\\u2019 factual predictions. In Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum?id=025X0zPfn.\\n\\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training, 2018. URL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf.\\n\\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\\n\\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\\n\\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/2002.08910.\\n\\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333\\u2013389, April 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL https://doi.org/10.1561/1500000019.\\n\\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019.\\n\\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440\\u20132448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\\n---\\n# References\\n\\n[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of pe 2018 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809\\u2013819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://www.aclweb.org/anpology/N18-1074.\\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classification wip elastic weight consolidation. ArXiv, abs/2004.14366, 2020. URL https://arxiv.org/abs/2004.14366.\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanapan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998\\u20136008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaap Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329.\\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of pe 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\\u2013355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anpology/W18-5446.\\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3261\\u20133275. Curran Associates, Inc., 2019. URL https://arxiv.org/abs/1905.00537.\\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain question answering. In Sheila A. McIlraip and Kilian Q. Weinberger, editors, Proceedings of pe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), pe 30p innovative Applications of Artificial Intelligence (IAAI-18), and pe 8p AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5981\\u20135988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712.\\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in open-domain question answering. In ICLR, 2018. URL https://openreview.net/forum?id=rJl3yM-Ab.\\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.\\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and refine: Improved sequence generation models for dialogue. In Proceedings of pe 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 87\\u201392, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anpology/W18-5713.\\n---\\n## References\\n\\n[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anpony Moi, Pierric Cistac, Tim Rault, R\\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface\\u2019s transformers: State-of-pe-art natural language processing. ArXiv, abs/1910.03771, 2019.\\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-supervised question answering. In Proceedings of pe 2019 Conference on Empirical Mepods in Natural Language Processing and pe 9p International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2495\\u20132509, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anpology/D19-1253.\\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019. URL https://arxiv.org/abs/1909.03745.\\n---\\n## Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\n### Implementation Details\\n\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not find beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\n\\n### Human Evaluation\\n\\n|Which sentence is more factually true?|Select option|\\n|---|---|\\n|Noje: Scna Guesucn?|snterzt \\\"The8r Nuso Rists|\\n|IncicateFich|Farcncllic AM; on Im|\\n|Iclbwng sentarces Is Mca luclualy Injb[ealecllo|ZLbko Uaino|\\n| |cnoc urdoco|\\n\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \\\"view tool guide\\\".\\n\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instructions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.\\n\\n### Training Setup Details\\n\\nWe train all RAG models and BART baselines using Fairseq. We train with mixed precision floating point arithmetic, distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We find that doing Maximum Inner Product Search with FAISS is sufficiently fast on CPU, so we store document index vectors on CPU, requiring \\u223c 100 GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers, which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS\\u2019s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/\\n\\n2. https://github.com/pytorch/fairseq\\n\\n3. https://github.com/huggingface/transformers\\n---\\n## Further Details on Open-Domain QA\\n\\nFor open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to find matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we filter out answer candidates if they do not occur in top 1000 documents for the query.\\n\\n## CuratedTrec preprocessing\\n\\nThe answers for CuratedTrec are given in the form of regular expressions, which has been suggested as a reason why it is unsuitable for answer-generation models [20]. To overcome this, we use a pre-processing step where we first retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\n\\n## TriviaQA Evaluation setups\\n\\nThe open-domain QA community customarily uses public development datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading comprehension purposes. We report our results using the datasets splits used in DPR [26], which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA official Wikipedia test set instead. F\\u00e9vry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See appendix of [14]). We report results on both test sets to enable fair comparison to both approaches. We find that our performance is much higher using the official Wiki test set, rather than the more conventional open-domain test set, which we attribute to the official Wiki test set questions being simpler to answer from Wikipedia.\\n\\n## Further Details on FEVER\\n\\nFor FEVER classification, we follow the practice from [32], and first re-generate the claim, and then classify using the representation of the final hidden state, before finally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The first is to classify the claim as either \\\"Supported\\\", \\\"Refuted\\\" or \\\"Not Enough Info\\\", which is the task we explore in the main paper. FEVER\\u2019s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classification prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work.\\n\\n## Null Document Probabilities\\n\\nWe experimented with adding \\\"Null document\\\" mechanism to RAG, similar to REALM [20] in order to model cases where no useful information could be retrieved for a given input. Here, if k documents were retrieved, we would additionally \\\"retrieve\\\" an empty document and predict a logit for the null document, before marginalizing over k + 1 predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not find that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to benefit from retrieval, suggesting that null document mechanisms may not be necessary for RAG.\\n\\n## Parameters\\n\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable.\\n---\\n|Task|Train|Development|Test|\\n|---|---|---|---|\\n|Natural Questions|79169|8758|3611|\\n|TriviaQA|78786|8838|11314|\\n|WebQuestions|3418|362|2033|\\n|CuratedTrec|635|134|635|\\n|Jeopardy Question Generation|97392|13714|26849|\\n|MS-MARCO|153726|12468|101093*|\\n|FEVER-3-way|145450|10000|10000|\\n|FEVER-2-way|96966|6666|6666|\\n\\nparameters. The best performing \\\"closed-book\\\" (parametric only) open-domain QA model is T5-11B\\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit floating\\npoint precision to manage memory and disk footprints.\\n\\n## Retrieval Collapse\\n\\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the\\nretrieval component would \\u201ccollapse\\u201d and learn to retrieve the same documents regardless of the\\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\\nin less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results\\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\\n\\n## Number of instances per dataset\\n\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\"\n}\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/5d4c1291-6bb2-47f2-b4c9-7ea8b33084ec"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"23e2306de0a944178c0a7bbee3275d6c","deepnote_cell_type":"text-cell-h3"},"source":"### Perplexity logic","block_group":"0e4227a98ac849f68efd6f49d758c87b"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"3be4b13fbe524788beef16331b6e2414","deepnote_cell_type":"text-cell-p"},"source":"query answer","block_group":"629068a9800a4552be5eee640da467dd"},{"cell_type":"code","metadata":{"source_hash":"363914a5","execution_start":1710468091780,"execution_millis":16220,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"9e7ad39375af42c48066449cb4cf540b","deepnote_cell_type":"code"},"source":"from openai import OpenAI\nimport textwrap\n\nasync def query_perplexity_response(job_id, user_query):\n    # Message configuration for Perplexity\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": (\n                \"You are an artificial intelligence assistant and you need to \"\n                \"engage in a helpful, detailed, polite conversation with a user.\"\n            ),\n        },\n        {\n            \"role\": \"user\",\n            \"content\": user_query\n        },\n    ]\n\n    client = OpenAI(api_key=perplexity_api_key, base_url=\"https://api.perplexity.ai\")\n\n    # chat completion without streaming\n    response = client.chat.completions.create(\n        model=\"pplx-70b-online\",\n        messages=messages,\n    )\n    answer = response.choices[0].message.content \n    # Establish a new database connection\n    async with aiopg.create_pool(DSN) as pool:\n        async with pool.acquire() as conn:\n            async with conn.cursor() as cur:\n                # Update the jobs table, setting perplexity_response\n                await cur.execute(\n                    \"UPDATE jobs SET perplexity_response = %s WHERE job_id = %s\",\n                    (answer, job_id)  # Store the response, wrapped to 75 characters\n                )\n\n    return textwrap.fill(answer, width=75)  # Return the formatted answer\n\n# Example usage\njob_id = 3\nuser_query = \"Top academic papers on LLMs\"\nasyncio.run(query_perplexity_response(job_id, user_query))","block_group":"0451575c2ba747759a10a8a65bb2f7e2","execution_count":237,"outputs":[{"output_type":"execute_result","execution_count":237,"data":{"text/plain":"\"As per your request, I'll provide a summary of top academic papers on Large\\nLanguage Models (LLMs) based on the resources provided. These papers cover\\na range of topics, from prompt engineering to the analysis of various LLM\\narchitectures and their applications.  ### Latest Papers (as of May 2023):\\nFrom the PromptingGuide.ai resource (), here are some notable papers:  1.\\nDemonstration-Retrieved In-context Learning 2. Probing in Context: Toward\\nBuilding Robust Classifiers via Probing Large Language Models 3. Skill-\\nBased Few-Shot Selection for In-Context Learning 4. Exploring Chain-of-\\nThought Style Prompting for Text-to-SQL 5. On Learning to Summarize with\\nLarge Language Models as References 6. Element-aware Summarization with\\nLarge Language Models: Expert-aligned Evaluation and Chain-of-Thought\\nMethod 7. Small Language Models Improve Giants by Rewriting Their Outputs\\n8. Prompting and Evaluating Large Language Models for Proactive Dialogues:\\nClarification, Target-guided, and Non-collaboration 9. Prompt-Based Monte-\\nCarlo Tree Search for Goal-Oriented Dialogue Policy Planning 10. Learning\\nto Generate Novel Scientific Directions with Contextualized Literature-\\nbased Discovery 11. Mitigating Language Model Hallucination with\\nInteractive Question-Knowledge Alignment 12. Learning In-context Learning\\nfor NLPs via Retrieval-Augmented Large Language Models 13. Temporal\\nKnowledge Graph Forecasting Without Human Intervention 14. Small Language\\nModels Improve Giants by Rewriting Their Outputs  ### Foundational Must-\\nRead Papers:  From the OpenAI Developer Forum thread (), here are the\\nsuggested must-read papers:  1. Sparks of Artificial General Intelligence:\\nEarly Experiments with GPT-4  ### Comprehensive Overview of LLMs:  From the\\ncomprehensive overview of LLMs ():  1. A Comprehensive Overview of Large\\nLanguage Models  ### Additional Papers:  From the discussion on Reddit ()\\nand TopBots' list of LLM research papers ():  1. Tree of Thoughts by\\nPrinceton University and Google DeepMind 2. Toolformer by Meta AI 3. PALM-2\\nby Google 4. LLaMA by Meta AI 5. GPT-4 by OpenAI  These papers provide a\\nsolid foundation for understanding the current state of LLM research and\\nadvancements in the field.  Please note that I'llustrate the papers I've\\nprovided a selection of recent and relevant papers that offer insights into\\nthe LLM landscape. I hope this information helps you in your research\\nneeds.\""},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/fb978033-b153-4a1a-93c7-8bfb3e4b5635"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"133e41a62e904dd2b46508abcfd947ae","deepnote_cell_type":"text-cell-h3"},"source":"### GPT logic","block_group":"d1aeac78675841e2a325e4344648d91a"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"fc74c990f647481e81d98b6c8c1aab51","deepnote_cell_type":"text-cell-p"},"source":"answer the user query","block_group":"1a1b62e99b2c45f8aeba1fb8aa2f65fb"},{"cell_type":"code","metadata":{"source_hash":"40c91d0f","execution_start":1710468108044,"execution_millis":3751,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"08bdf4abc1cb4238a14878887d4acf4e","deepnote_cell_type":"code"},"source":"import openai\nimport aiopg\nfrom openai import AsyncOpenAI\n\nasync def query_answer_with_gpt(job_id, user_query):\n\n    # Constructing the prompt\n    prompt = (\n        f\"Please answer the user query if you were a best in class expert on this subject. Keep answer short to 1 sentence. Include factual info and links. Query: {user_query}\"\n    )\n\n    # Setting up OpenAI client\n    client = AsyncOpenAI(api_key=openai_api_key) # Replace 'your_openai_api_key' with your actual OpenAI API key\n\n    # Making an asynchronous API call\n    response = await client.chat.completions.create(\n        messages=[\n            {\"role\": \"system\", \"content\": prompt}\n        ],\n        model=\"gpt-4-turbo-preview\"  # You can switch to other models if needed\n    )\n    answer = response.choices[0].message.content  # Extracting the response   answer = response.choices[0].message.content  # Extracting the response\n\n    async with aiopg.create_pool(DSN) as pool:\n        async with pool.acquire() as conn:\n            async with conn.cursor() as cur:\n                await cur.execute(\n                    \"UPDATE jobs SET gpt_response = %s WHERE job_id = %s\",\n                    (answer, job_id)  # Store the original query if sub_queries is None\n                )\n\n    return answer  # You might still return the answer for logging or other purposes\n\n# Example\njob_id = 3\nuser_query = \"Top academic papers on LLMs\" \nasyncio.run(query_answer_with_gpt(job_id, user_query))","block_group":"50480817e96c4d44af20694de8beef28","execution_count":238,"outputs":[{"output_type":"execute_result","execution_count":238,"data":{"text/plain":"'As of my last update in April 2023, \"Attention Is All You Need\" by Ashish Vaswani et al., which introduced the Transformer model foundational for current large language models (LLMs), is among the top academic papers in the field. You can access it here: https://arxiv.org/abs/1706.03762.'"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/1cdf2b74-5b3a-48db-8bb4-57c7852a2fb6"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"267223d2c4fb47cba2927d2a696cda74","deepnote_cell_type":"text-cell-p"},"source":"generate keyword queries","block_group":"275cb081c05246c49bad13471384597a"},{"cell_type":"code","metadata":{"source_hash":"35d9345b","execution_start":1710468111803,"execution_millis":2737,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"4133d19db235493cb49b23d230ca9114","deepnote_cell_type":"code"},"source":"import openai\nimport aiopg\nfrom openai import AsyncOpenAI\n\ndef generate_search_queries_prompt(question, max_iterations=3):\n    \"\"\" Generates the search queries prompt for the given question.\n    Args: question (str): The question to generate the search queries prompt for\n    Returns: str: The search queries prompt for the given question\n    \"\"\"\n\n    return f'Write {max_iterations} google search queries to search online that form an objective opinion from the following: \"{question}\"' \\\n           f'You must respond in a json format: [\"query 1\", \"query 2\", \"query 3\"].'\n\n\nasync def get_sub_queries(job_id, user_query):\n    client = AsyncOpenAI(api_key=openai_api_key) \n    attempts = 0\n    max_attempts = 3\n    sub_queries = None\n\n    while attempts < max_attempts:\n        try:\n            response = await client.chat.completions.create(\n                model=\"gpt-4-turbo-preview\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are an experienced academic research analyst.\"},\n                    {\"role\": \"user\", \"content\": generate_search_queries_prompt(user_query, max_iterations=3)}\n                ],\n                response_format={ \"type\": \"json_object\" },\n                temperature=0,\n            )\n            answer = response.choices[0].message.content\n            parsed_response = json.loads(answer)  # This attempts to parse the JSON\n            sub_queries = list(parsed_response.values())\n            break  # Exit loop if parsing is successful\n        except json.JSONDecodeError:  # If JSON is invalid\n            print(\"Attempt: \", attempts, \". Invalid JSON: \", json.JSONDecodeError)\n            attempts += 1  # Increment attempts and try again\n\n    # If parsing successful or max attempts reached, update the database\n    async with aiopg.create_pool(DSN) as pool:\n        async with pool.acquire() as conn:\n            async with conn.cursor() as cur:\n                # Update the jobs table, set keyword_search_queries\n                await cur.execute(\n                    \"UPDATE jobs SET keyword_search_queries = %s WHERE job_id = %s\",\n                    (json.dumps(sub_queries or [user_query]), job_id)  # Store the original query if sub_queries is None\n                )\n\n    return sub_queries or [query]  # Return the sub queries or the original query if failed\n\n# Example\njob_id = 3\nuser_query = \"Top academic papers on LLMs\" \nasyncio.run(get_sub_queries(job_id, user_query))","block_group":"9db1b28cebf846cfb06aa92053a0ba3a","execution_count":239,"outputs":[{"output_type":"execute_result","execution_count":239,"data":{"text/plain":"['most cited academic papers on LLMs',\n 'peer-reviewed research on large language models',\n 'high impact publications on LLMs']"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/2ac5468f-6d55-4cad-b767-512770d02672"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"207e0163080449a4aa14b0abe94a2e96","deepnote_cell_type":"text-cell-p"},"source":"analyze gpt and perplexity response to generate more google queries","block_group":"b25eec315c7f41d98ddb14f2b47d1a13"},{"cell_type":"code","metadata":{"source_hash":"ac5bfe10","execution_start":1710468114550,"execution_millis":3966,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"d85cfa6c7f104702a4b727c18d4c1855","deepnote_cell_type":"code"},"source":"import openai\nimport aiopg\nfrom openai import AsyncOpenAI\n\nasync def queries_based_on_LLM_responses(job_id, GPT, Perplexity):\n    client = AsyncOpenAI(api_key=openai_api_key) \n    attempts = 0\n    max_attempts = 3\n    sub_queries = None\n    parsing_successful = False\n\n    while attempts < max_attempts:\n        try:\n            response = await client.chat.completions.create(\n                model=\"gpt-4-turbo-preview\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"Extract titles of academic papers from the following responses. You must respond in a json format: ['title 1', 'title 2', 'title 3']\"},\n                    {\"role\": \"assistant\", \"content\": f'GPT response: \"{GPT}\"\\nPerplexity response: \"{Perplexity}\"'}\n                ],\n                response_format={ \"type\": \"json_object\" },\n                temperature=0,\n            )\n            answer = response.choices[0].message.content\n            parsed_response = json.loads(answer)  # This attempts to parse the JSON\n            sub_queries = parsed_response.get('titles', [])\n            parsing_successful = True\n            break  # Exit loop if parsing is successful\n        except json.JSONDecodeError:  # If JSON is invalid\n            print(\"Attempt: \", attempts, \". Invalid JSON: \", json.JSONDecodeError)\n            print(\"Answer: \", answer)\n            attempts += 1  # Increment attempts and try again\n\n    if parsing_successful:\n        # Only update the database if parsing was successful\n        async with aiopg.create_pool(DSN) as pool:\n            async with pool.acquire() as conn:\n                async with conn.cursor() as cur:\n                    await cur.execute(\n                        \"UPDATE jobs SET paper_search_queries = %s WHERE job_id = %s\",\n                        (json.dumps(sub_queries), job_id)  # Only update with sub_queries if parsing was successful\n                    )\n        return sub_queries  # Return the successfully parsed sub_queries\n    else:\n        return \"Error: Unable to parse the JSON response correctly after multiple attempts.\"\n\n# Example\njob_id = 3\nGPT = 'Identifying the \"top\" academic papers on Large Language Models (LLMs) is subjective, but one of the most foundational and widely cited in this area is \"Attention is All You Need\" by Vaswani et al., which introduced the Transformer architecture, the backbone of many current LLMs (https://arxiv.org/abs/1706.03762).' \nPerplexity = 'Based on the search results provided, I\\'ll share a selection of top\\nacademic papers on LLMs that would be particularly relevant for your\\ninterest in handling research papers, summarizing, and citing research\\npapers when possible.  1. **Sparks of Artificial General Intelligence:\\nEarly experiments with GPT-4** – This paper from Microsoft Research\\ndiscusses the capabilities and limits of GPT-4, a large language model. It\\nexplores the potential for GPT-4 to generate new science and its ability to\\ncite research papers when possible. The paper is mentioned in the OpenAI\\nDeveloper Forum as a must-read for understanding GPT/LLM capabilities and\\nuse.  2. **LLAMA: Open and Efficient Foundation Language Models** – This\\nresearch paper introduces LLaMA, a collection of foundational language\\nmodels by Meta AI, ranging from 7B to 65B parameters. The models were\\ntrained on publicly available datasets without relying on proprietary or\\nrestricted data. This paper is mentioned as a key contribution to the field\\nby TopBots.  3. **Tree of Thoughts: Deliberate Problem Solving with Large\\nLanguage Models** – This paper from Princeton University and Google\\nDeepMind presents the \"Tree of Thoughts\" approach, which enables LLMs to\\nmake deliberate decisions by considering multiple reasoning paths, self-\\nevaluating choices, and making global decisions by looking ahead or\\nbacktracking when needed. The approach is demonstrated to be effective on\\nchallenging tasks like Game of 24, Creative Writing, and Crosswords.  4.\\n**Prompting and Evaluating Large Language Models for Proactive Dialogues**\\n– This paper, listed on the Prompt Engineering Guide, discusses prompting\\nand evaluating LLMs for proactive dialogues, including clarification,\\ntarget-guided, and non-collaborative scenarios. It covers recent advances\\nin prompt engineering, which is crucial for effective interaction with LLMs\\nlike ChatGPT or similar systems.  5. **A Comprehensive Overview of Large\\nLanguage Models** – This paper is a comprehensive overview of LLMs,\\ndiscussing architectures, training pipelines, and utilization in different\\naspects, including a focus on LLMs for handling scientific literature and\\nsummarization.  I hope this selection of papers provides valuable insights\\ninto LLM capabilities, prompt engineering, and their potential to assist in\\nresearch paper handling, summarization, and citations. Let me know if you\\nneed further clarification or have additional questions!'\nasyncio.run(queries_based_on_LLM_responses(job_id, GPT, Perplexity))","block_group":"1f4ab02d819e416bb02979d8968f7439","execution_count":240,"outputs":[{"output_type":"execute_result","execution_count":240,"data":{"text/plain":"['Sparks of Artificial General Intelligence: Early experiments with GPT-4',\n 'LLAMA: Open and Efficient Foundation Language Models',\n 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models',\n 'Prompting and Evaluating Large Language Models for Proactive Dialogues',\n 'A Comprehensive Overview of Large Language Models']"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/96da1878-9073-4d75-8d26-bc2775fd3409"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"1dcec5f7685748878df18c4c45d87da8","deepnote_cell_type":"text-cell-p"},"source":"summary","block_group":"3a7685c1516d452cbc7666347722f8b3"},{"cell_type":"code","metadata":{"source_hash":"5c02a115","execution_start":1710468118547,"execution_millis":4423,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"9acb24a26620429f9947432265c0a73c","deepnote_cell_type":"code"},"source":"import openai\nimport aiopg\nfrom openai import AsyncOpenAI\n\nasync def query_info_with_gpt(paper_id, arxiv_paper_markdown, arxiv_metadata, user_query):\n\n    MAX_CONTEXT_LENGTH = 15500 \n\n    # Initial context setup and trimming\n    context = f\"Context: {arxiv_metadata}\\n{arxiv_paper_markdown}\"\n    while count_tokens(context) > MAX_CONTEXT_LENGTH:\n        char_to_token_ratio = len(context) / count_tokens(context)\n        max_char_length = int(MAX_CONTEXT_LENGTH * char_to_token_ratio)\n        context = context[:max_char_length]\n\n    # Constructing the prompt\n    prompt = (\n        f\"Summarize this in 100 characters based on the user query {user_query}: {context}\"\n    )\n\n    # Setting up OpenAI client\n    client = AsyncOpenAI(api_key=openai_api_key) # Replace 'your_openai_api_key' with your actual OpenAI API key\n\n    # Making an asynchronous API call\n    response = await client.chat.completions.create(\n        messages=[\n            {\"role\": \"system\", \"content\": prompt}\n        ],\n        model=\"gpt-4-turbo-preview\"  # You can switch to other models if needed\n    )\n    answer = response.choices[0].message.content  # Extracting the response   answer = response.choices[0].message.content  # Extracting the response\n\n    # Establish a new database connection\n    async with aiopg.create_pool(DSN) as pool:\n        async with pool.acquire() as conn:\n            async with conn.cursor() as cur:\n                # Update the database with the answer\n                await cur.execute(\n                    \"UPDATE Query_Papers SET relevant_answer = %s WHERE id = %s\",\n                    (answer, paper_id)\n                )\n\n    return answer  # You might still return the answer for logging or other purposes\n\n\n# Example\nuser_query = \"Top academic papers on LLMs\" \npaper_id = 801\narxiv_paper_markdown = \"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.\",\narxiv_metadata = \"## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\nPatrick Lewis\\u2020\\u2021, Ethan Perez\\u22c6, Aleksandra Piktus\\u2020, Fabio Petroni\\u2020, Vladimir Karpukhin\\u2020, Naman Goyal\\u2020, Heinrich K\\u00fcttler\\u2020\\n\\narXiv:2005.11401v4 [cs.CL] 12 Apr 2021\\n\\nMike Lewis\\u2020, Wen-tau Yih\\u2020, Tim Rockt\\u00e4schel\\u2020\\u2021, Sebastian Riedel\\u2020\\u2021, Douwe Kiela\\u2020\\n\\n\\u2020Facebook AI Research; \\u2021University College London; \\u22c6New York University;\\n\\nplewis@fb.com\\n\\n### Abstract\\n\\nLarge pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \\u2014 models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\\n\\n### Introduction\\n\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data [47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52]. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can\\u2019t straightforwardly provide insight into their predictions, and may produce \\u201challucinations\\u201d [38]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results.\\n---\\n## Define \\\"middle ear\\\"(x)\\n\\nThe middle ear includes the tympanic cavity and the three ossicles.\\n\\n## Question Answering:\\n\\n|Question Query|Query|Retriever p|\\u03b7|Document|Generator p|\\u03b8|\\n|---|---|---|---|---|---|---|\\n|Barack Obama was born in Hawaii.(x)|Encoder|(Non-Parametric)| |Index|(Parametric)|Answer Generation supports (y)|\\n\\n## Fact Verification: Fact Query\\n\\nThe Divine Comedy (x)\\n\\n## Jeopardy Question Generation:\\n\\nAnswer Query\\n\\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to find the top-K documents zi. For final prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents. but have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the \\u201cworkhorse of NLP,\\u201d i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned. There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stack-augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training. Our results highlight the benefits of combining parametric and non-parametric memory with generation for knowledge-intensive tasks\\u2014tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER [56] fact verification, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models\\u2019 knowledge as the world changes.\\n\\n## Methods\\n\\nWe explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever p\\u03b7(z|x) with parameters \\u03b7 that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator p\\u03b8(yi|x, z, y1:i\\u22121) parametrized\\n\\n1 Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transformers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n---\\n## by \\u03b8 that generates a current token based on a context of the previous i \\u2212 1 tokens y1:i\\u22121, the original input x and a retrieved passage z.\\n\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the p\\u03b7 and p\\u03b8 components, as well as the training and decoding procedure.\\n\\n### Models\\n\\n|RAG-Sequence Model|The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,|\\n|---|---|\\n| |pRAG-Sequence(y|x) \\u2248 p\\u03b7(z|x)p\\u03b8(y|x, z) = p\\u03b7(z|x) \\u03a3 p\\u03b8(yi|x, z, y1:i\\u22121) z\\u2208top-k(p(\\u00b7|x)) z\\u2208top-k(p(\\u00b7|x)) i|\\n|RAG-Token Model|In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we define:|\\n| |pRAG-Token(y|x) \\u2248 \\u03a3 z\\u2208top-k(p(\\u00b7|x)) p\\u03b7(z|x)p\\u03b8(yi|x, z, y1:i\\u22121)|\\n\\nFinally, we note that RAG can be used for sequence classification tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n\\n### Retriever: DPR\\n\\nThe retrieval component p\\u03b7(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: p\\u03b7(z|x) \\u221d exp d(z)\\u22a4q(x) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(p\\u03b7(\\u00b7|x)), the list of k documents z with highest prior probability p\\u03b7(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory.\\n\\n### Generator: BART\\n\\nThe generator component p\\u03b8(yi|x, z, y1:i\\u22121) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters \\u03b8 as the parametric memory henceforth.\\n\\n### Training\\n\\nWe jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a fine-tuning training corpus of input/output pairs (xj, yj), we\\n---\\nminimize the negative marginal log-likelihood of each target, j \\u2212log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder BERTq and the BART generator.\\n\\n## Decoding\\n\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate arg max y p(y|x).\\n\\n|RAG-Token|The RAG-Token model can be seen as a standard, autoregressive seq2seq generator with transition probability: p\\u2032 \\u03b8(yi|x, y1:i\\u22121) = z\\u2208top-k(p(\\u00b7|x)) p\\u03b7(zi|x)p\\u03b8(yi|x, zi, y1:i\\u22121) To decode, we can plug p\\u2032 \\u03b8(yi|x, y1:i\\u22121) into a standard beam decoder.|\\n|---|---|\\n|RAG-Sequence|For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using p\\u03b8(yi|x, z, y1:i\\u22121). This yields a set of hypotheses Y, some of which may not have appeared in the beams of all documents. To estimate the probability of a hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with p\\u03b7(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as \\u201cThorough Decoding.\\u201d For longer output sequences, |Y| can become large, requiring many forward passes. For more efficient decoding, we can make a further approximation that p\\u03b8(y|x, zi) \\u2248 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as \\u201cFast Decoding.\\u201d|\\n\\n## Experiments\\n\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k \\u2208 {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task.\\n\\n### Open-domain Question Answering\\n\\nOpen-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to \\u201cClosed-Book QA\\u201d approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\\n\\n### Abstractive Question Answering\\n\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG\\u2019s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n---\\nMSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as \\\"What is the weather in Volcano, CA?\\\" so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses.\\n\\nJeopardy Question Generation\\n\\nTo evaluate RAG\\u2019s generation abilities in a non-QA setting, we study open-domain question generation. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, \\\"The World Cup\\\" is the answer to the question \\\"In 1986 Mexico scored as the first country to host this international sports competition twice.\\\" As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task.\\n\\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for specificity. We define factuality as whether a statement can be corroborated by trusted external sources, and specificity as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options\\u2014question A is better, question B is better, both are good, or neither is good.\\n\\nFact Verification\\n\\nFEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unverifiable from Wikipedia alone. FEVER is a retrieval problem coupled with a challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models\\u2019 ability to handle classification rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren\\u2019t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n\\nResults\\n\\nOpen-domain Question Answering\\n\\n|Task|RAG|State-of-the-Art Models|\\n|---|---|---|\\n|All four open-domain QA tasks|RAG sets a new state of the art (only on the T5-comparable split for TQA)|RAG combines the generation flexibility of the \\u201cclosed-book\\u201d (parametric only) approaches and the performance of \\\"open-book\\\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized \\u201csalient span masking\\u201d pre-training [20]. It is worth noting that RAG\\u2019s retriever is initialized using DPR\\u2019s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based \\u201ccross-encoder\\u201d to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance.|\\n\\nThere are several advantages to generating answers even when it is possible to extract them. Documents with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading\\n---\\n## Table 1: Open-Domain QA Test Scores\\n\\n|Model|NQ|TQA|WQ|CT|\\n|---|---|---|---|---|\\n|Closed Book T5-11B [52]|34.5|- /50.1|37.4|-|\\n|Book T5-11B+SSM[52]|36.6|- /60.5|44.7|-|\\n|Open REALM [20]|40.4|- / -|40.7|46.8|\\n|Book DPR [26]|41.5|57.9/ -|41.1|50.6|\\n|RAG-Token|44.1|55.2/66.1|45.5|50.0|\\n|RAG-Seq.|44.5|56.8/68.0|45.2|52.2|\\n\\n## Table 2: Generation and classification Test Scores\\n\\n|Model|Jeopardy|MSMARCO|FVR3|FVR2|\\n|---|---|---|---|---|\\n|B-1|QB-1|R-L|B-1|Label Acc.|\\n|BART|15.1|19.7|38.2|41.6|\\n|RAG-Tok.|17.3|22.2|40.1|41.5|\\n|RAG-Seq.|14.7|21.4|40.8|44.2|\\n\\n4.2 Abstractive Question Answering\\n\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with specific information required to generate the reference answer, (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we find that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see \\u00a74.5).\\n\\n4.3 Jeopardy Question Generation\\n\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also find RAG generations to be more specific by a large margin. Table 3 shows typical generations from each model.\\n\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating \\u201cSun\\u201d, the posterior is high for document 2 which mentions \\u201cThe Sun Also Rises\\u201d. Similarly, document 1 dominates the posterior when \\u201cA Farewell to Arms\\u201d is generated. Intriguingly, after the first token of each book is generated, the document posterior flattens. This observation suggests that the generator can complete the titles without depending on specific documents. In other words, the model\\u2019s parametric knowledge is sufficient to complete the titles. We find evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \\\"The Sun. BART completes the generation \\\"The Sun Also Rises\\\" is a novel by this author of \\\"The Sun Also Rises\\\" indicating the title \\\"The Sun Also Rises\\\" is stored in BART\\u2019s parameters. Similarly, BART will complete the partial decoding \\\"The Sun Also Rises\\\" is a novel by this author of \\\"A with \\\"The Sun Also Rises\\\" is a novel by this author of \\\"A Farewell to Arms\\\". This example shows how parametric and non-parametric memories work together\\u2014the non-parametric component helps to guide the generation, drawing out specific knowledge stored in the parametric memory.\\n\\n4.4 Fact Verification\\n\\nTable 2 shows our results on FEVER. For 3-way classification, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-specific architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n---\\n## Document 1\\n\\nhis works are considered classics of American literature ... His wartime experiences formed the basis for his novel \\\"A Farewell to Arms\\\" (1929) ...\\n\\n## Document 2\\n\\n... artists of the 1920s \\\"Lost Generation\\\" expatriate community. His debut novel, \\\"The Sun Also Rises\\\", was published in 1926.\\n\\nFigure 2: RAG-Token document posterior p(zi|x, yi, y\\u2212i) for each generated token for input \\u201cHemingway\\\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating \\u201cA Farewell to Arms\\\" and for document 2 when generating \\u201cThe Sun Also Rises\\\".\\n\\n|Task|Input|Model|Generation|\\n|---|---|---|---|\\n|define middle ear|BART|?|The middle ear is the part of the ear between the middle ear and the nose.|\\n| |RAG-T| |The middle ear is the portion of the ear internal to the eardrum.|\\n|MS-MARCO|what currency needed in Scotland|BART|The currency needed in Scotland is Pound sterling.|\\n| |RAG-T| |Pound is the currency needed in Scotland.|\\n| |RAG-S| |The currency needed in Scotland is the pound sterling.|\\n| |BART|?|This state has the largest number of counties in the U.S.|\\n|Jeopardy Question Generation|Washington|RAG-T|It\\u2019s the only U.S. state named for a U.S. president|\\n| |RAG-S| |It\\u2019s the state where you\\u2019ll find Mount Rainier National Park|\\n| |BART|*|This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio|\\n|The Divine Comedy|RAG-T| |Dante\\u2019s \\\"Inferno\\\" is the first part of this epic poem|\\n| |RAG-S| |This 14th century work is divided into 3 sections: \\\"Inferno\\\", \\\"Purgatorio\\\" & \\\"Paradiso\\\"|\\n\\nFor 2-way classification, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved by RAG and gold evidence annotations. We find that the top retrieved document is from a gold article in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\\n\\n## Additional Results\\n\\nGeneration Diversity: Section 4.3 shows that RAG models are more factual and specific than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding, we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence\\u2019s generations are more diverse than RAG-Token\\u2019s, and both are significantly more diverse than BART without needing any diversity-promoting decoding.\\n\\nRetrieval Ablations: A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks.\\n\\nWe compare RAG\\u2019s dense retriever to a word overlap-based BM25 retriever. Here, we replace RAG\\u2019s retriever with a fixed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\n\\nIndex hot-swapping: An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n---\\n|Model|NQ|TQA|WQ|CT|Jeopardy-QGen|MSMarco|FVR-3|FVR-2|\\n|---|---|---|---|---|---|---|---|---|\\n|RAG-Token-BM25|29.7|41.5|32.1|33.1|17.5|22.3|55.5|48.4|75.1|91.6|\\n|RAG-Sequence-BM25|31.8|44.1|36.6|33.8|11.1|19.5|56.5|46.9|\\n|RAG-Token-Frozen|37.8|50.1|37.1|51.1|16.7|21.7|55.9|49.4|72.9|89.4|\\n|RAG-Sequence-Frozen|41.2|52.1|41.8|52.6|11.8|19.6|56.7|47.3|\\n|RAG-Token|43.5|54.8|46.5|51.9|17.9|22.6|56.2|49.4|74.5|90.6|\\n|RAG-Sequence|44.0|55.8|44.9|53.4|15.3|21.5|57.2|47.5|\\n\\n|Content|Page Number|\\n|---|---|\\n|Table 4: Human assessments for the Jeopardy Question Generation Task.| |\\n|Table 5: Ratio of distinct to total tri-grams for generation tasks.| |\\n|Table 6: Ablations on the dev set. As FEVER is a classification task, both RAG models are equivalent.| |\\n\\nBetween these dates and use a template \\u201cWho is {position}?\\u201d (e.g. \\u201cWho is the President of Peru?\\u201d) to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for 2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders). This shows we can update RAG\\u2019s world knowledge by simply replacing its non-parametric memory.\\n\\nEffect of Retrieving more documents: Models are trained with either 5 or 10 retrieved latent documents, and we do not observe significant differences in performance between them. We have the flexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n\\n| |NQ Answer Recall @ K|\\n|---|---|\\n|NQ Exact Match|80|\\n\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n\\nRelated Work: Single-Task Retrieval - Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering, fact checking, fact completion, long-form question answering, Wikipedia article generation, dialogue, translation, and language modeling. Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.\\n---\\n## General-Purpose Architectures for NLP\\n\\nPrior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks [60, 61] after fine-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models.\\n\\n## Learned Retrieval\\n\\nThere is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.\\n\\n## Memory-based Architectures\\n\\nOur document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model\\u2019s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval [9].\\n\\n## Retrieve-and-Edit approaches\\n\\nOur method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a final output. These approaches have proved successful in a number of domains including Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.\\n\\n## Discussion\\n\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG\\u2019s generation over purely parametric BART, finding RAG more factual and specific. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.\\n---\\n## Broader Impact\\n\\nThis work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it \\u201challucinate\\u201d less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.\\n\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [54]. Advanced language models may also lead to the automation of various jobs in the coming decades [16]. In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing.\\n\\n## Acknowledgments\\n\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.\\n\\n## References\\n\\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http://arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n[2] Petr Baudi\\u0161 and Jan \\u0160ediv` y. Modeling of pe question answering task in pe yodaqa system. In International Conference of pe Cross-Language Evaluation Forum for European Languages, pages 222\\u2013228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20.\\n[3] Jonapan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of pe 2013 Conference on Empirical Mepods in Natural Language Processing, pages 1533\\u20131544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anpology/D13-1160.\\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-ing&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159, 2020. URL https://arxiv.org/abs/2004.07159.\\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 1870\\u20131879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anpology/P17-1171.\\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonapan Berant. Coarse-to-fine question answering for long documents. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 209\\u2013220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anpology/P17-1020.\\n---\\nChristopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Comprehension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723. arXiv: 1710.10723.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of pe 2019 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anpology/N19-1423.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\\nMatpew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A New Q&A Dataset Augmented wip Context from a Search Engine. arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv: 1704.05179.\\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of pe 56p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 889\\u2013898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anpology/P18-1082.\\nAngela Fan, Yacine Jernite, Epan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of pe 57p Annual Meeting of pe Association for Computational Linguistics, pages 3558\\u20133567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/anpology/P19-1346.\\nAngela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers wip KNN-based composite memory, 2020. URL https://openreview.net/forum?id=H1gx1CNKPH.\\nThibault F\\u00e9vry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access wip entity supervision. ArXiv, abs/2004.07202, 2020. URL https://arxiv.org/abs/2004.07202.\\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wentau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710.\\nKatja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL http://arxiv.org/abs/1705.08807.\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pages 5133\\u20135140. AAAI press, 2018. 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018.\\nKelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of pe Association for Computational Linguistics, 6:437\\u2013450, 2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anpology/Q18-1031.\\n---\\n## References\\n\\n[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https://arxiv.org/abs/2002.08909.\\n[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 10052\\u201310062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf.\\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-edit-rerank text generation. In Proceedings of pe 58p Annual Meeting of pe Association for Computational Linguistics, pages 2532\\u20132538, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/anpology/2020.acl-main.228.\\n[23] Jeff Johnson, Matpijs Douze, and Herv\\u00e9 J\\u00e9gou. Billion-scale similarity search wip gpus. arXiv preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.\\n[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\\u20131611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anpology/P17-1147.\\n[25] Armand Joulin and Tomas Mikolov. Inferring algoripmic patterns wip stack-augmented recurrent nets. In Proceedings of pe 28p International Conference on Neural Information Processing Systems - Volume 1, NIPS\\u201915, page 190\\u2013198, Cambridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/5857-inferring-algoripmic-patterns-wip-stack-augmented-recurrent-nets.\\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization prough memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A mepod for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\\n[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matpew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. Transactions of pe Association of Computational Linguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf.\\n[30] Guillaume Lample, Alexandre Sablayrolles, Marc\\u2019 Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers wip product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\u2019 Alch\\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8548\\u20138559. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9061-large-memory-layers-wip-product-keys.pdf.\\n[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of pe 57p Annual Meeting of pe Association\\n---\\n## References\\n\\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of pe 2016 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, pages 110\\u2013119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anpology/N16-1014.\\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation wip optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL https://arxiv.org/abs/1909.03087.\\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation wip joint textual and phonetic embedding. In Proceedings of pe 57p Annual Meeting of pe Association for Computational Linguistics, pages 3044\\u20133049, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL https://www.aclweb.org/anpology/P19-1291.\\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hyg0vbWC-.\\n[37] Yury A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824\\u2013836, 2016. URL https://arxiv.org/abs/1603.09320.\\n[38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rockt\\u00e4schel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect pe verifiability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https://arxiv.org/abs/1911.03587.\\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\n[41] Nikita Moghe, Siddharpa Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploiting background knowledge for building conversation systems. In Proceedings of pe 2018 Conference on Empirical Mepods in Natural Language Processing, pages 2322\\u20132332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URL https://www.aclweb.org/anpology/D18-1255.\\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation systems. In Proceedings of pe 2018 Conference on Empirical Mepods in Natural Language Processing, pages 3950\\u20133959, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/anpology/D18-1429.\\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d\\u2019Avila Garcez, and Greg Wayne, editors, Proceedings of pe Workshop on Cognitive Computation: Integrating neural and symbolic.\\n---\\napproaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf.\\n\\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\\n\\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48\\u201353, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.org/anthology/N19-4009.\\n\\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402\\u20132411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\\n\\n[47] Fabio Petroni, Tim Rockt\\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\\u20132473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://www.aclweb.org/anthology/D19-1250.\\n\\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\\u00e4schel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models\\u2019 factual predictions. In Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum?id=025X0zPfn.\\n\\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training, 2018. URL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf.\\n\\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\\n\\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\\n\\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/2002.08910.\\n\\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333\\u2013389, April 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL https://doi.org/10.1561/1500000019.\\n\\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019.\\n\\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440\\u20132448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\\n---\\n# References\\n\\n[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of pe 2018 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809\\u2013819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://www.aclweb.org/anpology/N18-1074.\\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classification wip elastic weight consolidation. ArXiv, abs/2004.14366, 2020. URL https://arxiv.org/abs/2004.14366.\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanapan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998\\u20136008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaap Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329.\\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of pe 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\\u2013355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anpology/W18-5446.\\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3261\\u20133275. Curran Associates, Inc., 2019. URL https://arxiv.org/abs/1905.00537.\\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain question answering. In Sheila A. McIlraip and Kilian Q. Weinberger, editors, Proceedings of pe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), pe 30p innovative Applications of Artificial Intelligence (IAAI-18), and pe 8p AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5981\\u20135988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712.\\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in open-domain question answering. In ICLR, 2018. URL https://openreview.net/forum?id=rJl3yM-Ab.\\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.\\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and refine: Improved sequence generation models for dialogue. In Proceedings of pe 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 87\\u201392, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anpology/W18-5713.\\n---\\n## References\\n\\n[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anpony Moi, Pierric Cistac, Tim Rault, R\\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface\\u2019s transformers: State-of-pe-art natural language processing. ArXiv, abs/1910.03771, 2019.\\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-supervised question answering. In Proceedings of pe 2019 Conference on Empirical Mepods in Natural Language Processing and pe 9p International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2495\\u20132509, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anpology/D19-1253.\\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019. URL https://arxiv.org/abs/1909.03745.\\n---\\n## Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\n### Implementation Details\\n\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not find beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\n\\n### Human Evaluation\\n\\n|Which sentence is more factually true?|Select option|\\n|---|---|\\n|Noje: Scna Guesucn?|snterzt \\\"The8r Nuso Rists|\\n|IncicateFich|Farcncllic AM; on Im|\\n|Iclbwng sentarces Is Mca luclualy Injb[ealecllo|ZLbko Uaino cnoc urdoco|\\n\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \\\"view tool guide\\\".\\n\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instructions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.\\n\\n### Training Setup Details\\n\\nWe train all RAG models and BART baselines using Fairseq. We train with mixed precision floating point arithmetic, distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We find that doing Maximum Inner Product Search with FAISS is sufficiently fast on CPU, so we store document index vectors on CPU, requiring approximately 100 GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers, which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS\\u2019s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/\\n\\n2. https://github.com/pytorch/fairseq\\n\\n3. https://github.com/huggingface/transformers\\n---\\n## Further Details on Open-Domain QA\\n\\nFor open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to find matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we filter out answer candidates if they do not occur in top 1000 documents for the query.\\n\\n## CuratedTrec preprocessing\\n\\nThe answers for CuratedTrec are given in the form of regular expressions, which has been suggested as a reason why it is unsuitable for answer-generation models [20]. To overcome this, we use a pre-processing step where we first retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\n\\n## TriviaQA Evaluation setups\\n\\nThe open-domain QA community customarily uses public development datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading comprehension purposes. We report our results using the datasets splits used in DPR [26], which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA official Wikipedia test set instead. F\\u00e9vry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See appendix of [14]). We report results on both test sets to enable fair comparison to both approaches. We find that our performance is much higher using the official Wiki test set, rather than the more conventional open-domain test set, which we attribute to the official Wiki test set questions being simpler to answer from Wikipedia.\\n\\n## Further Details on FEVER\\n\\nFor FEVER classification, we follow the practice from [32], and first re-generate the claim, and then classify using the representation of the final hidden state, before finally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The first is to classify the claim as either \\\"Supported\\\", \\\"Refuted\\\" or \\\"Not Enough Info\\\", which is the task we explore in the main paper. FEVER\\u2019s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classification prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work.\\n\\n## Null Document Probabilities\\n\\nWe experimented with adding \\\"Null document\\\" mechanism to RAG, similar to REALM [20] in order to model cases where no useful information could be retrieved for a given input. Here, if k documents were retrieved, we would additionally \\\"retrieve\\\" an empty document and predict a logit for the null document, before marginalizing over k + 1 predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not find that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to benefit from retrieval, suggesting that null document mechanisms may not be necessary for RAG.\\n\\n## Parameters\\n\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable.\\n---\\n|Task|Train|Development|Test|\\n|---|---|---|---|\\n|Natural Questions|79169|8758|3611|\\n|TriviaQA|78786|8838|11314|\\n|WebQuestions|3418|362|2033|\\n|CuratedTrec|635|134|635|\\n|Jeopardy Question Generation|97392|13714|26849|\\n|MS-MARCO|153726|12468|101093*|\\n|FEVER-3-way|145450|10000|10000|\\n|FEVER-2-way|96966|6666|6666|\\n\\nparameters. The best performing \\\"closed-book\\\" (parametric only) open-domain QA model is T5-11B\\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit floating\\npoint precision to manage memory and disk footprints.\\n\\n## Retrieval Collapse\\n\\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the\\nretrieval component would \\u201ccollapse\\u201d and learn to retrieve the same documents regardless of the\\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\\nin less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results\\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\\n\\n## Number of instances per dataset\\n\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\",\nasyncio.run(query_info_with_gpt(paper_id, arxiv_paper_markdown, arxiv_metadata, user_query))","block_group":"af90ef5021e045349b44874359e92727","execution_count":241,"outputs":[{"output_type":"execute_result","execution_count":241,"data":{"text/plain":"'RAG models combine pre-trained language models with a Wikipedia index for NLP tasks, achieving state-of-the-art results on QA tasks.'"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/eef57ce4-3ba1-427e-a9d2-5229d28d3927"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"68c2a53f59e34471ac18e89954f9f5b9","deepnote_cell_type":"text-cell-h3"},"source":"### Relevance score (placeholder)","block_group":"f66c6c29fca947318e013ef5f42db0e8"},{"cell_type":"markdown","metadata":{"id":"v2O3rIRU34-D","deepnote_app_block_visible":true,"cell_id":"46b1b784cc1143cfa95c6439fedb8c62","deepnote_cell_type":"markdown"},"source":"# Processing loops","block_group":"39dc0b3d42fa46779e8ede7b01488c2c"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"898b5400689f45138bb79c6679624950","deepnote_cell_type":"text-cell-p"},"source":"Extract search results from Google based on user query","block_group":"53ccfb646a6042d58d42f1ae63a83842"},{"cell_type":"code","metadata":{"id":"eLiFFElg4BmL","colab":{"height":211,"base_uri":"https://localhost:8080/"},"outputId":"e0289559-f816-4108-c6bc-acb3769ac955","source_hash":"2e2737ef","execution_start":1710483539159,"execution_millis":83365,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"05fbe427a60b4dccb97f79f14b4d90c7","deepnote_cell_type":"code"},"source":"import json\nimport time\n\nclass PageEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, Page):\n            return obj.dict()  # Convert Page to a dictionary\n        elif isinstance(obj, Paper):\n            return obj.dict()  # Convert Paper to a dictionary\n        elif isinstance(obj, Link):\n            return obj.dict()  # Convert Link to a dictionary\n        return json.JSONEncoder.default(self, obj)  # Handle other types\n\ndef fetch_and_process(link, query, title, snippet, job_id):\n    conn = None\n    try:\n        conn = connection()  # Open a new connection\n        c = conn.cursor()  # Create a new cursor\n\n        c.execute(\"SELECT scraping_status, html FROM google_search_results WHERE url = %s\", (link,))\n        result = c.fetchone()\n        if result and result[0] == '200':\n            print(f\"Status: {result[0]}, already fetched for URL: {link}\")\n            html_content = result[1].replace('\\x00', '')  # Sanitize HTML content from database\n        else:\n            response = fetch_url_content(link)\n            print(f\"Status:{response['status']} for URL: {link}\")\n            html_content = response['soup'].decode('utf-8', 'replace') if response['status'] == 200 else \"\"\n            html_content = html_content.replace('\\x00', '')\n            insert_scraping_results(link, html_content, str(response['status']), query, title, snippet, job_id)\n\n        if html_content:\n            insert_arxiv_links_into_db(html_content, query, job_id)  # Adjust 'insert_arxiv_links_into_db' to take 'conn' and 'c' as additional parameters\n\n    except psycopg2.OperationalError as e:\n        print(f\"Database operation failed for URL: {link}, Error: {e}\")\n        if conn:\n            conn.rollback()  # Roll back any changes due to error\n\n    finally:\n        if c:\n            c.close()  # Close the cursor\n        if conn:\n            conn.close()  # Close the connection\n\ndef search_and_fetch_google(job_id):\n    conn = connection()  # Open a new connection\n    c = conn.cursor()  # Create a new cursor\n\n    c.execute(\"SELECT query, keyword_search_queries, paper_search_queries FROM jobs WHERE job_id = %s\", (job_id,))\n    job = c.fetchone()\n    if job:\n        # Unpack the fields from the job row\n        query, keyword_search_queries, paper_search_queries = job\n        # Convert keyword_search_queries from JSON format to Python list\n        paper_search_queries = json.loads(paper_search_queries) if paper_search_queries else []\n        keyword_search_queries = json.loads(keyword_search_queries)\n        all_search_queries = [query] + keyword_search_queries\n        print(f\"General Queries: {all_search_queries}, Paper Specific Queries: {paper_search_queries}\")\n    else:\n        print(f\"No job found with job_id {job_id}\")\n        return\n\n    # Now that you have the search queries, iterate over them\n    for search_query in all_search_queries:\n        search_results = search_google(search_query)  # Pass each search query to your search function\n        print(search_query, search_results)\n\n        # Sequential execution\n        for result in search_results:  # Each 'result' is a dictionary\n            try:\n                # Extract the URL from the result dictionary\n                url = result['link']  # Correctly access the 'link' from the dictionary\n                # Optionally, you can also pass 'title' and 'snippet' if needed\n                title = result['title']\n                snippet = result['snippet']\n                # Assuming fetch_and_process can handle these additional data, adjust accordingly\n                data = fetch_and_process(url, query, title, snippet, job_id)  # Modify this line as necessary based on your function's parameters\n            except Exception as exc:\n                print(f'fetch_and_process exception: {exc}')\n\n    # Second loop: Iterate over paper-specific search queries\n    for search_query in paper_search_queries:\n        search_results = search_google_for_specific_papers(search_query)  # Pass each search query to your search function\n        print(search_query, search_results)\n\n    print('Finished extracting search results pages')\n\n# Example usage\njob_id = 3\nsearch_and_fetch_google(job_id)\n","block_group":"5fc3839f5cad422fbec8edc389da989a","execution_count":261,"outputs":[{"name":"stdout","text":"Keyword search queries: ['Top academic papers on LLMs', 'most cited academic papers on LLMs', 'peer-reviewed research on large language models', 'high impact publications on LLMs']\nTop academic papers on LLMs [{'link': 'https://www.topbots.com/top-llm-research-papers-2023/', 'title': '10 Transformative LLM Research Papers of 2023 from ...', 'snippet': 'Top LLM Research Papers 2023 · 1. LLaMA by Meta AI · 2. LLaMA 2 by Meta AI · 3. GPT-4 by OpenAI · 4. Sparks of AGI by Microsoft · 5. BLIP-2 by ...'}, {'link': 'https://medium.com/@thedatabeast/top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f', 'title': 'Top 10 Breakthrough Research Papers on Large ...', 'snippet': 'Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications.'}, {'link': 'https://levelup.gitconnected.com/best-papers-on-large-language-models-ac01b13b94b3', 'title': 'Best Papers on Large Language Models (LLMs)', 'snippet': 'Best Papers on Large Language Models (LLMs) ; Papers to understand architectures of LLMs · GPT-3: Its Nature, Scope, Limits, and Consequences ; Improve performance ...'}, {'link': 'https://community.openai.com/t/foundational-must-read-gpt-llm-papers/197003', 'title': 'Foundational must read GPT/LLM papers', 'snippet': 'Initializing a new thread on the very best, must read, well-written, papers on Large Language Model capabilities, limits, and use.'}, {'link': 'https://www.reddit.com/r/MLQuestions/comments/ze9e5x/can_anyone_recommend_an_llm_that_handles_research/', 'title': 'Can anyone recommend an LLM that handles research ...', 'snippet': 'Can anyone recommend an LLM that handles research papers well? · GameStop · Moderna · Pfizer · Johnson & Johnson · AstraZeneca · Walgreens · Best Buy ...'}, {'link': 'https://analyticsindiamag.com/13-not-to-miss-research-papers-on-llms/', 'title': '13 Not-to-Miss Research Papers on LLMs', 'snippet': '13 Not-to-Miss Research Papers on LLMs · Attention is All You Need · A Neural Probabilistic Language Model · Training language models to follow ...'}, {'link': 'https://github.com/Hannibal046/Awesome-LLM', 'title': 'Awesome-LLM: a curated list of Large Language Model', 'snippet': 'Here is a curated list of papers about large language models, especially relating to ChatGPT. It also contains frameworks for LLM training, tools to deploy LLM, ...'}, {'link': 'https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023', 'title': 'Ten Noteworthy AI Research Papers of 2023 - Ahead of AI', 'snippet': 'This article is a compilation of 10 noteworthy AI research papers of 2023.'}, {'link': 'https://www.promptingguide.ai/papers', 'title': 'Papers', 'snippet': 'The following are the latest papers (sorted by release date) on prompt engineering for large language models (LLMs). We update the list of papers on a daily/ ...'}]\nStatus: 200, already fetched for URL: https://www.topbots.com/top-llm-research-papers-2023/\nStatus: 200, already fetched for URL: https://medium.com/@thedatabeast/top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\nStatus: 200, already fetched for URL: https://levelup.gitconnected.com/best-papers-on-large-language-models-ac01b13b94b3\nStatus: 200, already fetched for URL: https://community.openai.com/t/foundational-must-read-gpt-llm-papers/197003\nFailed to retrieve the page. Status code: 403\nStatus:403 for URL: https://www.reddit.com/r/MLQuestions/comments/ze9e5x/can_anyone_recommend_an_llm_that_handles_research/\nURL already exists in google_search_results. Skipping insert.\nStatus: 200, already fetched for URL: https://analyticsindiamag.com/13-not-to-miss-research-papers-on-llms/\nStatus: 200, already fetched for URL: https://github.com/Hannibal046/Awesome-LLM\nStatus: 200, already fetched for URL: https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023\nStatus: 200, already fetched for URL: https://www.promptingguide.ai/papers\nCleaned arXiv links[2]: ['https://arxiv.org/abs/2308.04889', 'https://arxiv.org/abs/2402.06196']\nSuccessfully inserted records associated with the query 'most cited academic papers on LLMs' into the database. Links: ['https://arxiv.org/abs/2308.04889', 'https://arxiv.org/abs/2402.06196']\nmost cited academic papers on LLMs [{'link': 'https://www.topbots.com/top-llm-research-papers-2023/', 'title': '10 Transformative LLM Research Papers of 2023 from ...', 'snippet': 'Top LLM Research Papers 2023 · 1. LLaMA by Meta AI · 2. LLaMA 2 by Meta AI · 3. GPT-4 by OpenAI · 4. Sparks of AGI by Microsoft · 5. BLIP-2 by ...'}, {'link': 'https://levelup.gitconnected.com/best-papers-on-large-language-models-ac01b13b94b3', 'title': 'Best Papers on Large Language Models (LLMs)', 'snippet': 'In this article, we will look into some of the must-read papers which inspired these kinds of Large Langue Models (LLMs). Papers to understand architectures of ...'}, {'link': 'https://medium.com/@thedatabeast/top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f', 'title': 'Top 10 Breakthrough Research Papers on Large ...', 'snippet': 'Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications.'}, {'link': 'https://community.openai.com/t/foundational-must-read-gpt-llm-papers/197003', 'title': 'Foundational must read GPT/LLM papers', 'snippet': 'Initializing a new thread on the very best, must read, well-written, papers on Large Language Model capabilities, limits, and use.'}, {'link': 'https://analyticsindiamag.com/13-not-to-miss-research-papers-on-llms/', 'title': '13 Not-to-Miss Research Papers on LLMs', 'snippet': '13 Not-to-Miss Research Papers on LLMs · Attention is All You Need · A Neural Probabilistic Language Model · Training language models to follow ...'}, {'link': 'https://arxiv.org/abs/2308.04889', 'title': 'What are the most influential current AI Papers?', 'snippet': 'Core issues investigated in the most heavily cited papers are: LLM efficiency, evaluation techniques, ethical considerations, embodied agents, ...'}, {'link': 'https://typeset.io/questions/what-are-the-most-cited-papers-in-the-area-of-llm-generated-22dx0v23xw', 'title': 'What are the most cited papers in the area of LLM ...', 'snippet': 'What are the most cited papers in the area of LLM generated text detection? · Feature (computer vision) · Convolutional neural network · Maximally ...'}, {'link': 'https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022', 'title': 'Must read: the 100 most cited AI papers in 2022', 'snippet': 'Must read: the 100 most cited AI papers in 2022. Updated: Mar 8, 2023. Who Is publishing the most Impactful AI research right now? With the ...'}, {'link': 'https://arxiv.org/abs/2402.06196', 'title': '[2402.06196] Large Language Models: A Survey', 'snippet': 'In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their ...'}, {'link': 'https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023', 'title': 'Ten Noteworthy AI Research Papers of 2023 - Ahead of AI', 'snippet': 'This article is a compilation of 10 noteworthy AI research papers of 2023.'}]\nStatus: 200, already fetched for URL: https://www.topbots.com/top-llm-research-papers-2023/\nStatus: 200, already fetched for URL: https://levelup.gitconnected.com/best-papers-on-large-language-models-ac01b13b94b3\nStatus: 200, already fetched for URL: https://medium.com/@thedatabeast/top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\nStatus: 200, already fetched for URL: https://community.openai.com/t/foundational-must-read-gpt-llm-papers/197003\nStatus: 200, already fetched for URL: https://analyticsindiamag.com/13-not-to-miss-research-papers-on-llms/\nStatus: 200, already fetched for URL: https://arxiv.org/abs/2308.04889\nStatus: 200, already fetched for URL: https://typeset.io/questions/what-are-the-most-cited-papers-in-the-area-of-llm-generated-22dx0v23xw\nStatus: 200, already fetched for URL: https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022\nStatus: 200, already fetched for URL: https://arxiv.org/abs/2402.06196\nStatus: 200, already fetched for URL: https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023\nCleaned arXiv links[1]: ['https://arxiv.org/abs/2307.06435']\nSuccessfully inserted records associated with the query 'peer-reviewed research on large language models' into the database. Links: ['https://arxiv.org/abs/2307.06435']\npeer-reviewed research on large language models [{'link': 'https://www.nature.com/articles/s43856-023-00370-1', 'title': 'The future landscape of large language models in medicine', 'snippet': 'Large language models (LLMs) are artificial intelligence (AI) tools specifically trained to process and generate text.'}, {'link': 'https://biodatamining.biomedcentral.com/articles/10.1186/s13040-023-00339-9', 'title': 'ChatGPT and large language models in academia', 'snippet': '... peer review critiques for grant applications and R&D contract proposals. ... ChatGPT utility in healthcare education, research, and practice: ...'}, {'link': 'https://www.researchgate.net/publication/372278221_Large_Language_Models_A_Comprehensive_Survey_of_its_Applications_Challenges_Limitations_and_Future_Prospects', 'title': '(PDF) Large Language Models: A Comprehensive Survey ...', 'snippet': 'Preprints and early-stage research may not have been peer reviewed yet. ... review of large language models research from 2017 to 2023,” arXiv.'}, {'link': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10485814/', 'title': 'Editorial – The Use of Large Language Models in Science', 'snippet': 'To present research findings through well-written, peer-reviewed scientific manuscripts is among the ultimate goals of any investigational research study.'}, {'link': 'https://arxiv.org/pdf/2307.06435', 'title': 'A Comprehensive Overview of Large Language Models', 'snippet': 'Our self- contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in ...'}, {'link': 'https://phys.org/news/2023-10-large-language-peer-review.html', 'title': 'Large language models prove helpful in peer-review process', 'snippet': 'They tested their theory by comparing reviewer feedback on several thousand papers from Nature journals and the International Conference on ...'}, {'link': 'https://liberalarts.utexas.edu/prc/news/nature-reviews-psychology-large-language-models-and-their-impact-on-the-field', 'title': 'Nature Reviews Psychology: Large Language Models and ...', 'snippet': \"This article explores the transformative potential of Large Language Models (LLMs), such as OpenAI's GPT-4, Google's Bard, and Meta's LLaMa, in ...\"}, {'link': 'https://sloanreview.mit.edu/article/the-working-limitations-of-large-language-models/', 'title': 'The Working Limitations of Large Language Models', 'snippet': 'Large language models (LLMs) ... peer-reviewed papers, and one was a paper that does not exist. ... Researchers are also training specialized models ...'}, {'link': 'https://www.nature.com/articles/s41586-023-06291-2', 'title': 'Large language models encode clinical knowledge', 'snippet': 'SciBERT: a pretrained language model for scientific ... Peer review. Peer review information. Nature ... Nature Reviews Bioengineering Research ...'}]\nStatus: 200, already fetched for URL: https://www.nature.com/articles/s43856-023-00370-1\nStatus: 200, already fetched for URL: https://biodatamining.biomedcentral.com/articles/10.1186/s13040-023-00339-9\nFailed to retrieve the page. Status code: 403\nStatus:403 for URL: https://www.researchgate.net/publication/372278221_Large_Language_Models_A_Comprehensive_Survey_of_its_Applications_Challenges_Limitations_and_Future_Prospects\nURL already exists in google_search_results. Skipping insert.\nStatus: 200, already fetched for URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10485814/\nStatus: 200, already fetched for URL: https://arxiv.org/pdf/2307.06435\nStatus: 200, already fetched for URL: https://phys.org/news/2023-10-large-language-peer-review.html\nStatus: 200, already fetched for URL: https://liberalarts.utexas.edu/prc/news/nature-reviews-psychology-large-language-models-and-their-impact-on-the-field\nStatus: 200, already fetched for URL: https://sloanreview.mit.edu/article/the-working-limitations-of-large-language-models/\nStatus: 200, already fetched for URL: https://www.nature.com/articles/s41586-023-06291-2\nCleaned arXiv links[1]: ['https://arxiv.org/abs/2307.10700']\nSuccessfully inserted records associated with the query 'high impact publications on LLMs' into the database. Links: ['https://arxiv.org/abs/2307.10700']\nhigh impact publications on LLMs [{'link': 'https://medium.com/@thedatabeast/top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f', 'title': 'Top 10 Breakthrough Research Papers on Large ...', 'snippet': 'Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications.'}, {'link': 'https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models', 'title': '16 of the best large language models', 'snippet': 'Top current LLMs. Below are some of the most relevant large language models today. They do natural language processing and influence the ...'}, {'link': 'https://www.nature.com/articles/s43856-023-00370-1', 'title': 'The future landscape of large language models in medicine', 'snippet': 'Here, we provide an overview of how LLMs could impact access to scientific knowledge, scientific writing, and programming tasks. Access to ...'}, {'link': 'https://community.openai.com/t/foundational-must-read-gpt-llm-papers/197003', 'title': 'Foundational must read GPT/LLM papers', 'snippet': 'Initializing a new thread on the very best, must read, well-written, papers on Large Language Model capabilities, limits, and use.'}, {'link': 'https://www.topbots.com/top-llm-research-papers-2023/', 'title': '10 Transformative LLM Research Papers of 2023 from ...', 'snippet': 'Top LLM Research Papers 2023 · 1. LLaMA by Meta AI · 2. LLaMA 2 by Meta AI · 3. GPT-4 by OpenAI · 4. Sparks of AGI by Microsoft · 5. BLIP-2 by ...'}, {'link': 'https://link.springer.com/article/10.1007/s00146-023-01791-1', 'title': 'Friend or foe? Exploring the implications of large language ...', 'snippet': 'While the impact on education has been a primary focus, there is limited empirical research on the effects of large language models (LLMs) and ...'}, {'link': 'https://vectara.com/blog/top-large-language-models-llms-gpt-4-llama-gato-bloom-and-when-to-choose-one-over-the-other/', 'title': 'Top Large Language Models (LLMs): GPT-4, LLaMA 2, ...', 'snippet': \"Here's a list of some of the top LLMs announced and released in the last few years, as well as our recommended picks for different use-cases and constraints.\"}, {'link': 'https://arxiv.org/abs/2307.10700', 'title': 'Topics, Authors, and Institutions in Large Language Model ...', 'snippet': '2018-2022. First, we study disciplinary shifts: LLM research increasingly considers societal impacts, evidenced by 20x growth in LLM submissions ...'}, {'link': 'https://ai.nejm.org/doi/full/10.1056/AIe2300128', 'title': 'Why We Support and Encourage the Use of Large Language ...', 'snippet': 'As a consequence, some publication venues have elected to prohibit the use of LLMs ... High-Impact Medical Journals Reflect Negative Sentiment ...'}, {'link': 'https://www.europol.europa.eu/publications-events/publications/chatgpt-impact-of-large-language-models-law-enforcement', 'title': 'the impact of Large Language Models on Law Enforcement', 'snippet': 'Large Language Models (LLMs) such as ChatGPT are undergoing rapid advances and have now entered the mainstream. This marks a significant ...'}]\nStatus: 200, already fetched for URL: https://medium.com/@thedatabeast/top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\nStatus: 200, already fetched for URL: https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models\nStatus: 200, already fetched for URL: https://www.nature.com/articles/s43856-023-00370-1\nStatus: 200, already fetched for URL: https://community.openai.com/t/foundational-must-read-gpt-llm-papers/197003\nStatus: 200, already fetched for URL: https://www.topbots.com/top-llm-research-papers-2023/\nStatus: 200, already fetched for URL: https://link.springer.com/article/10.1007/s00146-023-01791-1\nStatus: 200, already fetched for URL: https://vectara.com/blog/top-large-language-models-llms-gpt-4-llama-gato-bloom-and-when-to-choose-one-over-the-other/\nStatus: 200, already fetched for URL: https://arxiv.org/abs/2307.10700\nFailed to retrieve the page. Status code: 403\nStatus:403 for URL: https://ai.nejm.org/doi/full/10.1056/AIe2300128\nURL already exists in google_search_results. Skipping insert.\nStatus: 200, already fetched for URL: https://www.europol.europa.eu/publications-events/publications/chatgpt-impact-of-large-language-models-law-enforcement\nFinished extracting search results pages\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/da880ca9-55ba-4fc7-bec3-6cba335cfcf9"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"586ba61c8b7b426cb5d3cbe250911e75","deepnote_cell_type":"text-cell-p"},"source":"Extracting and processing arxiv papers: pdf, markdown, metadata, citations, versions","block_group":"c09889aa5b43411483a86049e10570f4"},{"cell_type":"code","metadata":{"source_hash":"91acf63f","execution_start":1710484142888,"execution_millis":804,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"43a127a8906a410d9fa9b3f2093bb753","deepnote_cell_type":"code"},"source":"import json\nimport psycopg2.extras\nimport concurrent.futures\n\ndef get_scholar_citations_versions_parallel(arxiv_link):\n    try:\n        # Fetch citations and versions\n        number_of_citations, number_of_versions = get_scholar_citations_versions(arxiv_link)\n        return arxiv_link, number_of_citations, number_of_versions\n    except Exception as e:\n        print(f\"An error occurred while processing paper {arxiv_link}: {e}\")\n        return arxiv_link, None, None  # Return None values if error\n\ndef get_scholar_citations_versions_loop(job_id):\n    conn = connection()  # Ensure this is a valid connection function\n    c = conn.cursor()\n\n    try:\n        # Fetch the first 30 Query_Papers rows associated with the given query\n        c.execute(\"\"\"\n            SELECT id, arxiv_link FROM Query_Papers \n            WHERE job_id = %s \n            LIMIT 30\n        \"\"\", (job_id,))\n        query_papers_to_update = c.fetchall()\n\n        # Initialize the lists for batch updates\n        papers_updates = []\n        query_papers_updates = []\n\n        # Prepare for parallel execution\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            # Map arxiv_links to future results\n            future_to_arxiv_link = {executor.submit(get_scholar_citations_versions_parallel, arxiv_link): (paper_id, arxiv_link) \n                                    for paper_id, arxiv_link in query_papers_to_update}\n\n            # Collect results as they complete\n            for future in concurrent.futures.as_completed(future_to_arxiv_link):\n                paper_id, arxiv_link = future_to_arxiv_link[future]\n                try:\n                    arxiv_link, number_of_citations, number_of_versions = future.result()\n                    if number_of_citations is not None and number_of_versions is not None:\n                        # Append data for batch update in Papers table\n                        papers_updates.append((number_of_citations, number_of_versions, arxiv_link))\n                        # Create JSON object with citations and versions, append for batch update in Query_Papers\n                        paper_stats_json = json.dumps({'citations': number_of_citations, 'versions': number_of_versions})\n                        query_papers_updates.append((paper_stats_json, paper_id))\n                except Exception as e:\n                    print(f\"An error occurred while processing future for paper {arxiv_link}: {e}\")\n\n        # Perform batch updates\n        psycopg2.extras.execute_batch(c, \"UPDATE Papers SET citations = %s, versions = %s WHERE arxiv_link = %s\",\n                                      papers_updates)\n        psycopg2.extras.execute_batch(c, \"UPDATE Query_Papers SET paper_stats = %s WHERE id = %s\",\n                                      query_papers_updates)\n\n        # Commit all changes\n        conn.commit()\n\n    except Exception as e:\n        # If an exception occurs, roll back all database changes\n        conn.rollback()\n        print(f\"An error occurred while fetching Query_Papers for the job_id '{job_id}': {e}\")\n\n    finally:\n        # Ensure resources are cleaned up\n        c.close()\n        conn.close()\n\n# Example usage\njob_id = 3\nget_scholar_citations_versions_loop(job_id)","block_group":"cbb5a5779c464f8683c681341813c442","execution_count":263,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"3a1ec0a9","execution_start":1710484725392,"execution_millis":718,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"aa08d8877922484d98ca1c23e525c895","deepnote_cell_type":"code"},"source":"def fetch_arxiv_paper_from_url_loop(job_id):\n    conn = connection()  # Ensure this is a function that returns a DB connection\n    c = conn.cursor()\n\n    # Select records from Query_Papers related to the specific query and with final_rank between 1 and 10\n    try:\n        c.execute(\"\"\"\n            SELECT Query_Papers.id, Papers.paper_title, Papers.arxiv_link\n            FROM Query_Papers\n            JOIN Papers ON Query_Papers.arxiv_link = Papers.arxiv_link\n            WHERE Query_Papers.job_id = %s AND final_rank BETWEEN 1 AND 20\n            ORDER BY final_rank ASC\n        \"\"\", (job_id,))\n\n        papers_to_update = c.fetchall()\n\n        for q_id, paper_title, arxiv_link in papers_to_update:\n            print(f\"Updating missing information for paper: {paper_title}\")\n            if arxiv_link:\n                try:\n                    # Fetch paper metadata from arXiv\n                    xml_data, pdf_url, title, file_name, abstract, published_date, authors = fetch_arxiv_paper_from_url(arxiv_link)\n\n                    # Update Papers table with fetched metadata\n                    c.execute(\"\"\"\n                        UPDATE Papers \n                        SET arxiv_title = %s, arxiv_abstract = %s, arxiv_metadata = %s, arxiv_filename = %s \n                        WHERE arxiv_link = %s\n                    \"\"\", (title, abstract, xml_data, file_name, arxiv_link))\n\n                    # Update Query_Papers table with filtered metadata and download link\n                    paper_metadata_filtered = {'title': title, 'abstract': abstract, 'published_date': published_date, 'authors': authors}\n                    c.execute(\"\"\"\n                        UPDATE Query_Papers \n                        SET paper_metadata_filtered = %s, download_link = %s \n                        WHERE id = %s\n                    \"\"\", (json.dumps(paper_metadata_filtered), pdf_url, q_id))\n\n                    # Commit the transaction\n                    conn.commit()\n\n                except Exception as e:\n                    print(f\"An error occurred while updating paper {paper_title}: {e}\")\n            else:\n                print(f\"No arXiv link found for paper: {paper_title}\")\n    except Exception as e:\n        print(f\"An error occurred while fetching Query_Papers for the job_id '{job_id}': {e}\")\n    finally:\n        if conn is not None:\n            c.close()\n            conn.close()\n\n# Example usage\njob_id = 3\nfetch_arxiv_paper_from_url_loop(job_id)","block_group":"d569342064174dadba839f4e063a2900","execution_count":266,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"1cef4275","execution_start":1710468143891,"execution_millis":69,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"2111b9a80cfc444f9cb9b4ca6ad28cfc","deepnote_cell_type":"code"},"source":"def download_pdf_loop(query):\n    conn = connection()  # Make sure this is a function that returns a DB connection\n    c = conn.cursor()\n\n    # Select records from Query_Papers related to the specific query and with final_rank between 1 and 10\n    try:\n        c.execute(\"\"\"\n            SELECT Query_Papers.id, Papers.paper_title, Papers.arxiv_link, Papers.arxiv_filename\n            FROM Query_Papers\n            JOIN Papers ON Query_Papers.arxiv_link = Papers.arxiv_link\n            WHERE Query_Papers.query = %s AND final_rank BETWEEN 1 AND 10\n            ORDER BY final_rank ASC\n        \"\"\", (query,))\n\n        papers_metadata = c.fetchall()\n\n        for id, paper_title, arxiv_link, file_name in papers_metadata:\n            print(f\"Downloading PDF for paper: {paper_title}\")\n            if arxiv_link and file_name:\n                # Typically, the PDF URL is derived from the arXiv link, adjust as necessary\n                pdf_url = f'https://arxiv.org/pdf/{arxiv_link.split(\"/\")[-1]}.pdf'  # Adjust based on actual URL format\n\n                # Download the PDF\n                file_path_or_error = download_pdf(pdf_url, file_name)\n                if 'Failed' not in file_path_or_error:\n                    print(f\"Download successful: {file_path_or_error}\")\n                else:\n                    print(f\"Download failed for paper: {paper_title}\")\n            else:\n                print(f\"No valid arXiv link or filename found for paper: {paper_title}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        if conn is not None:\n            c.close()\n            conn.close()\n","block_group":"61966d2d6e6e4734a0352ba3647767f7","execution_count":245,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"693c1d13","execution_start":1710468143895,"execution_millis":65,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"00335e774c5c44d999561da5cf3af97d","deepnote_cell_type":"code"},"source":"def convert_pdf_to_markdown_loop():\n    # Connect to SQLite database\n    conn = connection()\n    c = conn.cursor()\n\n    # Update papers with missing arxiv_paper_markdown\n    c.execute(\"SELECT id, arxiv_filename FROM Papers WHERE (arxiv_paper_markdown IS NULL OR arxiv_paper_markdown = '' OR arxiv_paper_markdown = 'None') AND arxiv_filename IS NOT NULL AND arxiv_filename != ''\")\n    papers_to_update = c.fetchall()\n\n    for id, arxiv_filename in papers_to_update:\n        try:\n            # Convert PDF to Markdown\n            markdown_content = convert_pdf_to_markdown(arxiv_filename)\n\n            # Update Papers table with Markdown content\n            c.execute(\"UPDATE Papers SET arxiv_paper_markdown = %s WHERE id = %s\", (markdown_content, rowid))\n            conn.commit()\n        except Exception as e:\n            print(f\"An error occurred while updating paper id {id}: {e}\")\n\n    print(\"Finished converting pdfs to markdown loop\")\n    if conn is not None:\n        # Close the cursor and connection\n        c.close()\n        conn.close()\n","block_group":"390b08c028f9461aae951d96b05d5dc3","execution_count":246,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"deepnote_app_block_visible":true,"cell_id":"ec0ba9d573c542f2ba4a9e45ee4ceb1e","deepnote_cell_type":"markdown"},"source":"Process papers against user query to arrive at the relevant answer and relevance score","block_group":"92258df3d96749a5b6982c9b3deb3d43"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"0ddd61f626494ad7bbf5a0eca4f777fe","deepnote_cell_type":"text-cell-p"},"source":"abstract","block_group":"265c43bea8ae4d399f461ee461120134"},{"cell_type":"code","metadata":{"source_hash":"8bb42ecf","execution_start":1710468143905,"execution_millis":571,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"9ad98a148db84e4091d11f062312901b","deepnote_cell_type":"code"},"source":"import asyncio\nimport aiopg\n\n# Construct DSN (Data Source Name) string from environment variables\ndsn = (\n    f\"dbname={os.environ['MY_INTEGRATION_DATABASE']} \"\n    f\"user={os.environ['MY_INTEGRATION_USER']} \"\n    f\"password={os.environ['MY_INTEGRATION_PASSWORD']} \"\n    f\"host={os.environ['MY_INTEGRATION_HOST']} \"\n    f\"port={os.environ['MY_INTEGRATION_PORT']}\"\n)\n\nasync def LLM_process_abstract_loop(query):\n    async with aiopg.create_pool(dsn) as pool:  # Use a connection pool\n        async with pool.acquire() as conn:\n            async with conn.cursor() as cur:\n                # Execute your SELECT query\n                await cur.execute(\"\"\"\n                    SELECT id, query, arxiv_link, relevance_score, final_rank, relevant_answer, paper_stats, paper_metadata_filtered, download_link\n                    FROM Query_Papers\n                    WHERE (relevant_answer IS NULL OR relevant_answer = '')\n                    AND query = %s AND final_rank BETWEEN 1 AND 10\n                    ORDER BY final_rank\n                \"\"\", (query,))\n                query_papers_to_update = await cur.fetchall()\n                print(f\"Total papers to process for '{query}': {len(query_papers_to_update)}\")\n\n                # Map tasks to their papers and prepare for concurrent processing\n                tasks = {asyncio.create_task(query_info_with_gpt(paper[0], paper[2], paper[3], query)): paper for paper in query_papers_to_update}\n\n                # Process tasks as they complete\n                for future in asyncio.as_completed(tasks):\n                    result = await future  # In this context, result is just for logging or additional processing\n                    print(f\"Processing completed with result: {result}\")\n\n                print(\"Finished processing query papers.\")\n\n# Example usage\nuser_query = \"Top academic papers on LLMs\"\nasyncio.run(LLM_process_abstract_loop(user_query))","block_group":"d0471d85397b42aaaa31695aec1ed2f7","execution_count":247,"outputs":[{"name":"stdout","text":"Total papers to process for 'Top academic papers on LLMs': 0\nFinished processing query papers.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/e9124133-1622-4594-ac80-67d80fd8aa33"},{"cell_type":"markdown","metadata":{"id":"2SvLKjS1qy6B","deepnote_app_block_visible":true,"cell_id":"092a8f1da325492e92f0f0b4844fa804","deepnote_cell_type":"markdown"},"source":"________________________________________________________________________________________________\n# RANKING\n________________________________________________________________________________________________","block_group":"79dd5951f9f0498b9bd65779c419df88"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"014107ff70b34a5ca3a8a6ec32140aca","deepnote_cell_type":"text-cell-p"},"source":"ranking by citations, versions","block_group":"2ed9bba3f258479183b69f9876ce2968"},{"cell_type":"code","metadata":{"source_hash":"4e31f440","execution_start":1710484395528,"execution_millis":824,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"500a6e6fccfd4fe9b2e16f493ba0f230","deepnote_cell_type":"code"},"source":"def update_final_ranks(job_id):\n    conn = connection()\n    c = conn.cursor()\n\n    # Fetch the corresponding papers along with their paper_stats where paper_stats is not null\n    c.execute(\"\"\"\n        SELECT id, paper_stats \n        FROM Query_Papers \n        WHERE job_id = %s AND final_rank IS NULL AND paper_stats IS NOT NULL\n    \"\"\", (job_id,))\n    papers = c.fetchall()\n\n    # Initialize lists to store rankings based on citations and versions\n    citation_ranks = []\n    version_ranks = []\n\n    # First loop to collect citation and version counts\n    for paper in papers:\n        id, stats_json = paper\n        if stats_json:\n            # Check if stats_json is not null\n            stats = json.loads(stats_json)\n            citations = stats.get('citations', 0) or 0  # Ensure default is 0 if None\n            versions = stats.get('versions', 0) or 0  # Ensure default is 0 if None\n            citation_ranks.append((id, citations))\n            version_ranks.append((id, versions))\n\n    # Sort and rank based on citations and versions separately\n    citation_ranks.sort(key=lambda x: x[1], reverse=True)\n    version_ranks.sort(key=lambda x: x[1], reverse=True)\n    citation_rank_dict = {paper_id: rank + 1 for rank, (paper_id, _) in enumerate(citation_ranks)}\n    version_rank_dict = {paper_id: rank + 1 for rank, (paper_id, _) in enumerate(version_ranks)}\n\n    # Combine the rankings to calculate the final rank\n    final_ranks = []\n    for id, _ in papers:\n        # Calculate average of the ranks; use large number if paper doesn't have rank in either\n        citation_rank = citation_rank_dict.get(id, len(papers))\n        version_rank = version_rank_dict.get(id, len(papers))\n        avg_rank = (citation_rank + version_rank) / 2.0\n        final_ranks.append((id, avg_rank))\n\n    # Sort papers based on the average rank\n    final_ranks.sort(key=lambda x: x[1])\n\n    # Update the final_rank column based on this ordering\n    for rank, (id, _) in enumerate(final_ranks, start=1):\n        # start=1 for ranking starting from 1\n        c.execute(\"UPDATE Query_Papers SET final_rank = %s WHERE id = %s\", (rank, id))\n\n    # Commit the changes to the database\n    conn.commit()\n\n    # Close the database connection\n    conn.close()\n    print(\"Finished updating final ranks for query papers.\")\n\n# Example execution\njob_id = 3\nupdate_final_ranks(job_id)","block_group":"dfaf2d2c2672422e89ca59d0402dbd7d","execution_count":264,"outputs":[{"name":"stdout","text":"Finished updating final ranks for query papers.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/9a96ff35-b3b6-486e-ab51-ec2394a39055"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"de7e2e1d4eef446bb964b000fdcf8d12","deepnote_cell_type":"text-cell-p"},"source":"reranking with publication date","block_group":"d5d14d1e5ef14844bf60b63f391737bb"},{"cell_type":"code","metadata":{"source_hash":"779a3141","execution_start":1710484694472,"execution_millis":887,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"383553be16464e71b64764923ce4e549","deepnote_cell_type":"code"},"source":"def update_final_ranks_with_date(job_id):\n    conn = connection()\n    c = conn.cursor()\n    try:\n        # For each query, fetch the corresponding papers along with their paper_stats\n        c.execute(\"SELECT id, paper_stats, paper_metadata_filtered FROM Query_Papers WHERE job_id = %s AND final_rank IS NULL\", (job_id,)) #\n        papers = c.fetchall()\n        print(f\"Total papers to process for '{job_id}': {len(papers)}\")\n        final_ranks = []\n\n        # First loop to collect citation and version counts\n        for paper in papers:\n            id, stats_json, metadata_json = paper\n            print(f\"Processing paper {id}\")\n            # Parse paper statistics and metadata\n            if stats_json and metadata_json:\n                print(f\"Stats and metadata found for paper {id}\")\n                stats = json.loads(stats_json)\n                metadata = json.loads(metadata_json)\n\n                # Extract citations, versions, and publication date\n                citations = stats.get('citations', 0) or 0\n                versions = stats.get('versions', 0) or 0\n                published_date_str = metadata.get('published_date')\n                \n                # Calculate days since publication\n                if published_date_str:\n                    published_date = datetime.strptime(published_date_str.split('T')[0], '%Y-%m-%d')\n                    days_since_published = (datetime.now() - published_date).days\n                    days_since_published = max(days_since_published, 1)  # Avoid division by zero\n\n                    # Adjust citations and versions based on days since publication\n                    citations_per_day = citations / days_since_published\n                    versions_per_day = versions / days_since_published\n\n                    final_ranks.append((id, citations_per_day, versions_per_day))\n            else:\n                print(f\"Missing stats or metadata for paper {id}\")\n\n        # Combine the rankings based on adjusted citations and versions\n        # Use geometric mean of citations_per_day and versions_per_day for final ranking score\n        final_ranks = [(paper_id, (citations * versions) ** 0.5) for paper_id, citations, versions in final_ranks]\n        final_ranks.sort(key=lambda x: x[1], reverse=True)  # Sort based on the final ranking score, highest first\n\n        # Update the final_rank column based on this ordering\n        for rank, (id, _) in enumerate(final_ranks, start=1):\n            print(f\"Attempting to update final rank for paper {id} to {rank}\")\n            c.execute(\"UPDATE Query_Papers SET final_rank = %s WHERE id = %s\", (rank, id))\n            print(f\"Updated final rank for paper {id} to {rank}\")\n\n        # Commit the changes to the database\n        conn.commit()\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        conn.rollback()  # Roll back the transaction on error\n    finally:\n        conn.close()\n    print(f\"Finished updating final ranks for '{job_id}' in table query_papers.\")\n\n# Example execution\njob_id = 3\nupdate_final_ranks(job_id)","block_group":"d85f380533824b0bbf3055141dff70b8","execution_count":265,"outputs":[{"name":"stdout","text":"Finished updating final ranks for query papers.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/5bde4c9e-301c-4b01-af8a-2a0bfe2f2f01"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"c2338f96181346d0aafdc3382087435b","deepnote_cell_type":"text-cell-p"},"source":"capturing terminal output in logs","block_group":"976fe428907947e78c0ce1f6ba9e34ae"},{"cell_type":"code","metadata":{"source_hash":"bb4a4a62","execution_start":1710468145714,"execution_millis":143,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"ada7351a1b9c4f3fae9e2e57b89a2e58","deepnote_cell_type":"code"},"source":"from datetime import datetime\n\ndef print_and_update_terminal_output(job_id, new_text):\n    # Print the new text to the terminal\n    print(new_text)\n\n    # Prepare the message with a timestamp\n    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    log_message = f\"[{timestamp}] - {new_text}\\n\"\n\n    # Ensure the 'logs' directory exists\n    logs_dir = os.path.join(os.getcwd(), 'logs')\n    if not os.path.exists(logs_dir):\n        os.makedirs(logs_dir)\n\n    # Define the path for the log file, naming it with the job_id\n    log_file_path = os.path.join(logs_dir, f\"{job_id}.log\")\n\n    # Write the log message to the file\n    with open(log_file_path, 'a') as log_file:\n        log_file.write(log_message)","block_group":"d48b8faca6bf479aa51ff7a0b19e3e9c","execution_count":250,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"a93bb607519d4a89848796a841b0a3f4","deepnote_cell_type":"text-cell-h1"},"source":"# Final loop","block_group":"8863b10f881f49e7960dd0ac560f84c3"},{"cell_type":"code","metadata":{"source_hash":"275e88ae","execution_start":1710468145719,"execution_millis":827609,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"17e553a5bed3468baaa47dcb85841fe5","deepnote_cell_type":"code"},"source":"import asyncio\n\n# Connect to the SQLite database\nconn = connection()\nc = conn.cursor()\n\ntry:\n    while True:  # Infinite loop to keep checking for new jobs\n        # Query to find new jobs with status 'new'\n        c.execute(\"SELECT job_id, query FROM jobs WHERE job_status = 'new'\")\n        new_jobs = c.fetchall()\n        GPT=None\n        Perplexity=None\n        # Check if there are any new jobs\n        if new_jobs:\n            print(\"Found new jobs:\", new_jobs)\n            # Process the new jobs\n            for job in new_jobs:\n                job_id, job_query = job  # Get the job_id and query from the tuple\n\n                # Update the job_status to 'running' for the new job\n                c.execute(\"UPDATE jobs SET job_status = 'running' WHERE job_id = %s\", (job_id,))\n                conn.commit()\n                print(f\"Updated job: {job_id}, query: {job_query} to 'running'\")                \n\n                Perplexity=query_perplexity_response(job_id, user_query)\n                GPT=query_answer_with_gpt(job_id, user_query)\n                get_sub_queries(job_id, user_query)\n                queries_based_on_LLM_responses(job_id, GPT, Perplexity)\n\n                print_and_update_terminal_output(job_id, f\"search_and_fetch_google\")\n                search_and_fetch_google(job_id)\n\n                print_and_update_terminal_output(job_id, f\"get_scholar_citations_versions_loop\")\n                get_scholar_citations_versions_loop(job_id)\n\n                print_and_update_terminal_output(job_id, f\"update_final_ranks\")\n                update_final_ranks(job_id)\n\n                print_and_update_terminal_output(job_id, f\"fetch_arxiv_paper_from_url_loop\")\n                fetch_arxiv_paper_from_url_loop(job_id)\n\n                update_final_ranks_with_date(job_id)\n                print_and_update_terminal_output(job_id, f\"update_final_ranks_with_date\")\n\n                # print_and_update_terminal_output(job_id, f\"LLM_process_abstract_loop\")\n                # try:\n                #     asyncio.run(LLM_process_abstract_loop(job_query))\n                # except RuntimeError:  # asyncio.run() cannot be called from a running event loop\n                #     loop = asyncio.get_event_loop()\n                #     if loop.is_running():\n                #         loop.create_task(LLM_process_abstract_loop(job_query))\n                #     else:\n                #         loop.run_until_complete(LLM_process_abstract_loop(job_query))\n                \n                # Update the job_status to 'done' after processing is complete\n                c.execute(\"UPDATE jobs SET job_status = 'done' WHERE job_id = %s\", (job_id,))\n                conn.commit()\n                print(f\"Updated job {job_id} to 'done'\")\n        \n        # Wait for half a second before checking again\n        time.sleep(0.5)\nexcept KeyboardInterrupt:\n    print(\"Stopped by user\")\nfinally:\n    # Close the database connection when done\n    conn.close()\n","block_group":"7e60dd81daa94869b0f1394485105627","execution_count":251,"outputs":[{"name":"stdout","text":"Stopped by user\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/eed92880-8f43-4359-87a7-2fc5034745cf"},{"cell_type":"code","metadata":{"source_hash":"6f214dba","execution_start":1710468973329,"execution_millis":25,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"9864637443744404a5bc9c6c2256e82c","deepnote_cell_type":"code"},"source":"# def erase_all_data():\n#     # List of all your table names\n#     table_names = ['google_search_results', 'Papers', 'Query_Papers', 'jobs']\n\n#     # Open a new connection\n#     conn = connection()\n#     c = conn.cursor()\n\n#     try:\n\n#         # Truncate each table\n#         for table in table_names:\n#             c.execute(f\"TRUNCATE TABLE {table} RESTART IDENTITY CASCADE;\")  # RESTART IDENTITY resets serial counters, CASCADE deletes data in dependent tables as well\n\n#         # Commit the transaction\n#         conn.commit()\n#         print(\"All data has been erased from all tables.\")\n#     except Exception as e:\n#         # If an error occurs, rollback any changes made during the transaction\n#         conn.rollback()\n#         print(f\"An error occurred: {e}. Transaction rolled back.\")\n#     finally:\n#         # Close the cursor and connection\n#         c.close()\n#         conn.close()\n\n# # Call the function\n# erase_all_data()\n","block_group":"ae2434ba568f41bb9fb86a583e85188a","execution_count":252,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6d52007a-f237-4857-b1f1-3ccb95216ee4' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_full_width":true,"deepnote_app_layout":"powerful-article","deepnote_app_hide_all_code_blocks_enabled":true,"deepnote_app_reactivity_enabled":true,"deepnote_notebook_id":"669fac77c2144914ba44f01ac43e97dd","deepnote_execution_queue":[]}}