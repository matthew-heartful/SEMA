{"cells":[{"cell_type":"markdown","metadata":{"id":"0N0o6eLorq6K","deepnote_app_block_visible":true,"cell_id":"10feb2e288194cd4be7eadcd85a07f4d","deepnote_cell_type":"markdown"},"source":"# SEMA Semantic Agent. Arxiv search powered by LLMs","block_group":"5433b36a96a84de4a5e7f3fd312964bc"},{"cell_type":"markdown","metadata":{"id":"DZtnkrGgcPZd","deepnote_app_block_visible":true,"cell_id":"8fd14e7465dc48159f0d4493170ace6a","deepnote_cell_type":"markdown"},"source":"What it does:\n- Convert user query into keyword search queries\n- Google search top 10 results with SERP API\n- Scrape html for each result, convert to markdown\n- Structure output using function calling -> json to get paper, title\n- Call arxiv to get paper, abstract, metadata\n- Call Google Scholar to get citations, ...\n- Use LLM to answer user query based on the paper, evaluate answer relevance\n- Rank results based on citations, relevance to user query\n- Print results in structured format, give links to download, or to use in notebook LM","block_group":"8ac24ea88e3c4de9bd85ad5825ecaadf"},{"cell_type":"markdown","metadata":{"id":"xLlMdqkpwwFV","deepnote_app_block_visible":true,"cell_id":"9b284d467385405697127d5f6d19d5dc","deepnote_cell_type":"markdown"},"source":"# Setup","block_group":"a4064cec25cc4539b8783c1c50a4ff7b"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"a7aa33f707ad49f290f4ac570e4994a3","deepnote_cell_type":"text-cell-p"},"source":"load secret variables","block_group":"61701f62befd4d9188a9d672a2342f2c"},{"cell_type":"code","metadata":{"id":"udzkaNSJlQtZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8afda221-8b65-4c34-cb39-a98c4c8f5957","source_hash":"879c255","execution_start":1710195695831,"execution_millis":52,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"f544246d8b4b4dad9a505de9fd73b72e","deepnote_cell_type":"code"},"source":"import os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nopenai_api_key = os.environ.get(\"OPENAI_API_KEY\")\nserp_api_key = os.environ.get(\"SERP_API_KEY\")\ngemini_api_key = os.environ.get(\"GEMINI_API_KEY\")\nllamaindex_api_key = os.environ.get(\"LLAMAINDEX_API_KEY\")\n\n# Hide part of the key\nopenai_api_key_hidden = openai_api_key[:3] + \"*\" * (len(openai_api_key) - 6) + openai_api_key[-3:]\nserp_api_key_hidden = serp_api_key[:3] + \"*\" * (len(serp_api_key) - 6) + serp_api_key[-3:]\ngemini_api_key_hidden = gemini_api_key[:3] + \"*\" * (len(gemini_api_key) - 6) + gemini_api_key[-3:]\nllamaindex_api_key_hidden = llamaindex_api_key[:3] + \"*\" * (len(llamaindex_api_key) - 6) + llamaindex_api_key[-3:]\n\n# Print the hidden keys\nprint(f\"OpenAI API Key (hidden): {openai_api_key_hidden}\")\nprint(f\"Serp API Key (hidden): {serp_api_key_hidden}\")\nprint(f\"Gemini API Key (hidden): {gemini_api_key_hidden}\")\nprint(f\"Llamaindex API Key (hidden): {llamaindex_api_key_hidden}\")","block_group":"ec6919e96c944f4ca1306d0196fb4368","execution_count":1,"outputs":[{"name":"stdout","text":"OpenAI API Key (hidden): sk-*********************************************0jF\nSerp API Key (hidden): 68c**********************************************************266\nGemini API Key (hidden): AIz*********************************MUc\nLlamaindex API Key (hidden): llx**********************************************hA3\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/82de34cd-3ba2-436e-841c-63a489a52e77"},{"cell_type":"code","metadata":{"id":"kQlg-GSNcDkc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a710e607-5778-4782-800c-0d18a9c60cde","source_hash":"ae373b63","execution_start":1710195697408,"execution_millis":261,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"a1aca2dfefa54140b8c63124b5f6fd24","deepnote_cell_type":"code"},"source":"import requests\nimport json\n\n# Set up your SERP API key\n# It's better to use an environment variable for API keys\n\ndef search_google(query):\n    params = {\n        \"engine\": \"google\",\n        \"q\": query,\n        \"api_key\": serp_api_key,\n        \"location\": \"San Francisco Bay Area, United States\",\n        \"google_domain\": \"google.com\",\n        \"gl\": \"us\",\n        \"hl\": \"en\",\n        \"num\": \"10\"\n    }\n    response = requests.get(\"https://serpapi.com/search\", params=params)\n    response.raise_for_status()  # Raises an HTTPError if the HTTP request returned an unsuccessful status code\n    results = response.json()\n    # Extracting only the needed information\n    formatted_data = {\n        \"organic_results\": [\n            {\n                \"link\": result[\"link\"],\n                \"title\": result[\"title\"],\n                \"snippet\": result.get(\"snippet\", \"\")\n            } for result in results.get(\"organic_results\", [])\n        ]\n    }\n    # Assuming search_results is your JSON dictionary obtained from the search\n    organic_results = formatted_data.get('organic_results', [])\n\n    # Initialize an empty list to store all the links\n    all_links = []\n\n    # Loop through each result in the organic results\n    for result in organic_results:\n        # Extract the link if it exists and add it to the list\n        if 'link' in result:\n            all_links.append(result['link'])\n    return all_links\n\n# Example usage\nquery = \"Top academic papers on LLMs\"\nsearch_google(query)","block_group":"e455b5ec168e43f2b26a6e7d94fd7da3","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"['https://www.topbots.com/top-llm-research-papers-2023/',\n 'https://levelup.gitconnected.com/best-papers-on-large-language-models-ac01b13b94b3',\n 'https://medium.com/@thedatabeast/top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f',\n 'https://community.openai.com/t/foundational-must-read-gpt-llm-papers/197003',\n 'https://www.reddit.com/r/MLQuestions/comments/ze9e5x/can_anyone_recommend_an_llm_that_handles_research/',\n 'https://analyticsindiamag.com/13-not-to-miss-research-papers-on-llms/',\n 'https://github.com/Hannibal046/Awesome-LLM',\n 'https://yousefhosni.medium.com/top-important-llm-papers-for-the-week-from-01-01-to-07-01-4e3be08ac69b',\n 'https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023']"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/a6c2f1a0-0976-4090-be2e-5d61b6617e8e"},{"cell_type":"markdown","metadata":{"source_hash":"512e8d6","execution_start":1709515167081,"execution_millis":9,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"8707b09ca5a24cfd814ed8656ea7fee8","deepnote_cell_type":"markdown"},"source":"# Main logic","block_group":"2ee1adc93b4e4ed8884aa912fa071dd0"},{"cell_type":"markdown","metadata":{"id":"PGhfirytcDkd","deepnote_app_block_visible":true,"cell_id":"2e779478d9f547b2a9682a8163537b97","deepnote_cell_type":"markdown"},"source":"### Scrape the content of the page displayed in the search results","block_group":"ee6c7fb7d3ad4320ae7b3997ed1e253d"},{"cell_type":"code","metadata":{"id":"KmYe0sLncDke","colab":{"base_uri":"https://localhost:8080/"},"outputId":"180473ad-84fd-4623-b3bc-500e7b60e2a8","source_hash":"d92a16f6","execution_start":1710195699408,"execution_millis":390,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"e0bb61fb8e5e45169f873d360edf04fd","deepnote_cell_type":"code"},"source":"import requests\nfrom bs4 import BeautifulSoup\n\ndef fetch_url_content(url):\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n    }\n    response = requests.get(url, headers=headers)\n\n    # Initialize the default response structure\n    result = {\n        \"status\": response.status_code,\n        \"soup\": None\n    }\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        # Adjusts encoding to match what the response seems to use\n        response.encoding = response.apparent_encoding\n        \n        # Now using response.text to utilize the corrected encoding rather than response.content\n        result['soup'] = BeautifulSoup(response.text, 'html.parser')\n    else:\n        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n\n    return result\n\n# Test the function with a URL\nurl = 'https://medium.com/@thedatabeast/top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f'\nresponse = fetch_url_content(url)\nprint(\"Status Code:\", response['status'])\nprint(\"Soup:\", response['soup'])","block_group":"a0e9a61cee0f4dbf8c675a5f13096b69","execution_count":3,"outputs":[{"name":"stdout","text":"Status Code: 200\nSoup: <!DOCTYPE html>\n<html lang=\"en\"><head><title data-rh=\"true\">Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications | by The Data Beast | Medium</title><meta charset=\"utf-8\" data-rh=\"true\"/><meta content=\"width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1\" data-rh=\"true\" name=\"viewport\"/><meta content=\"#000000\" data-rh=\"true\" name=\"theme-color\"/><meta content=\"Medium\" data-rh=\"true\" name=\"twitter:app:name:iphone\"/><meta content=\"828256236\" data-rh=\"true\" name=\"twitter:app:id:iphone\"/><meta content=\"Medium\" data-rh=\"true\" property=\"al:ios:app_name\"/><meta content=\"828256236\" data-rh=\"true\" property=\"al:ios:app_store_id\"/><meta content=\"com.medium.reader\" data-rh=\"true\" property=\"al:android:package\"/><meta content=\"542599432471018\" data-rh=\"true\" property=\"fb:app_id\"/><meta content=\"Medium\" data-rh=\"true\" property=\"og:site_name\"/><meta content=\"article\" data-rh=\"true\" property=\"og:type\"/><meta content=\"2023-11-13T09:09:19.152Z\" data-rh=\"true\" property=\"article:published_time\"/><meta content=\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications | by The Data Beast | Medium\" data-rh=\"true\" name=\"title\"/><meta content=\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering…\" data-rh=\"true\" property=\"og:title\"/><meta content=\"medium://p/7abfcb69da7f\" data-rh=\"true\" property=\"al:android:url\"/><meta content=\"medium://p/7abfcb69da7f\" data-rh=\"true\" property=\"al:ios:url\"/><meta content=\"Medium\" data-rh=\"true\" property=\"al:android:app_name\"/><meta content=\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering…. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" data-rh=\"true\" name=\"description\"/><meta content=\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" data-rh=\"true\" property=\"og:description\"/><meta content=\"https://medium.com/@thedatabeast/top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\" data-rh=\"true\" property=\"og:url\"/><meta content=\"https://medium.com/@thedatabeast/top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\" data-rh=\"true\" property=\"al:web:url\"/><meta content=\"https://miro.medium.com/v2/resize:fit:1024/1*DJCRL6_IaSlWcebV_rDzjw.png\" data-rh=\"true\" property=\"og:image\"/><meta content=\"https://medium.com/@thedatabeast\" data-rh=\"true\" property=\"article:author\"/><meta content=\"The Data Beast\" data-rh=\"true\" name=\"author\"/><meta content=\"index,follow,max-image-preview:large\" data-rh=\"true\" name=\"robots\"/><meta content=\"unsafe-url\" data-rh=\"true\" name=\"referrer\"/><meta content=\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering…\" data-rh=\"true\" property=\"twitter:title\"/><meta content=\"@Medium\" data-rh=\"true\" name=\"twitter:site\"/><meta content=\"medium://p/7abfcb69da7f\" data-rh=\"true\" name=\"twitter:app:url:iphone\"/><meta content=\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" data-rh=\"true\" property=\"twitter:description\"/><meta content=\"https://miro.medium.com/v2/resize:fit:1024/1*DJCRL6_IaSlWcebV_rDzjw.png\" data-rh=\"true\" name=\"twitter:image:src\"/><meta content=\"summary_large_image\" data-rh=\"true\" name=\"twitter:card\"/><meta content=\"@the_data_beast\" data-rh=\"true\" name=\"twitter:creator\"/><meta content=\"Reading time\" data-rh=\"true\" name=\"twitter:label1\"/><meta content=\"2 min read\" data-rh=\"true\" name=\"twitter:data1\"/><meta content=\"2\" data-rh=\"true\" name=\"twitter:tile:template:testing\"/><meta content=\"https://miro.medium.com/v2/resize:fit:1024/1*DJCRL6_IaSlWcebV_rDzjw.png\" data-rh=\"true\" name=\"twitter:tile:image\"/><meta content=\"Person\" data-rh=\"true\" name=\"twitter:tile:info1:icon\"/><meta content=\"The Data Beast\" data-rh=\"true\" name=\"twitter:tile:info1:text\"/><meta content=\"Calendar\" data-rh=\"true\" name=\"twitter:tile:info2:icon\"/><meta content=\"Nov 13, 2023\" data-rh=\"true\" name=\"twitter:tile:info2:text\"/><meta content=\"Read on Medium\" data-rh=\"true\" name=\"twitter:cta\"/><link data-rh=\"true\" href=\"https://miro.medium.com/v2/1*m-R_BkNf1Qjr1YbyOIJY2w.png\" rel=\"icon\"/><link data-rh=\"true\" href=\"/osd.xml\" rel=\"search\" title=\"Medium\" type=\"application/opensearchdescription+xml\"/><link data-rh=\"true\" href=\"https://miro.medium.com/v2/resize:fill:152:152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png\" rel=\"apple-touch-icon\" sizes=\"152x152\"/><link data-rh=\"true\" href=\"https://miro.medium.com/v2/resize:fill:120:120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png\" rel=\"apple-touch-icon\" sizes=\"120x120\"/><link data-rh=\"true\" href=\"https://miro.medium.com/v2/resize:fill:76:76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png\" rel=\"apple-touch-icon\" sizes=\"76x76\"/><link data-rh=\"true\" href=\"https://miro.medium.com/v2/resize:fill:60:60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png\" rel=\"apple-touch-icon\" sizes=\"60x60\"/><link color=\"#171717\" data-rh=\"true\" href=\"https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg\" rel=\"mask-icon\"/><link crossorigin=\"\" data-rh=\"true\" href=\"https://glyph.medium.com\" rel=\"preconnect\"/><link as=\"style\" data-rh=\"true\" href=\"https://glyph.medium.com/css/unbound.css\" id=\"glyph_preload_link\" rel=\"preload\" type=\"text/css\"/><link data-rh=\"true\" href=\"https://glyph.medium.com/css/unbound.css\" id=\"glyph_link\" rel=\"stylesheet\" type=\"text/css\"/><link data-rh=\"true\" href=\"https://medium.com/@thedatabeast\" rel=\"author\"/><link data-rh=\"true\" href=\"https://medium.com/@thedatabeast/top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\" rel=\"canonical\"/><link data-rh=\"true\" href=\"android-app://com.medium.reader/https/medium.com/p/7abfcb69da7f\" rel=\"alternate\"/><script data-rh=\"true\" type=\"application/ld+json\">{\"@context\":\"http:\\u002F\\u002Fschema.org\",\"@type\":\"NewsArticle\",\"image\":[\"https:\\u002F\\u002Fmiro.medium.com\\u002Fv2\\u002Fresize:fit:1200\\u002F1*DJCRL6_IaSlWcebV_rDzjw.png\"],\"url\":\"https:\\u002F\\u002Fmedium.com\\u002F@thedatabeast\\u002Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\",\"dateCreated\":\"2023-11-13T09:09:19.152Z\",\"datePublished\":\"2023-11-13T09:09:19.152Z\",\"dateModified\":\"2023-11-14T05:31:34.681Z\",\"headline\":\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications\",\"name\":\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications\",\"description\":\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering…. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\",\"identifier\":\"7abfcb69da7f\",\"author\":{\"@type\":\"Person\",\"name\":\"The Data Beast\",\"url\":\"https:\\u002F\\u002Fmedium.com\\u002F@thedatabeast\"},\"creator\":[\"The Data Beast\"],\"publisher\":{\"@type\":\"Organization\",\"name\":\"Medium\",\"url\":\"https:\\u002F\\u002Fmedium.com\\u002F\",\"logo\":{\"@type\":\"ImageObject\",\"width\":308,\"height\":60,\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fv2\\u002Fresize:fit:616\\u002F1*OMF3fSqH8t4xBJ9-6oZDZw.png\"}},\"mainEntityOfPage\":\"https:\\u002F\\u002Fmedium.com\\u002F@thedatabeast\\u002Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\",\"isAccessibleForFree\":\"False\",\"hasPart\":{\"@type\":\"WebPageElement\",\"isAccessibleForFree\":\"False\",\"cssSelector\":\".meteredContent\"}}</script><style data-fela-rehydration=\"447\" data-fela-type=\"STATIC\" type=\"text/css\">html{box-sizing:border-box;-webkit-text-size-adjust:100%}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}#speechify-root{font-family:Sohne, sans-serif}div[data-popper-reference-hidden=\"true\"]{visibility:hidden;pointer-events:none}\n/*XCode style (c) Angel Garcia <angelgarcia.mail@gmail.com>*/.hljs {background: #fff;color: black;\n}/* Gray DOCTYPE selectors like WebKit */\n.xml .hljs-meta {color: #c0c0c0;\n}.hljs-comment,\n.hljs-quote {color: #007400;\n}.hljs-tag,\n.hljs-attribute,\n.hljs-keyword,\n.hljs-selector-tag,\n.hljs-literal,\n.hljs-name {color: #aa0d91;\n}.hljs-variable,\n.hljs-template-variable {color: #3F6E74;\n}.hljs-code,\n.hljs-string,\n.hljs-meta .hljs-string {color: #c41a16;\n}.hljs-regexp,\n.hljs-link {color: #0E0EFF;\n}.hljs-title,\n.hljs-symbol,\n.hljs-bullet,\n.hljs-number {color: #1c00cf;\n}.hljs-section,\n.hljs-meta {color: #643820;\n}.hljs-title.class_,\n.hljs-class .hljs-title,\n.hljs-type,\n.hljs-built_in,\n.hljs-params {color: #5c2699;\n}.hljs-attr {color: #836C28;\n}.hljs-subst {color: #000;\n}.hljs-formula {background-color: #eee;font-style: italic;\n}.hljs-addition {background-color: #baeeba;\n}.hljs-deletion {background-color: #ffc8bd;\n}.hljs-selector-id,\n.hljs-selector-class {color: #9b703f;\n}.hljs-doctag,\n.hljs-strong {font-weight: bold;\n}.hljs-emphasis {font-style: italic;\n}\n</style><style data-fela-rehydration=\"447\" data-fela-type=\"KEYFRAME\" type=\"text/css\">@-webkit-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" type=\"text/css\">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Oxygen, Ubuntu, Cantarell, \"Open Sans\", \"Helvetica Neue\", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{display:block}.m{position:sticky}.n{top:0}.o{z-index:500}.p{padding:0 24px}.q{align-items:center}.r{border-bottom:solid 1px #F2F2F2}.y{height:41px}.z{line-height:20px}.ab{display:flex}.ac{height:57px}.ae{flex:1 0 auto}.af{color:inherit}.ag{fill:inherit}.ah{font-size:inherit}.ai{border:inherit}.aj{font-family:inherit}.ak{letter-spacing:inherit}.al{font-weight:inherit}.am{padding:0}.an{margin:0}.ao{cursor:pointer}.ap:disabled{cursor:not-allowed}.aq:disabled{color:#6B6B6B}.ar:disabled{fill:#6B6B6B}.au{fill:rgba(0, 0, 0, 1)}.av{height:22px}.aw{margin-left:16px}.ax{border:none}.ay{border-radius:20px}.az{width:240px}.ba{background:#F9F9F9}.bb path{fill:#6B6B6B}.bd{outline:none}.be{font-family:sohne, \"Helvetica Neue\", Helvetica, Arial, sans-serif}.bf{font-size:14px}.bg{width:100%}.bh{padding:10px 20px 10px 0}.bi{background-color:transparent}.bj{color:#242424}.bk::placeholder{color:#6B6B6B}.bl{display:inline-block}.bm{margin-left:12px}.bn{margin-right:12px}.bo{border-radius:4px}.bp{margin-left:24px}.bq{height:24px}.bw{background-color:#F9F9F9}.bx{border-radius:50%}.by{height:32px}.bz{width:32px}.ca{justify-content:center}.cg{max-width:680px}.ch{min-width:0}.ci{animation:k1 1.2s ease-in-out infinite}.cj{height:100vh}.ck{margin-bottom:16px}.cl{margin-top:48px}.cm{align-items:flex-start}.cn{flex-direction:column}.co{justify-content:space-between}.cp{margin-bottom:24px}.cv{width:80%}.cw{background-color:#F2F2F2}.dc{height:44px}.dd{width:44px}.de{margin:auto 0}.df{margin-bottom:4px}.dg{height:16px}.dh{width:120px}.di{width:80px}.do{margin-bottom:8px}.dp{width:96%}.dq{width:98%}.dr{width:81%}.ds{margin-left:8px}.dt{color:#6B6B6B}.du{font-size:13px}.dv{height:100%}.eo{color:#FFFFFF}.ep{fill:#FFFFFF}.eq{background:#1A8917}.er{border-color:#1A8917}.ev:disabled{cursor:inherit !important}.ew:disabled{opacity:0.3}.ex:disabled:hover{background:#1A8917}.ey:disabled:hover{border-color:#1A8917}.ez{border-radius:99em}.fa{border-width:1px}.fb{border-style:solid}.fc{box-sizing:border-box}.fd{text-decoration:none}.fe{text-align:center}.fh{margin-right:32px}.fi{position:relative}.fj{fill:#6B6B6B}.fm{background:transparent}.fn svg{margin-left:4px}.fo svg{fill:#6B6B6B}.fq{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.05)}.fr{position:absolute}.fy{margin:0 24px}.gc{background:rgba(255, 255, 255, 1)}.gd{border:1px solid #F2F2F2}.ge{box-shadow:0 1px 4px #F2F2F2}.gf{max-height:100vh}.gg{overflow-y:auto}.gh{left:0}.gi{top:calc(100vh + 100px)}.gj{bottom:calc(100vh + 100px)}.gk{width:10px}.gl{pointer-events:none}.gr{margin-right:4px}.gs{margin-top:2px}.gt{box-sizing:content-box}.gu{word-break:break-word}.gv{word-wrap:break-word}.gw:after{display:block}.gx:after{content:\"\"}.gy:after{clear:both}.gz{line-height:1.23}.ha{letter-spacing:0}.hb{font-style:normal}.hc{font-weight:700}.hx{margin-top:0px}.hy{@media all and (max-width: 551.98px):8px}.hz{@media all and (min-width: 552px) and (max-width: 727.98px):8px}.ia{@media all and (min-width: 728px) and (max-width: 903.98px):16px}.ib{@media all and (min-width: 904px) and (max-width: 1079.98px):16px}.ic{@media all and (min-width: 1080px):16px}.ii{align-items:baseline}.ij{width:48px}.ik{height:48px}.il{border:2px solid rgba(255, 255, 255, 1)}.im{z-index:0}.in{box-shadow:none}.io{border:1px solid rgba(0, 0, 0, 0.05)}.ip{margin-bottom:2px}.iq{flex-wrap:nowrap}.ir{font-size:16px}.is{line-height:24px}.iu{margin:0 8px}.iv{display:inline}.iw{color:#1A8917}.ix{fill:#1A8917}.ja{flex:0 0 auto}.jd{flex-wrap:wrap}.je{padding-left:8px}.jf{padding-right:8px}.kg> *{flex-shrink:0}.kh{overflow-x:scroll}.ki::-webkit-scrollbar{display:none}.kj{scrollbar-width:none}.kk{-ms-overflow-style:none}.kl{width:74px}.km{flex-direction:row}.kp{-webkit-user-select:none}.kq{border:0}.kr{fill:rgba(117, 117, 117, 1)}.ku{outline:0}.kv{user-select:none}.kw> svg{pointer-events:none}.lf{cursor:progress}.lg{margin-left:4px}.lh{opacity:1}.li{padding:4px 0}.ll{width:16px}.ln{display:inline-flex}.lt{max-width:100%}.lu{padding:8px 2px}.lv svg{color:#6B6B6B}.mm{margin-left:auto}.mn{margin-right:auto}.mo{max-width:1024px}.mp{clear:both}.mr{cursor:zoom-in}.ms{z-index:auto}.mu{height:auto}.mv{line-height:1.58}.mw{letter-spacing:-0.004em}.mx{font-family:source-serif-pro, Georgia, Cambria, \"Times New Roman\", Times, serif}.ns{margin-bottom:-0.46em}.nt{list-style-type:decimal}.nu{margin-left:30px}.nv{padding-left:0px}.nw{list-style-type:disc}.nx{text-decoration:underline}.od{border-top:none}.oj{height:52px}.ok{max-height:52px}.ol{position:static}.om{z-index:1}.oo{max-width:155px}.ou{margin-right:20px}.pa{align-items:flex-end}.pb{width:76px}.pc{height:76px}.pd{border:2px solid #F9F9F9}.pe{height:72px}.pf{width:72px}.pg{padding:8px 16px}.ph{width:auto}.pi{stroke:#F2F2F2}.pj{height:36px}.pk{width:36px}.pl{color:#F2F2F2}.pm{fill:#F2F2F2}.pn{background:#F2F2F2}.po{border-color:#F2F2F2}.pu{font-weight:500}.pv{font-size:24px}.pw{line-height:30px}.px{letter-spacing:-0.016em}.py{margin-top:8px}.pz{margin-top:16px}.qa{height:0px}.qb{border-bottom:solid 1px #E5E5E5}.qc{margin-top:72px}.qd{padding:24px 0}.qe{margin-bottom:0px}.qf{margin-right:16px}.as:hover:not(:disabled){color:rgba(25, 25, 25, 1)}.at:hover:not(:disabled){fill:rgba(25, 25, 25, 1)}.es:hover{background:#156D12}.et:hover{border-color:#156D12}.eu:hover{cursor:pointer}.fk:hover{color:#242424}.fl:hover{fill:#242424}.fp:hover svg{fill:#242424}.fs:hover{background-color:rgba(0, 0, 0, 0.1)}.it:hover{text-decoration:underline}.iy:hover:not(:disabled){color:#156D12}.iz:hover:not(:disabled){fill:#156D12}.kt:hover{fill:rgba(8, 8, 8, 1)}.lj:hover{fill:#000000}.lk:hover p{color:#000000}.lm:hover{color:#000000}.lw:hover svg{color:#000000}.pp:hover{background:#F2F2F2}.pq:hover{border-color:#F2F2F2}.pr:hover{cursor:wait}.ps:hover{color:#F2F2F2}.pt:hover{fill:#F2F2F2}.bc:focus-within path{fill:#242424}.ks:focus{fill:rgba(8, 8, 8, 1)}.lx:focus svg{color:#000000}.mt:focus{transform:scale(1.01)}.kx:active{border-style:none}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"all and (min-width: 1080px)\" type=\"text/css\">.d{display:none}.bv{width:64px}.cf{margin:0 64px}.cu{height:48px}.db{margin-bottom:52px}.dn{margin-bottom:48px}.ee{font-size:14px}.ef{line-height:20px}.el{font-size:13px}.em{padding:5px 12px}.fg{display:flex}.fx{margin-bottom:68px}.gb{max-width:680px}.gq{margin-top:40px}.ht{font-size:42px}.hu{margin-bottom:32px}.hv{line-height:52px}.hw{letter-spacing:-0.011em}.ih{align-items:center}.js{border-top:solid 1px #F2F2F2}.jt{border-bottom:solid 1px #F2F2F2}.ju{margin:32px 0 0}.jv{padding:3px 8px}.ke> *{margin-right:24px}.kf> :last-child{margin-right:0}.le{margin-top:0px}.ls{margin:0}.no{font-size:20px}.np{margin-top:2.14em}.nq{line-height:32px}.nr{letter-spacing:-0.003em}.oc{margin-top:1.14em}.oi{margin-bottom:88px}.ot{display:inline-block}.oz{padding-top:72px}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"all and (max-width: 1079.98px)\" type=\"text/css\">.e{display:none}.ld{margin-top:0px}.os{display:inline-block}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"all and (max-width: 903.98px)\" type=\"text/css\">.f{display:none}.lc{margin-top:0px}.or{display:inline-block}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"all and (max-width: 727.98px)\" type=\"text/css\">.g{display:none}.la{margin-top:0px}.lb{margin-right:0px}.oq{display:inline-block}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"all and (max-width: 551.98px)\" type=\"text/css\">.h{display:none}.s{display:flex}.t{justify-content:space-between}.br{width:24px}.cb{margin:0 24px}.cq{height:40px}.cx{margin-bottom:44px}.dj{margin-bottom:32px}.dw{font-size:13px}.dx{line-height:20px}.eg{padding:0px 8px 1px}.ft{margin-bottom:4px}.gm{margin-top:32px}.hd{font-size:32px}.he{margin-bottom:24px}.hf{line-height:38px}.hg{letter-spacing:-0.014em}.id{align-items:flex-start}.jb{flex-direction:column}.jg{margin:24px -24px 0}.jh{padding:0}.jw> *{margin-right:8px}.jx> :last-child{margin-right:24px}.kn{margin-left:0px}.ky{margin-top:0px}.kz{margin-right:0px}.lo{margin:0}.ly{border:1px solid #F2F2F2}.lz{border-radius:99em}.ma{padding:0px 16px 0px 12px}.mb{height:38px}.mc{align-items:center}.me svg{margin-right:8px}.my{font-size:18px}.mz{margin-top:1.56em}.na{line-height:28px}.nb{letter-spacing:-0.003em}.ny{margin-top:1.34em}.oe{margin-bottom:80px}.op{display:inline-block}.ov{padding-top:48px}.md:hover{border-color:#E5E5E5}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"all and (min-width: 904px) and (max-width: 1079.98px)\" type=\"text/css\">.i{display:none}.bu{width:64px}.ce{margin:0 64px}.ct{height:48px}.da{margin-bottom:52px}.dm{margin-bottom:48px}.ec{font-size:14px}.ed{line-height:20px}.ej{font-size:13px}.ek{padding:5px 12px}.ff{display:flex}.fw{margin-bottom:68px}.ga{max-width:680px}.gp{margin-top:40px}.hp{font-size:42px}.hq{margin-bottom:32px}.hr{line-height:52px}.hs{letter-spacing:-0.011em}.ig{align-items:center}.jo{border-top:solid 1px #F2F2F2}.jp{border-bottom:solid 1px #F2F2F2}.jq{margin:32px 0 0}.jr{padding:3px 8px}.kc> *{margin-right:24px}.kd> :last-child{margin-right:0}.lr{margin:0}.nk{font-size:20px}.nl{margin-top:2.14em}.nm{line-height:32px}.nn{letter-spacing:-0.003em}.ob{margin-top:1.14em}.oh{margin-bottom:88px}.oy{padding-top:72px}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"all and (min-width: 728px) and (max-width: 903.98px)\" type=\"text/css\">.j{display:none}.w{display:flex}.x{justify-content:space-between}.bt{width:64px}.cd{margin:0 48px}.cs{height:48px}.cz{margin-bottom:52px}.dl{margin-bottom:48px}.ea{font-size:13px}.eb{line-height:20px}.ei{padding:0px 8px 1px}.fv{margin-bottom:68px}.fz{max-width:680px}.go{margin-top:40px}.hl{font-size:42px}.hm{margin-bottom:32px}.hn{line-height:52px}.ho{letter-spacing:-0.011em}.if{align-items:center}.jk{border-top:solid 1px #F2F2F2}.jl{border-bottom:solid 1px #F2F2F2}.jm{margin:32px 0 0}.jn{padding:3px 8px}.ka> *{margin-right:24px}.kb> :last-child{margin-right:0}.lq{margin:0}.ng{font-size:20px}.nh{margin-top:2.14em}.ni{line-height:32px}.nj{letter-spacing:-0.003em}.oa{margin-top:1.14em}.og{margin-bottom:88px}.ox{padding-top:72px}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"all and (min-width: 552px) and (max-width: 727.98px)\" type=\"text/css\">.k{display:none}.u{display:flex}.v{justify-content:space-between}.bs{width:24px}.cc{margin:0 24px}.cr{height:40px}.cy{margin-bottom:44px}.dk{margin-bottom:32px}.dy{font-size:13px}.dz{line-height:20px}.eh{padding:0px 8px 1px}.fu{margin-bottom:4px}.gn{margin-top:32px}.hh{font-size:32px}.hi{margin-bottom:24px}.hj{line-height:38px}.hk{letter-spacing:-0.014em}.ie{align-items:flex-start}.jc{flex-direction:column}.ji{margin:24px 0 0}.jj{padding:0}.jy> *{margin-right:8px}.jz> :last-child{margin-right:8px}.ko{margin-left:0px}.lp{margin:0}.mf{border:1px solid #F2F2F2}.mg{border-radius:99em}.mh{padding:0px 16px 0px 12px}.mi{height:38px}.mj{align-items:center}.ml svg{margin-right:8px}.nc{font-size:18px}.nd{margin-top:1.56em}.ne{line-height:28px}.nf{letter-spacing:-0.003em}.nz{margin-top:1.34em}.of{margin-bottom:80px}.ow{padding-top:48px}.mk:hover{border-color:#E5E5E5}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"print\" type=\"text/css\">.on{display:none}</style><style data-fela-rehydration=\"447\" data-fela-type=\"RULE\" media=\"(prefers-reduced-motion: no-preference)\" type=\"text/css\">.mq{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}</style></head><body><div id=\"root\"><div class=\"a b c\"><div class=\"d e f g h i j k\"></div><script>document.domain = document.domain;</script><div class=\"l c\"><div class=\"l m n o c\"><div class=\"p q r s t u v w x i d y z\"><a class=\"dt ag du be ak b am an ao ap aq ar as at s u w i d q dv z\" href=\"https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7abfcb69da7f&amp;%7Efeature=LoOpenInAppButton&amp;%7Echannel=ShowPostUnderUser&amp;source=---two_column_layout_nav----------------------------------\" rel=\"noopener follow\">Open in app<svg class=\"ds\" fill=\"none\" height=\"10\" viewbox=\"0 0 10 10\" width=\"10\"><path d=\"M.98 8.48a.37.37 0 1 0 .54.54l-.54-.54zm7.77-7.23h.38c0-.2-.17-.38-.38-.38v.38zM8.37 6.5a.37.37 0 1 0 .76 0h-.76zM3.5.87a.37.37 0 1 0 0 .76V.88zM1.52 9.03l7.5-7.5-.54-.54-7.5 7.5.54.54zm6.86-7.77V6.5h.74V1.25h-.74zm-4.88.38h5.25V.88H3.5v.74z\" fill=\"currentColor\"></path></svg></a><div class=\"ab q\"><p class=\"be b dw dx dy dz ea eb ec ed ee ef dt\"><span><button class=\"be b dw dx eg dy dz eh ea eb ei ej ed ek el ef em eo ep eq er es et eu ev ew ex ey ez fa fb fc bl fd fe\" data-testid=\"headerSignUpButton\">Sign up</button></span></p><div class=\"aw l\"><p class=\"be b dw dx dy dz ea eb ec ed ee ef dt\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerSignInButton\" href=\"/m/signin?operation=login&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------\" rel=\"noopener follow\">Sign in</a></span></p></div></div></div><div class=\"p q r ab ac\"><div class=\"ab q ae\"><a aria-label=\"Homepage\" class=\"af ag ah ai aj ak al am an ao ap aq ar as at ab\" data-testid=\"headerMediumLogo\" href=\"/?source=---two_column_layout_nav----------------------------------\" rel=\"noopener follow\"><svg class=\"au av\" viewbox=\"0 0 3940 610\"><path d=\"M594.79 308.2c0 163.76-131.85 296.52-294.5 296.52S5.8 472 5.8 308.2 137.65 11.69 300.29 11.69s294.5 132.75 294.5 296.51M917.86 308.2c0 154.16-65.93 279.12-147.25 279.12s-147.25-125-147.25-279.12S689.29 29.08 770.61 29.08s147.25 125 147.25 279.12M1050 308.2c0 138.12-23.19 250.08-51.79 250.08s-51.79-112-51.79-250.08 23.19-250.08 51.8-250.08S1050 170.09 1050 308.2M1862.77 37.4l.82-.18v-6.35h-167.48l-155.51 365.5-155.51-365.5h-180.48v6.35l.81.18c30.57 6.9 46.09 17.19 46.09 54.3v434.45c0 37.11-15.58 47.4-46.15 54.3l-.81.18V587H1327v-6.35l-.81-.18c-30.57-6.9-46.09-17.19-46.09-54.3V116.9L1479.87 587h11.33l205.59-483.21V536.9c-2.62 29.31-18 38.36-45.68 44.61l-.82.19v6.3h213.3v-6.3l-.82-.19c-27.71-6.25-43.46-15.3-46.08-44.61l-.14-445.2h.14c0-37.11 15.52-47.4 46.08-54.3m97.43 287.8c3.49-78.06 31.52-134.4 78.56-135.37 14.51.24 26.68 5 36.14 14.16 20.1 19.51 29.55 60.28 28.09 121.21zm-2.11 22h250v-1.05c-.71-59.69-18-106.12-51.34-138-28.82-27.55-71.49-42.71-116.31-42.71h-1c-23.26 0-51.79 5.64-72.09 15.86-23.11 10.7-43.49 26.7-60.45 47.7-27.3 33.83-43.84 79.55-47.86 130.93-.13 1.54-.24 3.08-.35 4.62s-.18 2.92-.25 4.39a332.64 332.64 0 0 0-.36 21.69C1860.79 507 1923.65 600 2035.3 600c98 0 155.07-71.64 169.3-167.8l-7.19-2.53c-25 51.68-69.9 83-121 79.18-69.76-5.22-123.2-75.95-118.35-161.63m532.69 157.68c-8.2 19.45-25.31 30.15-48.24 30.15s-43.89-15.74-58.78-44.34c-16-30.7-24.42-74.1-24.42-125.51 0-107 33.28-176.21 84.79-176.21 21.57 0 38.55 10.7 46.65 29.37zm165.84 76.28c-30.57-7.23-46.09-18-46.09-57V5.28L2424.77 60v6.7l1.14-.09c25.62-2.07 43 1.47 53.09 10.79 7.9 7.3 11.75 18.5 11.75 34.26v71.14c-18.31-11.69-40.09-17.38-66.52-17.38-53.6 0-102.59 22.57-137.92 63.56-36.83 42.72-56.3 101.1-56.3 168.81C2230 518.72 2289.53 600 2378.13 600c51.83 0 93.53-28.4 112.62-76.3V588h166.65v-6.66zm159.29-505.33c0-37.76-28.47-66.24-66.24-66.24-37.59 0-67 29.1-67 66.24s29.44 66.24 67 66.24c37.77 0 66.24-28.48 66.24-66.24m43.84 505.33c-30.57-7.23-46.09-18-46.09-57h-.13V166.65l-166.66 47.85v6.5l1 .09c36.06 3.21 45.93 15.63 45.93 57.77V588h166.8v-6.66zm427.05 0c-30.57-7.23-46.09-18-46.09-57V166.65L3082 212.92v6.52l.94.1c29.48 3.1 38 16.23 38 58.56v226c-9.83 19.45-28.27 31-50.61 31.78-36.23 0-56.18-24.47-56.18-68.9V166.66l-166.66 47.85V221l1 .09c36.06 3.2 45.94 15.62 45.94 57.77v191.27a214.48 214.48 0 0 0 3.47 39.82l3 13.05c14.11 50.56 51.08 77 109 77 49.06 0 92.06-30.37 111-77.89v66h166.66v-6.66zM3934.2 588v-6.67l-.81-.19c-33.17-7.65-46.09-22.07-46.09-51.43v-243.2c0-75.83-42.59-121.09-113.93-121.09-52 0-95.85 30.05-112.73 76.86-13.41-49.6-52-76.86-109.06-76.86-50.12 0-89.4 26.45-106.25 71.13v-69.87l-166.66 45.89v6.54l1 .09c35.63 3.16 45.93 15.94 45.93 57V588h155.5v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66V255.72c7-16.35 21.11-35.72 49-35.72 34.64 0 52.2 24 52.2 71.28V588h155.54v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66v-248a160.45 160.45 0 0 0-2.2-27.68c7.42-17.77 22.34-38.8 51.37-38.8 35.13 0 52.2 23.31 52.2 71.28V588z\"></path></svg></a><div class=\"aw h\"><div class=\"ab ax ay az ba q bb bc\"><div aria-describedby=\"searchResults\" aria-hidden=\"false\" aria-labelledby=\"searchResults\" class=\"bl\"></div><div class=\"bm bn ab\"><svg fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z\" fill=\"currentColor\" fill-rule=\"evenodd\"></path></svg></div><input aria-controls=\"searchResults\" aria-expanded=\"false\" aria-label=\"search\" class=\"ax bd be bf z bg bh bi bj bk\" data-testid=\"headerSearchInput\" placeholder=\"Search\" role=\"combobox\" tabindex=\"0\" value=\"\"/></div></div></div><div class=\"h k w ff fg\"><div class=\"fh ab\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerWriteButton\" href=\"/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fnew-story&amp;source=---two_column_layout_nav-----------------------new_post_topnav-----------\" rel=\"noopener follow\"><div class=\"be b bf z dt fi fj ab q fk fl\"><svg aria-label=\"Write\" fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M14 4a.5.5 0 0 0 0-1v1zm7 6a.5.5 0 0 0-1 0h1zm-7-7H4v1h10V3zM3 4v16h1V4H3zm1 17h16v-1H4v1zm17-1V10h-1v10h1zm-1 1a1 1 0 0 0 1-1h-1v1zM3 20a1 1 0 0 0 1 1v-1H3zM4 3a1 1 0 0 0-1 1h1V3z\" fill=\"currentColor\"></path><path d=\"M17.5 4.5l-8.46 8.46a.25.25 0 0 0-.06.1l-.82 2.47c-.07.2.12.38.31.31l2.47-.82a.25.25 0 0 0 .1-.06L19.5 6.5m-2-2l2.32-2.32c.1-.1.26-.1.36 0l1.64 1.64c.1.1.1.26 0 .36L19.5 6.5m-2-2l2 2\" stroke=\"currentColor\"></path></svg><div class=\"ds l\">Write</div></div></a></span></div></div><div class=\"k j i d\"><div class=\"fh ab\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerSearchButton\" href=\"/search?source=---two_column_layout_nav----------------------------------\" rel=\"noopener follow\"><div class=\"be b bf z dt fi fj ab q fk fl\"><svg aria-label=\"Search\" fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z\" fill=\"currentColor\" fill-rule=\"evenodd\"></path></svg></div></a></div></div><div class=\"fh h k j\"><div class=\"ab q\"><p class=\"be b dw dx dy dz ea eb ec ed ee ef dt\"><span><button class=\"be b dw dx eg dy dz eh ea eb ei ej ed ek el ef em eo ep eq er es et eu ev ew ex ey ez fa fb fc bl fd fe\" data-testid=\"headerSignUpButton\">Sign up</button></span></p><div class=\"aw l\"><p class=\"be b dw dx dy dz ea eb ec ed ee ef dt\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerSignInButton\" href=\"/m/signin?operation=login&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------\" rel=\"noopener follow\">Sign in</a></span></p></div></div></div><div aria-hidden=\"false\" class=\"l\"><button aria-label=\"user options menu\" class=\"ax fm am ab q ao fn fo fp\" data-testid=\"headerUserIcon\"><div class=\"l fi\"><img alt=\"\" class=\"l fc bx by bz cw\" height=\"32\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png\" width=\"32\"/><div class=\"fq bx l by bz fr n ax fs\"></div></div></button></div></div></div><div class=\"l\"><div class=\"ft fu fv fw fx l\"><div class=\"ab ca\"><div class=\"ch bg fy fz ga gb\"></div></div><article class=\"meteredContent\"><div class=\"l\"><div class=\"l\"><span class=\"l\"></span><section><div><div class=\"fr gh gi gj gk gl\"></div><div><div class=\"speechify-ignore l\"><div class=\"gm gn go gp gq l\"></div><div class=\"ab ca\"><div class=\"ch bg fy fz ga gb\"><div class=\"ck l\"><div class=\"gt ab\"><div aria-hidden=\"false\" class=\"bl\"><button aria-label=\"Member-only story\" class=\"l ax ao am\"><div class=\"h k j i d\"><div><div aria-hidden=\"false\" class=\"bl\"><svg fill=\"none\" height=\"16\" viewbox=\"0 0 64 64\" width=\"16\"><path d=\"M39.64 40.83L33.87 56.7a1.99 1.99 0 0 1-3.74 0l-5.77-15.87a2.02 2.02 0 0 0-1.2-1.2L7.3 33.88a1.99 1.99 0 0 1 0-3.74l15.87-5.77a2.02 2.02 0 0 0 1.2-1.2L30.12 7.3a1.99 1.99 0 0 1 3.74 0l5.77 15.87a2.02 2.02 0 0 0 1.2 1.2l15.86 5.76a1.99 1.99 0 0 1 0 3.74l-15.87 5.77a2.02 2.02 0 0 0-1.2 1.2z\" fill=\"#FFC017\"></path></svg></div></div></div><div class=\"s u w ff fg q\"><svg class=\"gr gs\" fill=\"none\" height=\"16\" viewbox=\"0 0 64 64\" width=\"16\"><path d=\"M39.64 40.83L33.87 56.7a1.99 1.99 0 0 1-3.74 0l-5.77-15.87a2.02 2.02 0 0 0-1.2-1.2L7.3 33.88a1.99 1.99 0 0 1 0-3.74l15.87-5.77a2.02 2.02 0 0 0 1.2-1.2L30.12 7.3a1.99 1.99 0 0 1 3.74 0l5.77 15.87a2.02 2.02 0 0 0 1.2 1.2l15.86 5.76a1.99 1.99 0 0 1 0 3.74l-15.87 5.77a2.02 2.02 0 0 0-1.2 1.2z\" fill=\"#FFC017\"></path></svg><p class=\"be b bf z dt\">Member-only story</p></div></button></div></div></div></div></div></div></div><div class=\"gu gv gw gx gy\"><div class=\"ab ca\"><div class=\"ch bg fy fz ga gb\"><div><h1 class=\"pw-post-title gz ha hb be hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx bj\" data-testid=\"storyTitle\" id=\"1be4\">Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications</h1><div class=\"hy hz ia ib ic\"><div class=\"speechify-ignore ab co\"><div class=\"speechify-ignore bg l\"><div class=\"id ie if ig ih ab\"><div><div class=\"ab ii\"><a href=\"/@thedatabeast?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><div><div aria-hidden=\"false\" class=\"bl\"><div class=\"l ij ik bx il im\"><div class=\"l fi\"><img alt=\"The Data Beast\" class=\"l fc bx dc dd cw\" data-testid=\"authorPhoto\" height=\"44\" loading=\"lazy\" src=\"https://miro.medium.com/v2/resize:fill:88:88/1*A7eSHxmpHV-LOcc4-DxVnw.png\" width=\"44\"/><div class=\"in bx l dc dd fr n io fs\"></div></div></div></div></div></a></div></div><div class=\"bm bg l\"><div class=\"ab\"><div style=\"flex:1\"><span class=\"be b bf z bj\"><div class=\"ip ab q\"><div class=\"ab q iq\"><div class=\"ab q\"><div><div aria-hidden=\"false\" class=\"bl\"><p class=\"be b ir is bj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar it\" data-testid=\"authorName\" href=\"/@thedatabeast?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\">The Data Beast</a></p></div></div></div><span aria-hidden=\"true\" class=\"iu iv\"><span class=\"be b bf z dt\">·</span></span><p class=\"be b ir is dt\"><span><a class=\"iw ix ah ai aj ak al am an ao ap aq ar ew iy iz\" href=\"/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6bbc6865a96&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;user=The+Data+Beast&amp;userId=f6bbc6865a96&amp;source=post_page-f6bbc6865a96----7abfcb69da7f---------------------post_header-----------\" rel=\"noopener follow\">Follow</a></span></p></div></div></span></div></div><div class=\"l ja\"><span class=\"be b bf z dt\"><div class=\"ab cm jb jc jd\"><span class=\"be b bf z dt\"><div class=\"ab ae\"><span data-testid=\"storyReadTime\">2 min read</span><div aria-hidden=\"true\" class=\"je jf l\"><span aria-hidden=\"true\" class=\"l\"><span class=\"be b bf z dt\">·</span></span></div><span data-testid=\"storyPublishDate\">Nov 13, 2023</span></div></span></div></span></div></div></div><div class=\"ab co jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv\"><div class=\"h k w ff fg q\"><div class=\"kl l\"><div class=\"ab q km\"><div class=\"pw-multi-vote-icon fi gr kn ko kp\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerClapButton\" href=\"/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F7abfcb69da7f&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;user=The+Data+Beast&amp;userId=f6bbc6865a96&amp;source=-----7abfcb69da7f---------------------clap_footer-----------\" rel=\"noopener follow\"><div><div aria-hidden=\"false\" class=\"bl\"><div class=\"kq ao kr ks kt ku am kv kw kx kp\"><svg aria-label=\"clap\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\" fill-rule=\"evenodd\"></path></svg></div></div></div></a></span></div><div class=\"pw-multi-vote-count l ky kz la lb lc ld le\"><p class=\"be b du z dt\"><span class=\"lf\">--</span></p></div></div></div><div><div aria-hidden=\"false\" class=\"bl\"><button aria-label=\"responses\" class=\"ao kq lh li ab q fj lj lk\"><svg class=\"hx\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg><p class=\"be b du z dt\"><span class=\"pw-responses-count lg hx\">1</span></p></button></div></div></div><div class=\"ab q jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk\"><div class=\"ll k j i d\"></div><div class=\"h k\"><div><div aria-hidden=\"false\" class=\"bl\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"headerBookmarkButton\" href=\"/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7abfcb69da7f&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;source=-----7abfcb69da7f---------------------bookmark_footer-----------\" rel=\"noopener follow\"><svg aria-label=\"Add to list bookmark button\" class=\"dt lm\" fill=\"none\" height=\"25\" viewbox=\"0 0 25 25\" width=\"25\"><path d=\"M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z\" fill=\"currentColor\"></path></svg></a></span></div></div></div><div class=\"fc ln cm\"><div class=\"l ae\"><div class=\"ab ca\"><div class=\"lo lp lq lr ls lt ch bg\"><div class=\"ab\"></div></div></div></div></div><div aria-describedby=\"postFooterSocialMenu\" aria-hidden=\"false\" aria-labelledby=\"postFooterSocialMenu\" class=\"bl\"><div><div aria-hidden=\"false\" class=\"bl\"><button aria-controls=\"postFooterSocialMenu\" aria-expanded=\"false\" aria-label=\"Share Post\" class=\"af fj ah ai aj ak al lu an ao ap ew lv lw lk lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml\" data-testid=\"headerSocialShareButton\"><svg fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z\" fill=\"currentColor\" fill-rule=\"evenodd\"></path></svg><div class=\"j i d\"><p class=\"be b bf z dt\">Share</p></div></button></div></div></div></div></div></div></div></div></div><figure class=\"gm gn go gp gq mp mm mn paragraph-image\"><div class=\"mq mr fi ms bg mt\" role=\"button\" tabindex=\"0\"><div class=\"mm mn mo\"><picture><source sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*DJCRL6_IaSlWcebV_rDzjw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*DJCRL6_IaSlWcebV_rDzjw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*DJCRL6_IaSlWcebV_rDzjw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*DJCRL6_IaSlWcebV_rDzjw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*DJCRL6_IaSlWcebV_rDzjw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*DJCRL6_IaSlWcebV_rDzjw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DJCRL6_IaSlWcebV_rDzjw.png 1400w\" type=\"image/webp\"/><source data-testid=\"og\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*DJCRL6_IaSlWcebV_rDzjw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*DJCRL6_IaSlWcebV_rDzjw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*DJCRL6_IaSlWcebV_rDzjw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*DJCRL6_IaSlWcebV_rDzjw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*DJCRL6_IaSlWcebV_rDzjw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*DJCRL6_IaSlWcebV_rDzjw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*DJCRL6_IaSlWcebV_rDzjw.png 1400w\"/><img alt=\"\" class=\"bg lt mu c\" height=\"700\" loading=\"eager\" role=\"presentation\" width=\"700\"/></picture></div></div></figure><ol class=\"\"><li class=\"mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv bj\" id=\"62b7\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</li></ol><ul class=\"\"><li class=\"mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nw nu nv bj\" id=\"b27d\">Link: <a class=\"af nx\" href=\"https://arxiv.org/abs/1810.04805\" rel=\"noopener ugc nofollow\" target=\"_blank\">BERT Paper</a></li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"6118\">Details: Released by Google AI Language team, BERT introduced a deep bidirectional architecture, which enhanced transfer learning demonstrated by unsupervised pre-training.</li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"d23e\">Practical Implementation: Used for state-of-the-art (SOTA) models in language inference and simple question-answer tasks​​.</li></ul><p class=\"pw-post-body-paragraph mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns gu bj\" id=\"2349\">2. BlenderBot 3: A deployed conversational agent that continually learns to responsibly engage</p><ul class=\"\"><li class=\"mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nw nu nv bj\" id=\"685c\">Link: <a class=\"af nx\" href=\"https://arxiv.org/abs/2208.03188\" rel=\"noopener ugc nofollow\" target=\"_blank\">BlenderBot 3 Paper</a></li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"d33f\">Details: From Meta AI, BlenderBot 3, with its 175 billion parameters, can scour the internet, setting it apart from other conversational bots.</li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"7127\">Practical Implementation: Continually learns from deployment data, enhancing engagement and response quality​​.</li></ul><p class=\"pw-post-body-paragraph mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns gu bj\" id=\"3dc8\">3. Improving alignment of dialogue agents via targeted human judgements</p><ul class=\"\"><li class=\"mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nw nu nv bj\" id=\"0d47\">Link: <a class=\"af nx\" href=\"https://arxiv.org/abs/2209.14375\" rel=\"noopener ugc nofollow\" target=\"_blank\">Sparrow Paper</a></li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"3ef2\">Details: DeepMind’s Sparrow employs the RLHF method for training, focusing on using human feedback for generative models.</li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"3e22\">Practical Implementation: Helps in building complex goals in chatbots by integrating human feedback effectively​​.</li></ul><p class=\"pw-post-body-paragraph mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns gu bj\" id=\"2012\">4. Improving Language Understanding by Generative Pre-Training</p><ul class=\"\"><li class=\"mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nw nu nv bj\" id=\"51bc\">Link: <a class=\"af nx\" href=\"https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035\" rel=\"noopener ugc nofollow\" target=\"_blank\">GPT Paper</a></li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"1e7c\">Details: OpenAI’s paper on GPT, demonstrating the use of a decoder-style LLM for generative modeling.</li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"979b\">Practical Implementation: Pioneered NLP tasks by generative pre-training of a language model​​.</li></ul><p class=\"pw-post-body-paragraph mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns gu bj\" id=\"5157\">5. Scaling Laws for Neural Language Models</p><ul class=\"\"><li class=\"mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nw nu nv bj\" id=\"0dfe\">Link: <a class=\"af nx\" href=\"https://arxiv.org/abs/2001.08361\" rel=\"noopener ugc nofollow\" target=\"_blank\">Scaling Laws Paper</a></li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"52b8\">Details: OpenAI’s theoretical investigation into the relationship between the size of a language model and its performance.</li><li class=\"mv mw hb mx b my ny na nb nc nz ne nf ng oa ni nj nk ob nm nn no oc nq nr ns nw nu nv bj\" id=\"292b\">Practical Implementation: Provides empirical evidence for the scaling laws that govern model performance​​.</li></ul><p class=\"pw-post-body-paragraph mv mw hb mx b my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns gu bj\" id=\"2f6f\">6. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation…</p></div></div></div></div></section></div></div></article><div class=\"ab ca\"><div class=\"ch bg fy fz ga gb\"></div></div></div><div class=\"l\"></div><footer class=\"od oe of og oh oi oj ok gt ab q ol om c\"><div class=\"l ae\"><div class=\"ab ca\"><div class=\"ch bg fy fz ga gb\"><div class=\"ab co on\"><div class=\"ab q km\"><div class=\"oo l\"><span class=\"l op oq or e d\"><div class=\"ab q km\"><div class=\"pw-multi-vote-icon fi gr kn ko kp\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"footerClapButton\" href=\"/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F7abfcb69da7f&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;user=The+Data+Beast&amp;userId=f6bbc6865a96&amp;source=-----7abfcb69da7f---------------------clap_footer-----------\" rel=\"noopener follow\"><div><div aria-hidden=\"false\" class=\"bl\"><div class=\"kq ao kr ks kt ku am kv kw kx kp\"><svg aria-label=\"clap\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\" fill-rule=\"evenodd\"></path></svg></div></div></div></a></span></div><div class=\"pw-multi-vote-count l ky kz la lb lc ld le\"><p class=\"be b du z dt\"><span class=\"lf\">--</span></p></div></div></span><span class=\"l h g f os ot\"><div class=\"ab q km\"><div class=\"pw-multi-vote-icon fi gr kn ko kp\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"footerClapButton\" href=\"/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F7abfcb69da7f&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;user=The+Data+Beast&amp;userId=f6bbc6865a96&amp;source=-----7abfcb69da7f---------------------clap_footer-----------\" rel=\"noopener follow\"><div><div aria-hidden=\"false\" class=\"bl\"><div class=\"kq ao kr ks kt ku am kv kw kx kp\"><svg aria-label=\"clap\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z\" fill-rule=\"evenodd\"></path></svg></div></div></div></a></span></div><div class=\"pw-multi-vote-count l ky kz la lb lc ld le\"><p class=\"be b du z dt\"><span class=\"lf\">--</span></p></div></div></span></div><div class=\"bp ab\"><div><div aria-hidden=\"false\" class=\"bl\"><button aria-label=\"responses\" class=\"ao kq lh li ab q fj lj lk\"><svg class=\"hx\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path d=\"M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z\"></path></svg><p class=\"be b bf z dt\"><span class=\"pw-responses-count lg hx\">1</span></p></button></div></div></div></div><div class=\"ab q\"><div class=\"ou l ja\"><div><div aria-hidden=\"false\" class=\"bl\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" data-testid=\"footerBookmarkButton\" href=\"/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7abfcb69da7f&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;source=--------------------------bookmark_footer-----------\" rel=\"noopener follow\"><svg aria-label=\"Add to list bookmark button\" class=\"dt lm\" fill=\"none\" height=\"25\" viewbox=\"0 0 25 25\" width=\"25\"><path d=\"M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z\" fill=\"currentColor\"></path></svg></a></span></div></div></div><div class=\"ou l ja\"><div aria-describedby=\"postFooterSocialMenu\" aria-hidden=\"false\" aria-labelledby=\"postFooterSocialMenu\" class=\"bl\"><div><div aria-hidden=\"false\" class=\"bl\"><button aria-controls=\"postFooterSocialMenu\" aria-expanded=\"false\" aria-label=\"Share Post\" class=\"af fj ah ai aj ak al lu an ao ap ew lv lw lk lx\" data-testid=\"footerSocialShareButton\"><svg fill=\"none\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\"><path clip-rule=\"evenodd\" d=\"M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z\" fill=\"currentColor\" fill-rule=\"evenodd\"></path></svg></button></div></div></div></div></div></div></div></div></div></footer><div class=\"ov ow ox oy oz l bw\"><div class=\"ab ca\"><div class=\"ch bg fy fz ga gb\"><div class=\"ck ab pa co\"><div class=\"ab ii\"><a href=\"/@thedatabeast?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><div class=\"l pb pc bx pd im\"><div class=\"l fi\"><img alt=\"The Data Beast\" class=\"l fc bx pe pf cw\" height=\"72\" loading=\"lazy\" src=\"https://miro.medium.com/v2/resize:fill:144:144/1*A7eSHxmpHV-LOcc4-DxVnw.png\" width=\"72\"/><div class=\"in bx l pe pf fr n io fs\"></div></div></div></a></div><div class=\"j i d\"><div class=\"ab\"><span><button class=\"be b bf z eo pg ep eq er es et eu ev ew ex ey ez ph fa fb fc bl fd fe\">Follow</button></span><div class=\"ds l\"><div><div><div aria-hidden=\"false\" class=\"bl\"><div class=\"l\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F920e325a463a&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;newsletterV3=f6bbc6865a96&amp;newsletterV3Id=920e325a463a&amp;user=The+Data+Beast&amp;userId=f6bbc6865a96&amp;source=-----7abfcb69da7f---------------------subscribe_user-----------\" rel=\"noopener follow\"><button aria-label=\"Subscribe\" class=\"be b bf z pl am pm pn po pp pq pr ps pt ev ew ex ey ez fa fb fc bl fd fe\"><svg class=\"pi pj pk\" fill=\"none\" height=\"38\" viewbox=\"0 0 38 38\" width=\"38\"><rect height=\"6.5\" rx=\"0.25\" width=\"0.5\" x=\"26.25\" y=\"9.25\"></rect><rect height=\"6.5\" rx=\"0.25\" transform=\"rotate(90 29.75 12.25)\" width=\"0.5\" x=\"29.75\" y=\"12.25\"></rect><path d=\"M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5\"></path><path d=\"M11.5 14.5L19 20l4-3\"></path></svg></button></a></span></div></div></div></div></div></div></div></div><div class=\"ab cm co\"><div class=\"l\"><div class=\"ab q\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at ab q\" href=\"/@thedatabeast?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><h2 class=\"pw-author-name be pu pv pw px bj\"><span class=\"gu\">Written by <!-- -->The Data Beast</span></h2></a></div><div class=\"py ab\"><div class=\"l ja\"><span class=\"pw-follower-count be b bf z bj\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar it\" href=\"/@thedatabeast/followers?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\">326 Followers</a></span></div></div><div class=\"pz l\"></div></div><div class=\"h k\"><div class=\"ab\"><span><button class=\"be b bf z eo pg ep eq er es et eu ev ew ex ey ez ph fa fb fc bl fd fe\">Follow</button></span><div class=\"ds l\"><div><div><div aria-hidden=\"false\" class=\"bl\"><div class=\"l\"><span><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F920e325a463a&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f&amp;newsletterV3=f6bbc6865a96&amp;newsletterV3Id=920e325a463a&amp;user=The+Data+Beast&amp;userId=f6bbc6865a96&amp;source=-----7abfcb69da7f---------------------subscribe_user-----------\" rel=\"noopener follow\"><button aria-label=\"Subscribe\" class=\"be b bf z pl am pm pn po pp pq pr ps pt ev ew ex ey ez fa fb fc bl fd fe\"><svg class=\"pi pj pk\" fill=\"none\" height=\"38\" viewbox=\"0 0 38 38\" width=\"38\"><rect height=\"6.5\" rx=\"0.25\" width=\"0.5\" x=\"26.25\" y=\"9.25\"></rect><rect height=\"6.5\" rx=\"0.25\" transform=\"rotate(90 29.75 12.25)\" width=\"0.5\" x=\"29.75\" y=\"12.25\"></rect><path d=\"M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5\"></path><path d=\"M11.5 14.5L19 20l4-3\"></path></svg></button></a></span></div></div></div></div></div></div></div></div><div class=\"qa bg qb gm gn go gp gq\"></div></div></div><div class=\"h k j\"><div class=\"qa bg qb qc\"></div><div class=\"ab ca\"><div class=\"ch bg fy fz ga gb\"><div class=\"qd ab km jd\"><div class=\"qe qf l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://help.medium.com/hc/en-us?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Help</p></a></div><div class=\"qe qf l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://medium.statuspage.io/?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Status</p></a></div><div class=\"qe qf l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"/about?autoplay=1&amp;source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">About</p></a></div><div class=\"qe qf l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Careers</p></a></div><div class=\"qe qf l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://blog.medium.com/?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Blog</p></a></div><div class=\"qe qf l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Privacy</p></a></div><div class=\"qe qf l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Terms</p></a></div><div class=\"qe qf l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"https://speechify.com/medium?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Text to speech</p></a></div><div class=\"qe l\"><a class=\"af ag ah ai aj ak al am an ao ap aq ar as at\" href=\"/business?source=post_page-----7abfcb69da7f--------------------------------\" rel=\"noopener follow\"><p class=\"be b du z dt\">Teams</p></a></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__=\"main-20240308-234842-a5200d2619\"</script><script>window.__GRAPHQL_URI__ = \"https://medium.com/_/graphql\"</script><script>window.__PRELOADED_STATE__ = {\"algolia\":{\"queries\":{}},\"cache\":{\"experimentGroupSet\":true,\"reason\":\"\",\"group\":\"enabled\",\"tags\":[\"group-edgeCachePosts\",\"post-7abfcb69da7f\",\"user-f6bbc6865a96\"],\"serverVariantState\":\"44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\",\"middlewareEnabled\":true,\"cacheStatus\":\"DYNAMIC\",\"shouldUseCache\":true,\"vary\":[],\"loHomepageEnabled\":false,\"updatedPostPreviewsEnabled\":false},\"client\":{\"hydrated\":false,\"isUs\":false,\"isNativeMedium\":false,\"isSafariMobile\":false,\"isSafari\":false,\"isFirefox\":false,\"routingEntity\":{\"type\":\"DEFAULT\",\"explicit\":false},\"viewerIsBot\":false},\"debug\":{\"requestId\":\"89b536c6-af57-4f8c-b658-7140d5f53ffe\",\"hybridDevServices\":[],\"originalSpanCarrier\":{\"ot-tracer-spanid\":\"68e29e9d30174b72\",\"ot-tracer-traceid\":\"135ac5b6a3dc0aa5\",\"ot-tracer-sampled\":\"true\"}},\"multiVote\":{\"clapsPerPost\":{}},\"navigation\":{\"branch\":{\"show\":null,\"hasRendered\":null,\"blockedByCTA\":false},\"hideGoogleOneTap\":false,\"hasRenderedAlternateUserBanner\":null,\"currentLocation\":\"https:\\u002F\\u002Fmedium.com\\u002F@thedatabeast\\u002Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\",\"host\":\"medium.com\",\"hostname\":\"medium.com\",\"referrer\":\"\",\"hasSetReferrer\":false,\"susiModal\":{\"step\":null,\"operation\":\"register\"},\"postRead\":false,\"queryString\":\"\",\"currentHash\":\"\"},\"config\":{\"nodeEnv\":\"production\",\"version\":\"main-20240308-234842-a5200d2619\",\"target\":\"production\",\"productName\":\"Medium\",\"publicUrl\":\"https:\\u002F\\u002Fcdn-client.medium.com\\u002Flite\",\"authDomain\":\"medium.com\",\"authGoogleClientId\":\"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com\",\"favicon\":\"production\",\"glyphUrl\":\"https:\\u002F\\u002Fglyph.medium.com\",\"branchKey\":\"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm\",\"algolia\":{\"appId\":\"MQ57UUUQZ2\",\"apiKeySearch\":\"394474ced050e3911ae2249ecc774921\",\"indexPrefix\":\"medium_\",\"host\":\"-dsn.algolia.net\"},\"recaptchaKey\":\"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk\",\"recaptcha3Key\":\"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5\",\"recaptchaEnterpriseKeyId\":\"6Le-uGgpAAAAAPprRaokM8AKthQ9KNGdoxaGUvVp\",\"datadog\":{\"applicationId\":\"6702d87d-a7e0-42fe-bbcb-95b469547ea0\",\"clientToken\":\"pub853ea8d17ad6821d9f8f11861d23dfed\",\"rumToken\":\"pubf9cc52896502b9413b68ba36fc0c7162\",\"context\":{\"deployment\":{\"target\":\"production\",\"tag\":\"main-20240308-234842-a5200d2619\",\"commit\":\"a5200d2619b9422b2f8b826496ae3c4debb09146\"}},\"datacenter\":\"us\"},\"googleAnalyticsCode\":\"G-7JY7T788PK\",\"googlePay\":{\"apiVersion\":\"2\",\"apiVersionMinor\":\"0\",\"merchantId\":\"BCR2DN6TV7EMTGBM\",\"merchantName\":\"Medium\",\"instanceMerchantId\":\"13685562959212738550\"},\"applePay\":{\"version\":3},\"signInWallCustomDomainCollectionIds\":[\"3a8144eabfe3\",\"336d898217ee\",\"61061eb0c96b\",\"138adf9c44c\",\"819cc2aaeee0\"],\"mediumMastodonDomainName\":\"me.dm\",\"mediumOwnedAndOperatedCollectionIds\":[\"8a9336e5bb4\",\"b7e45b22fec3\",\"193b68bd4fba\",\"8d6b8a439e32\",\"54c98c43354d\",\"3f6ecf56618\",\"d944778ce714\",\"92d2092dc598\",\"ae2a65f35510\",\"1285ba81cada\",\"544c7006046e\",\"fc8964313712\",\"40187e704f1c\",\"88d9857e584e\",\"7b6769f2748b\",\"bcc38c8f6edf\",\"cef6983b292\",\"cb8577c9149e\",\"444d13b52878\",\"713d7dbc99b0\",\"ef8e90590e66\",\"191186aaafa0\",\"55760f21cdc5\",\"9dc80918cc93\",\"bdc4052bbdba\",\"8ccfed20cbb2\"],\"tierOneDomains\":[\"medium.com\",\"thebolditalic.com\",\"arcdigital.media\",\"towardsdatascience.com\",\"uxdesign.cc\",\"codeburst.io\",\"psiloveyou.xyz\",\"writingcooperative.com\",\"entrepreneurshandbook.co\",\"prototypr.io\",\"betterhumans.coach.me\",\"theascent.pub\"],\"topicsToFollow\":[\"d61cf867d93f\",\"8a146bc21b28\",\"1eca0103fff3\",\"4d562ee63426\",\"aef1078a3ef5\",\"e15e46793f8d\",\"6158eb913466\",\"55f1c20aba7a\",\"3d18b94f6858\",\"4861fee224fd\",\"63c6f1f93ee\",\"1d98b3a9a871\",\"decb52b64abf\",\"ae5d4995e225\",\"830cded25262\"],\"topicToTagMappings\":{\"accessibility\":\"accessibility\",\"addiction\":\"addiction\",\"android-development\":\"android-development\",\"art\":\"art\",\"artificial-intelligence\":\"artificial-intelligence\",\"astrology\":\"astrology\",\"basic-income\":\"basic-income\",\"beauty\":\"beauty\",\"biotech\":\"biotech\",\"blockchain\":\"blockchain\",\"books\":\"books\",\"business\":\"business\",\"cannabis\":\"cannabis\",\"cities\":\"cities\",\"climate-change\":\"climate-change\",\"comics\":\"comics\",\"coronavirus\":\"coronavirus\",\"creativity\":\"creativity\",\"cryptocurrency\":\"cryptocurrency\",\"culture\":\"culture\",\"cybersecurity\":\"cybersecurity\",\"data-science\":\"data-science\",\"design\":\"design\",\"digital-life\":\"digital-life\",\"disability\":\"disability\",\"economy\":\"economy\",\"education\":\"education\",\"equality\":\"equality\",\"family\":\"family\",\"feminism\":\"feminism\",\"fiction\":\"fiction\",\"film\":\"film\",\"fitness\":\"fitness\",\"food\":\"food\",\"freelancing\":\"freelancing\",\"future\":\"future\",\"gadgets\":\"gadgets\",\"gaming\":\"gaming\",\"gun-control\":\"gun-control\",\"health\":\"health\",\"history\":\"history\",\"humor\":\"humor\",\"immigration\":\"immigration\",\"ios-development\":\"ios-development\",\"javascript\":\"javascript\",\"justice\":\"justice\",\"language\":\"language\",\"leadership\":\"leadership\",\"lgbtqia\":\"lgbtqia\",\"lifestyle\":\"lifestyle\",\"machine-learning\":\"machine-learning\",\"makers\":\"makers\",\"marketing\":\"marketing\",\"math\":\"math\",\"media\":\"media\",\"mental-health\":\"mental-health\",\"mindfulness\":\"mindfulness\",\"money\":\"money\",\"music\":\"music\",\"neuroscience\":\"neuroscience\",\"nonfiction\":\"nonfiction\",\"outdoors\":\"outdoors\",\"parenting\":\"parenting\",\"pets\":\"pets\",\"philosophy\":\"philosophy\",\"photography\":\"photography\",\"podcasts\":\"podcast\",\"poetry\":\"poetry\",\"politics\":\"politics\",\"privacy\":\"privacy\",\"product-management\":\"product-management\",\"productivity\":\"productivity\",\"programming\":\"programming\",\"psychedelics\":\"psychedelics\",\"psychology\":\"psychology\",\"race\":\"race\",\"relationships\":\"relationships\",\"religion\":\"religion\",\"remote-work\":\"remote-work\",\"san-francisco\":\"san-francisco\",\"science\":\"science\",\"self\":\"self\",\"self-driving-cars\":\"self-driving-cars\",\"sexuality\":\"sexuality\",\"social-media\":\"social-media\",\"society\":\"society\",\"software-engineering\":\"software-engineering\",\"space\":\"space\",\"spirituality\":\"spirituality\",\"sports\":\"sports\",\"startups\":\"startup\",\"style\":\"style\",\"technology\":\"technology\",\"transportation\":\"transportation\",\"travel\":\"travel\",\"true-crime\":\"true-crime\",\"tv\":\"tv\",\"ux\":\"ux\",\"venture-capital\":\"venture-capital\",\"visual-design\":\"visual-design\",\"work\":\"work\",\"world\":\"world\",\"writing\":\"writing\"},\"defaultImages\":{\"avatar\":{\"imageId\":\"1*dmbNkD5D-u45r44go_cf0g.png\",\"height\":150,\"width\":150},\"orgLogo\":{\"imageId\":\"1*OMF3fSqH8t4xBJ9-6oZDZw.png\",\"height\":106,\"width\":545},\"postLogo\":{\"imageId\":\"1*kFrc4tBFM_tCis-2Ic87WA.png\",\"height\":810,\"width\":1440},\"postPreviewImage\":{\"imageId\":\"1*hn4v1tCaJy7cWMyb0bpNpQ.png\",\"height\":386,\"width\":579}},\"collectionStructuredData\":{\"8d6b8a439e32\":{\"name\":\"Elemental\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fcdn-images-1.medium.com\\u002Fmax\\u002F980\\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png\",\"width\":980,\"height\":159}}},\"3f6ecf56618\":{\"name\":\"Forge\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fcdn-images-1.medium.com\\u002Fmax\\u002F596\\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png\",\"width\":596,\"height\":183}}},\"ae2a65f35510\":{\"name\":\"GEN\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F264\\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png\",\"width\":264,\"height\":140}}},\"88d9857e584e\":{\"name\":\"LEVEL\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F540\\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png\",\"width\":540,\"height\":108}}},\"7b6769f2748b\":{\"name\":\"Marker\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fcdn-images-1.medium.com\\u002Fmax\\u002F383\\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png\",\"width\":383,\"height\":92}}},\"444d13b52878\":{\"name\":\"OneZero\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F540\\u002F1*cw32fIqCbRWzwJaoQw6BUg.png\",\"width\":540,\"height\":123}}},\"8ccfed20cbb2\":{\"name\":\"Zora\",\"data\":{\"@type\":\"NewsMediaOrganization\",\"ethicsPolicy\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Farticles\\u002F360043290473\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F540\\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png\",\"width\":540,\"height\":106}}}},\"embeddedPostIds\":{\"coronavirus\":\"cd3010f9d81f\"},\"sharedCdcMessaging\":{\"COVID_APPLICABLE_TAG_SLUGS\":[],\"COVID_APPLICABLE_TOPIC_NAMES\":[],\"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE\":[],\"COVID_MESSAGES\":{\"tierA\":{\"text\":\"For more information on the novel coronavirus and Covid-19, visit cdc.gov.\",\"markups\":[{\"start\":66,\"end\":73,\"href\":\"https:\\u002F\\u002Fwww.cdc.gov\\u002Fcoronavirus\\u002F2019-nCoV\"}]},\"tierB\":{\"text\":\"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.\",\"markups\":[{\"start\":37,\"end\":45,\"href\":\"https:\\u002F\\u002Fhelp.medium.com\\u002Fhc\\u002Fen-us\\u002Fcategories\\u002F201931128-Policies-Safety\"},{\"start\":125,\"end\":132,\"href\":\"https:\\u002F\\u002Fwww.cdc.gov\\u002Fcoronavirus\\u002F2019-nCoV\"}]},\"paywall\":{\"text\":\"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.\",\"markups\":[{\"start\":56,\"end\":70,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fmembership\"},{\"start\":138,\"end\":145,\"href\":\"https:\\u002F\\u002Fwww.cdc.gov\\u002Fcoronavirus\\u002F2019-nCoV\"}]},\"unbound\":{\"text\":\"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.\",\"markups\":[{\"start\":45,\"end\":59,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fmembership\"},{\"start\":127,\"end\":134,\"href\":\"https:\\u002F\\u002Fwww.cdc.gov\\u002Fcoronavirus\\u002F2019-nCoV\"}]}},\"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST\":[\"3b31a67bff4a\"]},\"sharedVoteMessaging\":{\"TAGS\":[\"politics\",\"election-2020\",\"government\",\"us-politics\",\"election\",\"2020-presidential-race\",\"trump\",\"donald-trump\",\"democrats\",\"republicans\",\"congress\",\"republican-party\",\"democratic-party\",\"biden\",\"joe-biden\",\"maga\"],\"TOPICS\":[\"politics\",\"election\"],\"MESSAGE\":{\"text\":\"Find out more about the U.S. election results here.\",\"markups\":[{\"start\":46,\"end\":50,\"href\":\"https:\\u002F\\u002Fcookpolitical.com\\u002F2020-national-popular-vote-tracker\"}]},\"EXCLUDE_POSTS\":[\"397ef29e3ca5\"]},\"embedPostRules\":[],\"recircOptions\":{\"v1\":{\"limit\":3},\"v2\":{\"limit\":8}},\"braintreeClientKey\":\"production_zjkj96jm_m56f8fqpf7ngnrd4\",\"braintree\":{\"enabled\":true,\"merchantId\":\"m56f8fqpf7ngnrd4\",\"merchantAccountId\":{\"usd\":\"AMediumCorporation_instant\",\"eur\":\"amediumcorporation_EUR\",\"cad\":\"amediumcorporation_CAD\"},\"publicKey\":\"ds2nn34bg2z7j5gd\",\"braintreeEnvironment\":\"production\",\"dashboardUrl\":\"https:\\u002F\\u002Fwww.braintreegateway.com\\u002Fmerchants\",\"gracePeriodDurationInDays\":14,\"mediumMembershipPlanId\":{\"monthly\":\"ce105f8c57a3\",\"monthlyWithTrial\":\"d5ee3dbe3db8\",\"monthlyPremium\":\"fa741a9b47a2\",\"yearly\":\"a40ad4a43185\",\"yearlyStaff\":\"d74fb811198a\",\"yearlyWithTrial\":\"b3bc7350e5c7\",\"yearlyPremium\":\"e21bd2c12166\",\"monthlyCad\":\"p52orjkaceei\",\"yearlyCad\":\"h4q9g2up9ktt\"},\"braintreeDiscountId\":{\"oneMonthFree\":\"MONTHS_FREE_01\",\"threeMonthsFree\":\"MONTHS_FREE_03\",\"sixMonthsFree\":\"MONTHS_FREE_06\",\"fiftyPercentOffOneYear\":\"FIFTY_PERCENT_OFF_ONE_YEAR\"},\"3DSecureVersion\":\"2\",\"defaultCurrency\":\"usd\",\"providerPlanIdCurrency\":{\"4ycw\":\"usd\",\"rz3b\":\"usd\",\"3kqm\":\"usd\",\"jzw6\":\"usd\",\"c2q2\":\"usd\",\"nnsw\":\"usd\",\"q8qw\":\"usd\",\"d9y6\":\"usd\",\"fx7w\":\"cad\",\"nwf2\":\"cad\"}},\"paypalClientId\":\"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v\",\"paypal\":{\"host\":\"https:\\u002F\\u002Fapi.paypal.com:443\",\"clientMode\":\"production\",\"serverMode\":\"live\",\"webhookId\":\"4G466076A0294510S\",\"monthlyPlan\":{\"planId\":\"P-9WR0658853113943TMU5FDQA\",\"name\":\"Medium Membership (Monthly) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"yearlyPlan\":{\"planId\":\"P-7N8963881P8875835MU5JOPQ\",\"name\":\"Medium Membership (Annual) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"oneYearGift\":{\"name\":\"Medium Membership (1 Year, Digital Gift Code)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\\u002Fredeem.\",\"price\":\"50.00\",\"currency\":\"USD\",\"sku\":\"membership-gift-1-yr\"},\"oldMonthlyPlan\":{\"planId\":\"P-96U02458LM656772MJZUVH2Y\",\"name\":\"Medium Membership (Monthly)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"oldYearlyPlan\":{\"planId\":\"P-59P80963JF186412JJZU3SMI\",\"name\":\"Medium Membership (Annual)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"monthlyPlanWithTrial\":{\"planId\":\"P-66C21969LR178604GJPVKUKY\",\"name\":\"Medium Membership (Monthly) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"yearlyPlanWithTrial\":{\"planId\":\"P-6XW32684EX226940VKCT2MFA\",\"name\":\"Medium Membership (Annual) with setup fee\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"oldMonthlyPlanNoSetupFee\":{\"planId\":\"P-4N046520HR188054PCJC7LJI\",\"name\":\"Medium Membership (Monthly)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed monthly.\"},\"oldYearlyPlanNoSetupFee\":{\"planId\":\"P-7A4913502Y5181304CJEJMXQ\",\"name\":\"Medium Membership (Annual)\",\"description\":\"Unlimited access to the best and brightest stories on Medium. Membership billed annually.\"},\"sdkUrl\":\"https:\\u002F\\u002Fwww.paypal.com\\u002Fsdk\\u002Fjs\"},\"stripePublishableKey\":\"pk_live_7FReX44VnNIInZwrIIx6ghjl\",\"log\":{\"json\":true,\"level\":\"info\"},\"imageUploadMaxSizeMb\":25,\"staffPicks\":{\"title\":\"Staff Picks\",\"catalogId\":\"c7bc6e1ee00f\"}},\"session\":{\"xsrf\":\"\"}}</script><script>window.__APOLLO_STATE__ = {\"ROOT_QUERY\":{\"__typename\":\"Query\",\"viewer\":null,\"isLoggedIn\":false,\"collectionByDomainOrSlug({\\\"domainOrSlug\\\":\\\"medium.com\\\"})\":null,\"postResult({\\\"id\\\":\\\"7abfcb69da7f\\\"})\":{\"__ref\":\"Post:7abfcb69da7f\"}},\"LinkedAccounts:f6bbc6865a96\":{\"__typename\":\"LinkedAccounts\",\"mastodon\":null,\"id\":\"f6bbc6865a96\"},\"UserViewerEdge:userId:f6bbc6865a96-viewerId:lo_418bdac11fdd\":{\"__typename\":\"UserViewerEdge\",\"id\":\"userId:f6bbc6865a96-viewerId:lo_418bdac11fdd\",\"isFollowing\":false,\"isUser\":false,\"isMuting\":false},\"NewsletterV3:920e325a463a\":{\"__typename\":\"NewsletterV3\",\"id\":\"920e325a463a\",\"type\":\"NEWSLETTER_TYPE_AUTHOR\",\"slug\":\"f6bbc6865a96\",\"name\":\"f6bbc6865a96\",\"collection\":null,\"user\":{\"__ref\":\"User:f6bbc6865a96\"}},\"User:f6bbc6865a96\":{\"__typename\":\"User\",\"id\":\"f6bbc6865a96\",\"name\":\"The Data Beast\",\"username\":\"thedatabeast\",\"newsletterV3\":{\"__ref\":\"NewsletterV3:920e325a463a\"},\"linkedAccounts\":{\"__ref\":\"LinkedAccounts:f6bbc6865a96\"},\"isSuspended\":false,\"imageId\":\"1*A7eSHxmpHV-LOcc4-DxVnw.png\",\"mediumMemberAt\":0,\"verifications\":{\"__typename\":\"VerifiedInfo\",\"isBookAuthor\":false},\"socialStats\":{\"__typename\":\"SocialStats\",\"followerCount\":326},\"customDomainState\":null,\"hasSubdomain\":false,\"bio\":\"\",\"isPartnerProgramEnrolled\":true,\"viewerEdge\":{\"__ref\":\"UserViewerEdge:userId:f6bbc6865a96-viewerId:lo_418bdac11fdd\"},\"viewerIsUser\":false,\"postSubscribeMembershipUpsellShownAt\":0,\"allowNotes\":true,\"membership\":null,\"twitterScreenName\":\"the_data_beast\"},\"Paragraph:2cf054fdb682_0\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_0\",\"name\":\"1be4\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*DJCRL6_IaSlWcebV_rDzjw.png\":{\"__typename\":\"ImageMetadata\",\"id\":\"1*DJCRL6_IaSlWcebV_rDzjw.png\",\"originalHeight\":1024,\"originalWidth\":1024,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null},\"Paragraph:2cf054fdb682_1\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_1\",\"name\":\"bf56\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"__ref\":\"ImageMetadata:1*DJCRL6_IaSlWcebV_rDzjw.png\"},\"text\":\"\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_2\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_2\",\"name\":\"62b7\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_3\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_3\",\"name\":\"b27d\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Link: BERT Paper\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":6,\"end\":16,\"href\":\"https:\\u002F\\u002Farxiv.org\\u002Fabs\\u002F1810.04805\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_4\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_4\",\"name\":\"6118\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Details: Released by Google AI Language team, BERT introduced a deep bidirectional architecture, which enhanced transfer learning demonstrated by unsupervised pre-training.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_5\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_5\",\"name\":\"d23e\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Practical Implementation: Used for state-of-the-art (SOTA) models in language inference and simple question-answer tasks​​.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_6\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_6\",\"name\":\"2349\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"2. BlenderBot 3: A deployed conversational agent that continually learns to responsibly engage\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_7\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_7\",\"name\":\"685c\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Link: BlenderBot 3 Paper\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":6,\"end\":24,\"href\":\"https:\\u002F\\u002Farxiv.org\\u002Fabs\\u002F2208.03188\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_8\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_8\",\"name\":\"d33f\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Details: From Meta AI, BlenderBot 3, with its 175 billion parameters, can scour the internet, setting it apart from other conversational bots.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_9\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_9\",\"name\":\"7127\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Practical Implementation: Continually learns from deployment data, enhancing engagement and response quality​​.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_10\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_10\",\"name\":\"3dc8\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"3. Improving alignment of dialogue agents via targeted human judgements\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_11\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_11\",\"name\":\"0d47\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Link: Sparrow Paper\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":6,\"end\":19,\"href\":\"https:\\u002F\\u002Farxiv.org\\u002Fabs\\u002F2209.14375\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_12\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_12\",\"name\":\"3ef2\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Details: DeepMind’s Sparrow employs the RLHF method for training, focusing on using human feedback for generative models.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_13\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_13\",\"name\":\"3e22\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Practical Implementation: Helps in building complex goals in chatbots by integrating human feedback effectively​​.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_14\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_14\",\"name\":\"2012\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"4. Improving Language Understanding by Generative Pre-Training\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_15\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_15\",\"name\":\"51bc\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Link: GPT Paper\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":6,\"end\":15,\"href\":\"https:\\u002F\\u002Fwww.semanticscholar.org\\u002Fpaper\\u002FImproving-Language-Understanding-by-Generative-Radford-Narasimhan\\u002Fcd18800a0fe0b668a1cc19f2ec95b5003d0a5035\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_16\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_16\",\"name\":\"1e7c\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Details: OpenAI’s paper on GPT, demonstrating the use of a decoder-style LLM for generative modeling.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_17\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_17\",\"name\":\"979b\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Practical Implementation: Pioneered NLP tasks by generative pre-training of a language model​​.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_18\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_18\",\"name\":\"5157\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"5. Scaling Laws for Neural Language Models\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_19\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_19\",\"name\":\"0dfe\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Link: Scaling Laws Paper\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"__typename\":\"Markup\",\"type\":\"A\",\"start\":6,\"end\":24,\"href\":\"https:\\u002F\\u002Farxiv.org\\u002Fabs\\u002F2001.08361\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null}],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_20\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_20\",\"name\":\"52b8\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Details: OpenAI’s theoretical investigation into the relationship between the size of a language model and its performance.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_21\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_21\",\"name\":\"292b\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Practical Implementation: Provides empirical evidence for the scaling laws that govern model performance​​.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Paragraph:2cf054fdb682_22\":{\"__typename\":\"Paragraph\",\"id\":\"2cf054fdb682_22\",\"name\":\"2f6f\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"6. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation…\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"codeBlockMetadata\":null,\"iframe\":null,\"mixtapeMetadata\":null},\"Tag:llm\":{\"__typename\":\"Tag\",\"id\":\"llm\",\"displayTitle\":\"Llm\",\"normalizedTagSlug\":\"llm\"},\"Tag:ai\":{\"__typename\":\"Tag\",\"id\":\"ai\",\"displayTitle\":\"AI\",\"normalizedTagSlug\":\"ai\"},\"Tag:data-science\":{\"__typename\":\"Tag\",\"id\":\"data-science\",\"displayTitle\":\"Data Science\",\"normalizedTagSlug\":\"data-science\"},\"Tag:research\":{\"__typename\":\"Tag\",\"id\":\"research\",\"displayTitle\":\"Research\",\"normalizedTagSlug\":\"research\"},\"Tag:beginner\":{\"__typename\":\"Tag\",\"id\":\"beginner\",\"displayTitle\":\"Beginner\",\"normalizedTagSlug\":\"beginner\"},\"Post:7abfcb69da7f\":{\"__typename\":\"Post\",\"id\":\"7abfcb69da7f\",\"collection\":null,\"content({\\\"postMeteringOptions\\\":{}})\":{\"__typename\":\"PostContent\",\"isLockedPreviewOnly\":true,\"bodyModel\":{\"__typename\":\"RichText\",\"sections\":[{\"__typename\":\"Section\",\"name\":null,\"startIndex\":0,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null}],\"paragraphs\":[{\"__ref\":\"Paragraph:2cf054fdb682_0\"},{\"__ref\":\"Paragraph:2cf054fdb682_1\"},{\"__ref\":\"Paragraph:2cf054fdb682_2\"},{\"__ref\":\"Paragraph:2cf054fdb682_3\"},{\"__ref\":\"Paragraph:2cf054fdb682_4\"},{\"__ref\":\"Paragraph:2cf054fdb682_5\"},{\"__ref\":\"Paragraph:2cf054fdb682_6\"},{\"__ref\":\"Paragraph:2cf054fdb682_7\"},{\"__ref\":\"Paragraph:2cf054fdb682_8\"},{\"__ref\":\"Paragraph:2cf054fdb682_9\"},{\"__ref\":\"Paragraph:2cf054fdb682_10\"},{\"__ref\":\"Paragraph:2cf054fdb682_11\"},{\"__ref\":\"Paragraph:2cf054fdb682_12\"},{\"__ref\":\"Paragraph:2cf054fdb682_13\"},{\"__ref\":\"Paragraph:2cf054fdb682_14\"},{\"__ref\":\"Paragraph:2cf054fdb682_15\"},{\"__ref\":\"Paragraph:2cf054fdb682_16\"},{\"__ref\":\"Paragraph:2cf054fdb682_17\"},{\"__ref\":\"Paragraph:2cf054fdb682_18\"},{\"__ref\":\"Paragraph:2cf054fdb682_19\"},{\"__ref\":\"Paragraph:2cf054fdb682_20\"},{\"__ref\":\"Paragraph:2cf054fdb682_21\"},{\"__ref\":\"Paragraph:2cf054fdb682_22\"}]},\"validatedShareKey\":\"\",\"shareKeyCreator\":null},\"creator\":{\"__ref\":\"User:f6bbc6865a96\"},\"inResponseToEntityType\":null,\"isLocked\":true,\"isMarkedPaywallOnly\":false,\"lockedSource\":\"LOCKED_POST_SOURCE_UGC\",\"mediumUrl\":\"https:\\u002F\\u002Fmedium.com\\u002F@thedatabeast\\u002Ftop-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\",\"primaryTopic\":null,\"topics\":[{\"__typename\":\"Topic\",\"slug\":\"machine-learning\"}],\"isPublished\":true,\"latestPublishedVersion\":\"2cf054fdb682\",\"visibility\":\"LOCKED\",\"postResponses\":{\"__typename\":\"PostResponses\",\"count\":1},\"createdAt\":1699866174818,\"firstPublishedAt\":1699866559152,\"latestPublishedAt\":1699866559152,\"clapCount\":53,\"allowResponses\":true,\"isLimitedState\":false,\"title\":\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering…\",\"isSeries\":false,\"sequence\":null,\"uniqueSlug\":\"top-10-breakthrough-research-papers-on-large-language-models-llms-in-2023-pioneering-7abfcb69da7f\",\"socialTitle\":\"\",\"socialDek\":\"\",\"noIndex\":null,\"canonicalUrl\":\"\",\"metaDescription\":\"\",\"readingTime\":1.9622641509433962,\"previewContent\":{\"__typename\":\"PreviewContent\",\"subtitle\":\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"},\"previewImage\":{\"__ref\":\"ImageMetadata:1*DJCRL6_IaSlWcebV_rDzjw.png\"},\"isShortform\":false,\"seoTitle\":\"\",\"updatedAt\":1699939894681,\"shortformType\":\"SHORTFORM_TYPE_LINK\",\"seoDescription\":\"\",\"isSuspended\":false,\"license\":\"ALL_RIGHTS_RESERVED\",\"tags\":[{\"__ref\":\"Tag:llm\"},{\"__ref\":\"Tag:ai\"},{\"__ref\":\"Tag:data-science\"},{\"__ref\":\"Tag:research\"},{\"__ref\":\"Tag:beginner\"}],\"pendingCollection\":null,\"statusForCollection\":null,\"detectedLanguage\":\"en\",\"wordCount\":467,\"layerCake\":0}}</script><script>window.__MIDDLEWARE_STATE__={\"session\":{\"xsrf\":\"\"},\"cache\":{\"cacheStatus\":\"HIT\"}}</script><script src=\"https://cdn-client.medium.com/lite/static/js/manifest.749d8bab.js\"></script><script src=\"https://cdn-client.medium.com/lite/static/js/3057.5e22bbb0.js\"></script><script src=\"https://cdn-client.medium.com/lite/static/js/main.24597a77.js\"></script><script src=\"https://cdn-client.medium.com/lite/static/js/instrumentation.7c58a71f.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/reporting.2021fe63.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/4398.db4d4378.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/7883.0e445e04.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/6733.1d85727b.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/4711.043615ac.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/8695.9065ba3d.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/4341.e697d2a1.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/5971.2c86ab13.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/5203.e7a22052.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/5465.248bcf72.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/6487.eef0a2d8.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/5459.80a6ee18.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/6804.2cda7ee2.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/1711.b70f1a35.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/7652.f5b06845.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/7966.0942fdc8.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/9174.24f568ee.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/1128.fce43c64.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/4129.ee8ae2c8.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/8580.feeb2549.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/8883.c8b03d13.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/4078.da7800a7.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/9408.3df4db57.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/9150.42fafb2e.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/5005.b5d4a37c.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/6605.224598fd.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/2393.aaa1ee6d.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/2211.706ab0f5.chunk.js\"></script>\n<script src=\"https://cdn-client.medium.com/lite/static/js/PostPage.MainContent.d6090a33.chunk.js\"></script><script>window.main();</script><script crossorigin=\"anonymous\" data-cf-beacon='{\"rayId\":\"862ef0530a6e580e\",\"b\":1,\"version\":\"2024.2.4\",\"token\":\"0b5f665943484354a59c39c6833f7078\"}' defer=\"\" integrity=\"sha512-euoFGowhlaLqXsPWQ48qSkBSCFs3DPRyiwVu3FjR96cMPx+Fr+gpWRhIafcHwqwCqWS42RZhIudOvEI+Ckf6MA==\" src=\"https://static.cloudflareinsights.com/beacon.min.js/v84a3a4012de94ce1a686ba8c167c359c1696973893317\"></script>\n</body></html>\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/49ac6444-2dc9-4299-89c5-941d1b560981"},{"cell_type":"markdown","metadata":{"id":"TR-IFH1YcDke","deepnote_app_block_visible":true,"cell_id":"6969273f02bb420281a927da879f8112","deepnote_cell_type":"markdown"},"source":"### Convert resulting html into markdown","block_group":"325824d38afb4e399934590f1fbda7be"},{"cell_type":"code","metadata":{"id":"SCfo_vCVcDke","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fb377d00-e08a-472e-e765-dac66e5f4cec","source_hash":"4374083","execution_start":1710195701937,"execution_millis":304,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"38a6fd08b43c49af8acb319e7aa72510","deepnote_cell_type":"code"},"source":"import html2text\n# Function to convert HTML to Markdown\ndef html_to_markdown(html_content):\n    # Create a converter object\n    converter = html2text.HTML2Text()\n    converter.ignore_links = False  # Set to True if you want to ignore converting links\n    \n    # Convert the HTML content to Markdown\n    markdown = converter.handle(html_content)\n\n    return markdown\n\nmarkdown = html_to_markdown(str(response['soup']))\nprint(markdown)","block_group":"8cc94267aef54334ba4fa598f67235c4","execution_count":4,"outputs":[{"name":"stdout","text":"[Open in\napp](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7abfcb69da7f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------)\n\nSign up\n\n[Sign\nin](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&source=post_page---two_column_layout_nav\n-----------------------global_nav-----------)\n\n[](/?source=---two_column_layout_nav----------------------------------)\n\n[Write](/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-\nstory&source=---two_column_layout_nav-----------------------\nnew_post_topnav-----------)\n\n[](/search?source=---two_column_layout_nav----------------------------------)\n\nSign up\n\n[Sign\nin](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&source=post_page---two_column_layout_nav\n-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\nMember-only story\n\n# Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023:\nPioneering Developments and Practical Applications\n\n[![The Data Beast](https://miro.medium.com/v2/resize:fill:88:88/1*A7eSHxmpHV-\nLOcc4-DxVnw.png)](/@thedatabeast?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[The Data Beast](/@thedatabeast?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n·\n\n[Follow](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff6bbc6865a96&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&user=The+Data+Beast&userId=f6bbc6865a96&source=post_page-f6bbc6865a96\n----7abfcb69da7f---------------------post_header-----------)\n\n2 min read\n\n·\n\nNov 13, 2023\n\n[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F7abfcb69da7f&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&user=The+Data+Beast&userId=f6bbc6865a96&source=-----7abfcb69da7f\n---------------------clap_footer-----------)\n\n\\--\n\n1\n\n[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7abfcb69da7f&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&source=-----7abfcb69da7f---------------------\nbookmark_footer-----------)\n\nShare\n\n  1. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n\n  * Link: [BERT Paper](https://arxiv.org/abs/1810.04805)\n  * Details: Released by Google AI Language team, BERT introduced a deep bidirectional architecture, which enhanced transfer learning demonstrated by unsupervised pre-training.\n  * Practical Implementation: Used for state-of-the-art (SOTA) models in language inference and simple question-answer tasks​​.\n\n2\\. BlenderBot 3: A deployed conversational agent that continually learns to\nresponsibly engage\n\n  * Link: [BlenderBot 3 Paper](https://arxiv.org/abs/2208.03188)\n  * Details: From Meta AI, BlenderBot 3, with its 175 billion parameters, can scour the internet, setting it apart from other conversational bots.\n  * Practical Implementation: Continually learns from deployment data, enhancing engagement and response quality​​.\n\n3\\. Improving alignment of dialogue agents via targeted human judgements\n\n  * Link: [Sparrow Paper](https://arxiv.org/abs/2209.14375)\n  * Details: DeepMind’s Sparrow employs the RLHF method for training, focusing on using human feedback for generative models.\n  * Practical Implementation: Helps in building complex goals in chatbots by integrating human feedback effectively​​.\n\n4\\. Improving Language Understanding by Generative Pre-Training\n\n  * Link: [GPT Paper](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n  * Details: OpenAI’s paper on GPT, demonstrating the use of a decoder-style LLM for generative modeling.\n  * Practical Implementation: Pioneered NLP tasks by generative pre-training of a language model​​.\n\n5\\. Scaling Laws for Neural Language Models\n\n  * Link: [Scaling Laws Paper](https://arxiv.org/abs/2001.08361)\n  * Details: OpenAI’s theoretical investigation into the relationship between the size of a language model and its performance.\n  * Practical Implementation: Provides empirical evidence for the scaling laws that govern model performance​​.\n\n6\\. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language\nGeneration…\n\n[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F7abfcb69da7f&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&user=The+Data+Beast&userId=f6bbc6865a96&source=-----7abfcb69da7f\n---------------------clap_footer-----------)\n\n\\--\n\n[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F7abfcb69da7f&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&user=The+Data+Beast&userId=f6bbc6865a96&source=-----7abfcb69da7f\n---------------------clap_footer-----------)\n\n\\--\n\n1\n\n[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7abfcb69da7f&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&source=--------------------------bookmark_footer-----------)\n\n[![The Data\nBeast](https://miro.medium.com/v2/resize:fill:144:144/1*A7eSHxmpHV-\nLOcc4-DxVnw.png)](/@thedatabeast?source=post_page-----\n7abfcb69da7f--------------------------------)\n\nFollow\n\n[](/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F920e325a463a&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&newsletterV3=f6bbc6865a96&newsletterV3Id=920e325a463a&user=The+Data+Beast&userId=f6bbc6865a96&source=-----7abfcb69da7f\n---------------------subscribe_user-----------)\n\n## [Written by The Data Beast](/@thedatabeast?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[326 Followers](/@thedatabeast/followers?source=post_page-----\n7abfcb69da7f--------------------------------)\n\nFollow\n\n[](/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F920e325a463a&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40thedatabeast%2Ftop-10-breakthrough-\nresearch-papers-on-large-language-models-llms-\nin-2023-pioneering-7abfcb69da7f&newsletterV3=f6bbc6865a96&newsletterV3Id=920e325a463a&user=The+Data+Beast&userId=f6bbc6865a96&source=-----7abfcb69da7f\n---------------------subscribe_user-----------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[About](/about?autoplay=1&source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[Careers](/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[Blog](https://blog.medium.com/?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-\npolicy-f03bf92035c9?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-\nservice-9db0094a1e0f?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n[Teams](/business?source=post_page-----\n7abfcb69da7f--------------------------------)\n\n\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/9651cdc3-248c-4909-832a-63344a8ca7d6"},{"cell_type":"markdown","metadata":{"id":"wlDwyEyvcDke","deepnote_app_block_visible":true,"cell_id":"5da578362ba7423a9a797927fcfc504f","deepnote_cell_type":"markdown"},"source":"### Convert markdown into a structured JSON format using function calling","block_group":"c264d0bf9f694ac2b4706ca00f463cb2"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"3da761f3e9e541b58f50f314a487b1c3","deepnote_cell_type":"text-cell-p"},"source":"we'll structure the JSON to include the page title, page summary, and details for each paragraph (title, content, and links)","block_group":"0082319406244e5c8b7fd49fa6fe28e5"},{"cell_type":"code","metadata":{"id":"8x0Cm5xycDke","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f79ef568-15f1-4c02-911b-8df9f3998327","source_hash":"1e42b023","execution_start":1710195704044,"execution_millis":10791,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"8830ea8a1c9948198cf2654fc5ccad79","deepnote_cell_type":"code"},"source":"from pydantic import BaseModel, HttpUrl\nfrom typing import List\nfrom llama_index.program.openai import OpenAIPydanticProgram\nfrom llama_index.llms.openai import OpenAI\nimport tiktoken\nfrom llama_index.core.callbacks import CallbackManager, TokenCountingHandler\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\n# Define your Pydantic models\nclass Link(BaseModel):\n    url: HttpUrl\n\nclass Paper(BaseModel):\n    paper_title: str\n    content: str\n    links: List[Link] = []\n\nclass Page(BaseModel):\n    title: str\n    summary: str\n    paragraphs: List[Paper]\n\n# Define the OpenAI Pydantic program\ndef process_markdown(markdown: str, query: str):\n    max_length: int = 16000  # Updated max length for token count\n\n    # Check token length before splitting\n    token_count = count_tokens(markdown)  # Implement this function\n    if token_count > max_length:\n        markdown_parts = split_into_parts(markdown, max_length)\n    else:\n        markdown_parts = [markdown]  # No need to split\n\n    results = []\n    for part in markdown_parts:\n        print(\"Current part length (tokens):\", count_tokens(part))\n\n        # Define the OpenAI Pydantic program\n        prompt_template_str = \"\"\"\n        Given the following markdown_content, extract only structured information about academic papers including paper title, content, and links. The papers should reflect answers to the user query {user_query}:\n        {markdown_content}\n        \"\"\"\n        program = OpenAIPydanticProgram.from_defaults(\n            output_cls=Page,\n            llm=OpenAI(model=\"gpt-3.5-turbo-1106\"),\n            prompt_template_str=prompt_template_str,\n            allow_multiple=False,\n            verbose=True,\n        )\n\n        # Run the program to get structured output\n        description_str = f\"Structured json of search results based on a user {query}\"\n        try:\n            output = program(markdown_content=part, user_query=query, description=description_str)\n            results.append(output)\n        except Exception as e:\n            # Catch all exceptions\n            if hasattr(e, 'error') and 'message' in e.error:\n                print(f\"Error: {e.error['message']}\")\n            elif hasattr(e, 'args') and e.args:\n                print(f\"Error: {e.args[0]}\")\n            else:\n                print(f\"An unexpected error occurred: {e}\")\n            continue\n\n    # Combine results from all parts or handle as needed\n    combined_result = combine_page_results(results)\n    return combined_result\n\n# Function to count tokens (replace with your implementation)\ndef count_tokens(text: str) -> int:\n    # Use your preferred tokenizer (e.g., tiktoken)\n    tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n    return len(tokenizer(text))\n\n# Assuming 'results' is a list of Page objects or similar structured data\ndef combine_page_results(results: List[Page]) -> Page:\n    if not results:\n        return None  # Or some default value\n    \n    # Start with the title and summary from the first result\n    combined_title = results[0].title\n    combined_summary = results[0].summary\n    combined_paragraphs = []\n\n    # Iterate through all results and combine the paragraphs\n    for result in results:\n        combined_paragraphs.extend(result.paragraphs)  # Assuming 'paragraphs' is a list of 'Paper' objects\n    \n    # Create a new combined Page object\n    combined_page = Page(\n        title=combined_title,\n        summary=combined_summary,\n        paragraphs=combined_paragraphs\n    )\n    return combined_page\n\ndef split_into_parts(text: str, max_length: int) -> List[str]:\n    paragraphs = text.split('\\n\\n')\n    parts = []\n    current_part = \"\"\n\n    for paragraph in paragraphs:\n        if count_tokens(current_part) + count_tokens(paragraph) + 2 > max_length:  # +2 for the two newlines\n            parts.append(current_part)\n            current_part = paragraph  # Start new part with the current paragraph\n        else:\n            # Add paragraph to current part, include two newlines if it's not the first paragraph\n            current_part += ('\\n\\n' + paragraph) if current_part else paragraph\n\n    if current_part:  # Add the last part if not empty\n        parts.append(current_part)\n    \n    return parts\n\n\nresult = process_markdown(markdown, query)\nprint(result)","block_group":"f4717fadcdb9479e882bccadb360185f","execution_count":5,"outputs":[{"name":"stdout","text":"Current part length (tokens): 2513\nFunction call: Page with args: {\"title\":\"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications\",\"summary\":\"The following are the top 10 breakthrough research papers on large language models (LLMs) in 2023, along with their practical applications and details.\",\"paragraphs\":[{\"paper_title\":\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\"content\":\"Released by Google AI Language team, BERT introduced a deep bidirectional architecture, which enhanced transfer learning demonstrated by unsupervised pre-training.\",\"links\":[{\"url\":\"https://arxiv.org/abs/1810.04805\"}]},{\"paper_title\":\"BlenderBot 3: A deployed conversational agent that continually learns to responsibly engage\",\"content\":\"From Meta AI, BlenderBot 3, with its 175 billion parameters, can scour the internet, setting it apart from other conversational bots.\",\"links\":[{\"url\":\"https://arxiv.org/abs/2208.03188\"}]},{\"paper_title\":\"Improving alignment of dialogue agents via targeted human judgements\",\"content\":\"DeepMind’s Sparrow employs the RLHF method for training, focusing on using human feedback for generative models.\",\"links\":[{\"url\":\"https://arxiv.org/abs/2209.14375\"}]},{\"paper_title\":\"Improving Language Understanding by Generative Pre-Training\",\"content\":\"OpenAI’s paper on GPT, demonstrating the use of a decoder-style LLM for generative modeling.\",\"links\":[{\"url\":\"https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035\"}]},{\"paper_title\":\"Scaling Laws for Neural Language Models\",\"content\":\"OpenAI’s theoretical investigation into the relationship between the size of a language model and its performance.\",\"links\":[{\"url\":\"https://arxiv.org/abs/2001.08361\"}]}]}\ntitle='Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications' summary='The following are the top 10 breakthrough research papers on large language models (LLMs) in 2023, along with their practical applications and details.' paragraphs=[Paper(paper_title='BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', content='Released by Google AI Language team, BERT introduced a deep bidirectional architecture, which enhanced transfer learning demonstrated by unsupervised pre-training.', links=[Link(url=HttpUrl('https://arxiv.org/abs/1810.04805', ))]), Paper(paper_title='BlenderBot 3: A deployed conversational agent that continually learns to responsibly engage', content='From Meta AI, BlenderBot 3, with its 175 billion parameters, can scour the internet, setting it apart from other conversational bots.', links=[Link(url=HttpUrl('https://arxiv.org/abs/2208.03188', ))]), Paper(paper_title='Improving alignment of dialogue agents via targeted human judgements', content='DeepMind’s Sparrow employs the RLHF method for training, focusing on using human feedback for generative models.', links=[Link(url=HttpUrl('https://arxiv.org/abs/2209.14375', ))]), Paper(paper_title='Improving Language Understanding by Generative Pre-Training', content='OpenAI’s paper on GPT, demonstrating the use of a decoder-style LLM for generative modeling.', links=[Link(url=HttpUrl('https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035', ))]), Paper(paper_title='Scaling Laws for Neural Language Models', content='OpenAI’s theoretical investigation into the relationship between the size of a language model and its performance.', links=[Link(url=HttpUrl('https://arxiv.org/abs/2001.08361', ))])]\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/20828d14-1b3f-406b-b0d2-3a341c6df5f2"},{"cell_type":"code","metadata":{"id":"DZb-IZjXcDke","colab":{"base_uri":"https://localhost:8080/"},"outputId":"89704bdc-dd84-4a75-a5cc-337437b66aaf","source_hash":"6aa8e4bb","execution_start":1710195714837,"execution_millis":259,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"0b6506fb99b44058bff8c039d8dae400","deepnote_cell_type":"code"},"source":"import json\n\n# Assuming `output` is your object and it has a method `.dict()` to convert it to a dictionary.\n# If `output` is already a dictionary, you can skip the `.dict()` conversion.\noutput_dict = result.dict() if hasattr(result, 'dict') else result\n\n# Convert to JSON string with indentation for readability\npretty_output = json.dumps(output_dict, indent=4, default=str)\n\n# Print with added line breaks\nprint(pretty_output)","block_group":"acb1e3543ff849019a00c96527b04748","execution_count":6,"outputs":[{"name":"stdout","text":"{\n    \"title\": \"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications\",\n    \"summary\": \"The following are the top 10 breakthrough research papers on large language models (LLMs) in 2023, along with their practical applications and details.\",\n    \"paragraphs\": [\n        {\n            \"paper_title\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n            \"content\": \"Released by Google AI Language team, BERT introduced a deep bidirectional architecture, which enhanced transfer learning demonstrated by unsupervised pre-training.\",\n            \"links\": [\n                {\n                    \"url\": \"https://arxiv.org/abs/1810.04805\"\n                }\n            ]\n        },\n        {\n            \"paper_title\": \"BlenderBot 3: A deployed conversational agent that continually learns to responsibly engage\",\n            \"content\": \"From Meta AI, BlenderBot 3, with its 175 billion parameters, can scour the internet, setting it apart from other conversational bots.\",\n            \"links\": [\n                {\n                    \"url\": \"https://arxiv.org/abs/2208.03188\"\n                }\n            ]\n        },\n        {\n            \"paper_title\": \"Improving alignment of dialogue agents via targeted human judgements\",\n            \"content\": \"DeepMind\\u2019s Sparrow employs the RLHF method for training, focusing on using human feedback for generative models.\",\n            \"links\": [\n                {\n                    \"url\": \"https://arxiv.org/abs/2209.14375\"\n                }\n            ]\n        },\n        {\n            \"paper_title\": \"Improving Language Understanding by Generative Pre-Training\",\n            \"content\": \"OpenAI\\u2019s paper on GPT, demonstrating the use of a decoder-style LLM for generative modeling.\",\n            \"links\": [\n                {\n                    \"url\": \"https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035\"\n                }\n            ]\n        },\n        {\n            \"paper_title\": \"Scaling Laws for Neural Language Models\",\n            \"content\": \"OpenAI\\u2019s theoretical investigation into the relationship between the size of a language model and its performance.\",\n            \"links\": [\n                {\n                    \"url\": \"https://arxiv.org/abs/2001.08361\"\n                }\n            ]\n        }\n    ]\n}\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/35750eb5-aea0-4b7e-bb85-54a2de1a5500"},{"cell_type":"markdown","metadata":{"id":"f5hOKcVZGXDC","deepnote_app_block_visible":true,"cell_id":"edd872556d404844a30c5cd6893483a4","deepnote_cell_type":"markdown"},"source":"### Put data into a local database","block_group":"5cb36415784b4a82b85ac5ad8e784e5a"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"3581664c78784171bb50391685060a4d","deepnote_cell_type":"text-cell-p"},"source":"establish db connection","block_group":"bbce0bed79c448a8a74c0b1b48b453b6"},{"cell_type":"code","metadata":{"source_hash":"3ae08e6d","execution_start":1710195714838,"execution_millis":524,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"9bc33b69c73a45e19ec264a0048ab348","deepnote_cell_type":"code"},"source":"import psycopg2\nimport os\n\ndef connection():\n    \"\"\"Creates and returns a new database connection.\"\"\"\n    try:\n        conn = psycopg2.connect(\n            user=os.environ[\"MY_INTEGRATION_USER\"],\n            password=os.environ[\"MY_INTEGRATION_PASSWORD\"],\n            host=os.environ[\"MY_INTEGRATION_HOST\"],\n            port=os.environ[\"MY_INTEGRATION_PORT\"],\n            database=os.environ[\"MY_INTEGRATION_DATABASE\"]\n        )\n        \n        # Test the connection\n        with conn.cursor() as cursor:\n            cursor.execute(\"SELECT version();\")\n            record = cursor.fetchone()\n            # print(\"You are connected to - \", record)\n        \n        return conn  # Return the connection object if successful\n\n    except (Exception, psycopg2.Error) as error:\n        print(\"Error while connecting to database\", error)\n        return None  # Return None if connection was not successful\n\nconn = connection()","block_group":"7e1738211d2d488c96250a71c0085ae8","execution_count":7,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"707e1ac9","execution_start":1710195715365,"execution_millis":807,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"70492ce7597342ed9c27db0807c987d0","deepnote_cell_type":"code"},"source":"import psycopg2\nimport os\n\n# Function to create tables in the database\ndef create_tables():\n    # Define your SQL statements for creating tables\n    sql_commands = [\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS google_search_results (\n            url TEXT PRIMARY KEY,\n            html TEXT,\n            scraping_status TEXT,\n            processed_markdown TEXT,\n            query TEXT\n        );\n        \"\"\",\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS Papers (\n            id SERIAL PRIMARY KEY,\n            paper_title TEXT,\n            source_content TEXT,\n            links TEXT,\n            arxiv_link TEXT UNIQUE,\n            arxiv_title TEXT,\n            arxiv_abstract TEXT,\n            arxiv_metadata TEXT,\n            arxiv_filename TEXT,\n            arxiv_paper_markdown TEXT,\n            citations INTEGER,\n            versions INTEGER\n        );\n        \"\"\",\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS Query_Papers (\n            id SERIAL PRIMARY KEY,\n            query TEXT,\n            arxiv_link TEXT,\n            relevance_score REAL,\n            final_rank INTEGER,\n            relevant_answer TEXT,\n            paper_stats TEXT,\n            paper_metadata_filtered TEXT,\n            download_link TEXT,\n            CONSTRAINT unique_query_arxiv_link UNIQUE (query, arxiv_link)\n        );\n        \"\"\",\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS jobs (\n            job_id SERIAL PRIMARY KEY,\n            query TEXT,\n            job_status TEXT,\n            printed_ranks INTEGER DEFAULT 0,\n            terminal_output TEXT\n        );\n        \"\"\"\n    ]\n    try:\n        with conn.cursor() as cursor:\n            # Execute each SQL command separately\n            for sql_command in sql_commands:\n                cursor.execute(sql_command)\n            conn.commit()  # Commit the transaction\n            print(\"All tables are created successfully.\")\n\n    except (Exception, psycopg2.Error) as error:\n        print(\"Failed to create tables\", error)\n        conn.rollback()  # Rollback the transaction on error\n\n    finally:\n        if conn:\n            conn.close()\n            print(\"Database connection is closed.\")\n# Main script execution\ntry:\n    connection()\n    create_tables()\n\nexcept (Exception, psycopg2.Error) as error:\n    print(\"Error while connecting to database\", error)\n","block_group":"bfd069d9767d431abab154ac1cdd1f1b","execution_count":8,"outputs":[{"name":"stdout","text":"All tables are created successfully.\nDatabase connection is closed.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/c3457638-8653-47dc-844e-fb4c4f101317"},{"cell_type":"code","metadata":{"source_hash":"39cc25e9","execution_start":1710281759062,"execution_millis":652,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"3f342ab2cfc84051bf55a1eca6442e18","deepnote_cell_type":"code"},"source":"import re\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import unquote, urlparse, parse_qs\n\ndef extract_arxiv_url_from_wrapped_url(wrapped_url):\n    # Unquote to handle deeply nested or doubly-encoded URLs\n    wrapped_url = unquote(wrapped_url)\n    # print('Wrapped URL:', wrapped_url)\n    parsed_url = urlparse(wrapped_url)\n    # print('Parsed URL:', parsed_url)\n    query_params = parse_qs(parsed_url.query)\n    # print('Query Parameters:', query_params)\n    # Explore all possible URLs found in the 'url' parameter and extract the arXiv link\n    if 'url' in query_params:\n        for possible_url in query_params['url']:\n            # Check if we have nested URLs and extract the innermost one\n            while 'url=' in possible_url:\n                inner_parsed = parse_qs(urlparse(possible_url).query)\n                if 'url' in inner_parsed:\n                    possible_url = inner_parsed['url'][0]\n                else:\n                    break  # Break if no inner 'url' parameter is found\n            possible_url = unquote(possible_url)  # Ensure the inner URL is fully decoded\n            # print('Possible URL after extraction:', possible_url)\n            \n            # Now check if the final extracted URL is an arXiv link\n            if 'arxiv.org' in possible_url:\n                # print('Extracted arXiv URL:', possible_url)\n                return possible_url  # Return the first arXiv URL found\n    return None  # Return None if no arXiv URL is found\n\ndef clean_arxiv_link(link):\n    # Clean the extracted arXiv link\n    link = unquote(link)  # Ensure the link is fully decoded\n    link_obj = urlparse(link)\n    if re.search(r'(/abs/|/pdf/)[0-9]+\\.[0-9]+', link_obj.path):\n        clean_path = re.sub(r'/pdf/', '/abs/', link_obj.path.split('.pdf')[0])\n        clean_path = re.sub(r'v\\d+$', '', clean_path)  # Remove versioning\n        return f\"https://arxiv.org{clean_path}\"\n    return None\n\ndef insert_arxiv_links_into_db(html_content, user_query):\n    soup = BeautifulSoup(html_content, 'html.parser')\n    links = soup.find_all('a', href=True)\n    \n    arxiv_links = []\n\n    for link in links:\n        href = unquote(link['href']).split(']')[0]  # Decode and preliminarily clean the URL\n        if 'arxiv.org/ct?url=' in href:\n            # Special handling for wrapped links\n            extracted_url = extract_arxiv_url_from_wrapped_url(href)\n            if extracted_url:\n                # Clean and validate the extracted arXiv URL\n                cleaned_link = clean_arxiv_link(extracted_url)\n                if cleaned_link:\n                    arxiv_links.append(cleaned_link)\n                    continue  # Move to the next link after handling a wrapped URL\n\n        # Regular processing for non-wrapped arXiv links\n        if 'arxiv.org' in href and not any(x in href for x in ['/login', '/search', '/about', '/help', '/status']):\n            cleaned_link = clean_arxiv_link(href)\n            if cleaned_link:\n                arxiv_links.append(cleaned_link)\n    \n    # Insert cleaned links into the database\n    print(f\"Cleaned arXiv links[{len(arxiv_links)}]: {arxiv_links}\")\n    # print(f\"Cleaned arXiv links[{len(arxiv_links)}]:\")\n    # for i, link in enumerate(arxiv_links, start=1):\n    #     print(f\"{i}. {link}\")\n\n    # Insertion into the database would go here - omitted for brevity\n    if arxiv_links:\n        try:\n            conn = connection()\n            c = conn.cursor()\n            # Assuming auto-commit is enabled by default; otherwise, manage transactions explicitly if needed\n            # If transactions need to be managed manually, ensure this is done outside of a transaction block\n\n            # Insert arxiv links into Papers table in bulk if there are any\n            arxiv_links_data = [(link,) for link in arxiv_links]  # Prepare data for bulk insert\n            psycopg2.extras.execute_batch(\n                c, \n                \"INSERT INTO Papers (arxiv_link) VALUES (%s) ON CONFLICT (arxiv_link) DO NOTHING\",\n                arxiv_links_data\n            )\n            \n            # Insert records associated with user query in Query_Papers table in bulk\n            query_papers_data = [(user_query, link) for link in arxiv_links]  # Prepare data\n            psycopg2.extras.execute_batch(\n                c, \n                \"INSERT INTO Query_Papers (query, arxiv_link) VALUES (%s, %s) ON CONFLICT (query, arxiv_link) DO NOTHING\",\n                query_papers_data\n            )\n            # Commit the transaction\n            conn.commit()\n            # print(f\"Successfully inserted records associated with the query '{user_query}' into the database.\")\n\n        except Exception as e:\n            # Rollback any changes if an error occurs\n            conn.rollback()\n            print(f\"Transaction rolled back. Error occurred: {e}\")\n        if conn:\n            conn.close()\n# Connect to the database\n\n# Example user query\nuser_query = \"Example Query for Testing\"\n# Example HTML content\nhtml_content = \"\"\"\n<html>\n    <body>\n        <p>Here are some arXiv papers that might interest you:</p>\n        <a href=\"https://arxiv.org/abs/12457457234623434.56789\">Paper 1</a>\n        <a href=\"https://arxiv.org/abs/98724723463246234623466.54321\">Paper 2</a>\n        <a href=\"http://example.com\">Non-arXiv link</a>\n        <a href=\"https://arxiv.org/abs/11223472347234722.3344\">Paper 3</a>\n        <a href=\"https://info.arxiv.org/help/submit_latex_best_practices.html\">LaTeX Best Practices</a>\n        <a href=\"https://info.dev.arxiv.org/about/accessibility_html_error_messages.html\">Accessibility Info</a>\n        <a href=\"https://arxiv.org/abs/2112.08726v1\">Versioned Paper</a>\n        <a href=\"https://arxiv.org/pdf/2112.08726.pdf\">PDF Paper</a>\n        <a href=\"https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf\">FTP PDF Paper</a>\n        <a href=\"https://arxiv.org/pdf/2310.06825.pdf%5D%5D%3E\">Dirty PDF Link</a>\n        <a href=\"https://arxiv.org/abs/2308.09687v2\">Versioned Paper 2</a>\n        <a href=\"https://arxiv.org/ct?url=http://www.bibsonomy.org/BibtexHandler?requTask%3Dupload%26url%3Dhttps://arxiv.org/abs/2402.10200%26description%3DChain-of-Thought+Reasoning+Without+Prompting&v=50e87f9b\">Wrapped Link 1</a>\n        <a href=\"https://arxiv.org/ct?url=https://reddit.com/submit?url%3Dhttps://arxiv.org/abs/2402.99200%26title%3DChain-of-Thought+Reasoning+Without+Prompting&v=8392271f\">Wrapped Link 2</a>\n        <a href=\"https://arxiv.org/login\">Login Page</a>\n        <a href=\"https://arxiv.org/search/advanced\">Advanced Search</a>\n        <a href=\"https://arxiv.org/search/cs?searchtype=author&query=Wang,+X\">Author Search 1</a>\n        <a href=\"https://arxiv.org/search/cs?searchtype=author&query=Zhou,+D\">Author Search 2</a>\n        <a href=\"https://info.arxiv.org/about\">About arXiv</a>\n        <a href=\"https://info.arxiv.org/about/donate.html\">Donate to arXiv</a>\n        <a href=\"https://info.arxiv.org/about/ourmembers.html\">Our Members</a>\n        <a href=\"https://info.arxiv.org/help\">Help Page</a>\n        <a href=\"https://info.arxiv.org/help/contact.html\">Contact Page</a>\n        <a href=\"https://status.arxiv.org\">Status Page</a>\n    </body>\n</html>\n\"\"\"\ninsert_arxiv_links_into_db(html_content, user_query)","block_group":"1b61ac5205dc4d72a902767398261840","execution_count":83,"outputs":[{"name":"stdout","text":"Cleaned arXiv links[9]: ['https://arxiv.org/abs/12457457234623434.56789', 'https://arxiv.org/abs/98724723463246234623466.54321', 'https://arxiv.org/abs/11223472347234722.3344', 'https://arxiv.org/abs/2112.08726', 'https://arxiv.org/abs/2112.08726', 'https://arxiv.org/abs/2310.06825', 'https://arxiv.org/abs/2308.09687', 'https://arxiv.org/abs/2402.10200', 'https://arxiv.org/abs/2402.99200']\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/69e66001-f5b6-4499-8f18-386baa6153be"},{"cell_type":"code","metadata":{"id":"M70X-8aepAuW","colab":{"height":297,"base_uri":"https://localhost:8080/"},"outputId":"11e1c1ba-2216-4163-f33d-6387a72c171d","source_hash":"cb7f2bbc","execution_start":1710195717708,"execution_millis":1037,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"8b6c49cdea1748af9d89532fc84022d9","deepnote_cell_type":"code"},"source":"def insert_papers_into_db(result, query):\n    arxiv_links = []\n    if result is None:\n        print(\"No data to insert into Papers table.\")\n        return\n    # Parse JSON data\n    output_dict = result.dict() if hasattr(result, 'dict') else result\n    pretty_output = json.dumps(output_dict, indent=4, default=str)\n    data = json.loads(pretty_output)\n    if data is None or 'paragraphs' not in data:\n        print(\"Invalid or empty data.\")\n        print('Parsed website stuctured data=', data)\n        return\n\n    # Connect to SQLite database\n    conn = connection()\n    c = conn.cursor()\n\n    try:\n        # Start transaction\n        c.execute(\"BEGIN;\")\n        # Insert data into Papers table\n        for paragraph in data['paragraphs']:\n            paper_title = paragraph['paper_title']\n            source_content = paragraph['content']\n            links = json.dumps(paragraph['links'])  # Convert list of links to JSON string\n            # Initialize an empty arXiv link\n            arxiv_link = None\n            # Search for the arXiv link among the links\n            for link in paragraph['links']:\n                if 'arxiv.org' in link['url']:\n                    temp_link = link['url'].replace('.pdf', '')  # Remove .pdf if present\n                    # Remove any trailing file identifiers after the arXiv ID\n                    temp_link = temp_link.split('/abs/')[1] if '/abs/' in temp_link else temp_link.split('/')[-1]\n                    arxiv_link = 'https://arxiv.org/abs/' + temp_link  # Construct the cleaned arXiv link\n                    # Add the arXiv link to the list\n                    if arxiv_link not in arxiv_links:\n                        arxiv_links.append(arxiv_link)\n                    break  # Stop searching once the arXiv link is found\n            # Check if the arxiv_link already exists in the database\n            c.execute('SELECT COUNT(*) FROM Papers WHERE arxiv_link = %s', (arxiv_link,))\n            if c.fetchone()[0] == 0:  # If the count is 0, then the link does not exist\n                # SQL statement for inserting data\n                insert_sql = '''\n                INSERT INTO Papers (paper_title, source_content, links, arxiv_link) VALUES (%s, %s, %s, %s)\n                '''\n                c.execute(insert_sql, (paper_title, source_content, links, arxiv_link))\n            else:\n                print(f'Skipping insert: arXiv link already exists in the database: {arxiv_link}')\n\n        for link in arxiv_links:\n            # Insert new row into Query_Papers if it does not exist\n            c.execute(\"INSERT INTO Query_Papers (query, arxiv_link) SELECT %s, %s WHERE NOT EXISTS (SELECT 1 FROM Query_Papers WHERE query = %s AND arxiv_link = %s)\", (query, link, query, link))\n\n        # Commit the transaction\n        conn.commit()\n        print(f\"Processed and inserted links associated with the query '{query}' into the database.\")\n\n    except Exception as e:\n        # Rollback the transaction on error\n        conn.rollback()\n        print(f\"An error occurred: {e}. Transaction was rolled back.\")\n\n    finally:\n        if conn:\n            conn.close()\n        pass\n\n# Example data\nquery = \"Example Query for Testing\"\nresult_data = {\n    \"title\": \"Top 10 Breakthrough Research Papers on Large Language Models (LLMs) in 2023: Pioneering Developments and Practical Applications\",\n    \"summary\": \"The following are the top 10 breakthrough research papers on large language models (LLMs) in 2023, along with their practical applications and details.\",\n    \"paragraphs\": [\n        {\"paper_title\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"content\": \"Released by Google AI Language team, BERT introduced a deep bidirectional architecture, which enhanced transfer learning demonstrated by unsupervised pre-training.\", \"links\": [{\"url\": \"https://arxiv.org/abs/1810.04805\"}]},\n        {\"paper_title\": \"BlenderBot 3: A deployed conversational agent that continually learns to responsibly engage\", \"content\": \"From Meta AI, BlenderBot 3, with its 175 billion parameters, can scour the internet, setting it apart from other conversational bots.\", \"links\": [{\"url\": \"https://arxiv.org/abs/2208.03188\"}]},\n        # Add more papers as needed...\n    ]\n}\n# result_string = json.dumps(result_data)\ninsert_papers_into_db(result_data, query)","block_group":"03b49bf5028a4bc396f92e67ac2011e8","execution_count":10,"outputs":[{"name":"stdout","text":"Processed and inserted links associated with the query 'Example Query for Testing' into the database.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/534a33b9-ea06-4b45-b2f6-26dd11f0d94b"},{"cell_type":"code","metadata":{"id":"OWn2sNiIp8uf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2fa2724f-ea6c-4fe1-e2db-3821a368efeb","allow_embed":false,"source_hash":"26ec98cd","execution_start":1710195720181,"execution_millis":708,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"50f9b9b63f9e4b70968896648923a5d6","deepnote_cell_type":"code"},"source":"def print_papers_table():\n    conn = connection()\n    c = conn.cursor()\n\n    try:\n        # Start transaction (mainly useful if there are preceding data manipulations)\n        c.execute(\"BEGIN;\")\n\n        # Query all records from Papers table\n        query_sql = 'SELECT * FROM Papers'  # Add any condition if necessary\n        c.execute(query_sql)\n\n        # Fetch all rows from the query\n        all_rows = c.fetchall()\n\n        # Get the column names\n        field_names = [description[0] for description in c.description]\n\n        # Check if the table is not empty\n        if all_rows:\n            print(\"Preview of Papers Table:\")\n            for row_counter, row in enumerate(all_rows, start=1):\n                print(f\"Row {row_counter}:\")\n                row_with_field_names = {\n                    field_name: (content[:60] + '...' if isinstance(content, str) and len(content) > 60 else content) \n                    for field_name, content in zip(field_names, row)\n                }\n                for field, content in row_with_field_names.items():\n                    print(f\"{field}: {content}\")\n                print(\"-------------\")  # Separator for readability\n        else:\n            print(\"The Papers table is currently empty.\")\n\n        # Commit if there were preceding changes; otherwise, this is optional for read-only operations\n        conn.commit()\n\n    except Exception as e:\n        # Rollback any changes if an exception occurs\n        conn.rollback()\n        print(f\"An error occurred: {e}\")\n\n    finally:\n        if conn:\n            conn.close()\n        pass\n\n# Call the function\nprint_papers_table()","block_group":"6ab16ec9c7a64d16b69f5328f45ed294","execution_count":11,"outputs":[{"name":"stdout","text":"Preview of Papers Table:\nRow 1:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/12457457234623434.56789\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 1\n-------------\nRow 2:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/98724723463246234623466.54321\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 2\n-------------\nRow 3:\npaper_title: None\nsource_content: None\nlinks: None\narxiv_link: https://arxiv.org/abs/11223472347234722.3344\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 3\n-------------\nRow 4:\npaper_title: BERT: Pre-training of Deep Bidirectional Transformers for La...\nsource_content: Released by Google AI Language team, BERT introduced a deep ...\nlinks: [{\"url\": \"https://arxiv.org/abs/1810.04805\"}]\narxiv_link: https://arxiv.org/abs/1810.04805\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 4\n-------------\nRow 5:\npaper_title: BlenderBot 3: A deployed conversational agent that continual...\nsource_content: From Meta AI, BlenderBot 3, with its 175 billion parameters,...\nlinks: [{\"url\": \"https://arxiv.org/abs/2208.03188\"}]\narxiv_link: https://arxiv.org/abs/2208.03188\narxiv_title: None\narxiv_abstract: None\narxiv_metadata: None\narxiv_filename: None\narxiv_paper_markdown: None\ncitations: None\nversions: None\nid: 5\n-------------\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/dedabc4b-1101-4ede-b05a-cd14912a1a1f"},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"527df4672114483c8e3b15334d550cc0","deepnote_cell_type":"markdown"},"source":"Define database function to insert scraping results","block_group":"4636c78c977f43b7b48896a49fc9bb5f"},{"cell_type":"code","metadata":{"id":"WT_D0XfPXrYR","source_hash":"57cbd1ff","execution_start":1710195737671,"execution_millis":651,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"3f7ccc20e42e42229ce39af07759da6b","deepnote_cell_type":"code"},"source":"def insert_scraping_results(url, html, status, query):    \n    conn = connection()\n    c = conn.cursor()\n    try:\n        # Check if the URL already exists in the table\n        c.execute('SELECT COUNT(*) FROM google_search_results WHERE url = %s', (url,))\n        count = c.fetchone()[0]\n\n        if count == 0:\n            # URL does not exist, insert new row\n            c.execute('''\n                INSERT INTO google_search_results (url, html, scraping_status, query)\n                VALUES (%s, %s, %s, %s)\n            ''', (url, html, status, query))\n        else:\n            # URL exists, skip inserting\n            print(\"URL already exists in google_search_results. Skipping insert.\")\n\n        # Commit the transaction\n        conn.commit()\n\n    except Exception as e:\n        # Rollback the transaction if an error occurs\n        conn.rollback()\n        print(f\"An error occurred: {e}. Transaction was rolled back.\")\n    if conn:\n        conn.close()\n\n# Example\ninsert_scraping_results('https://www.topbots.com/top-llm-research-papers-2023/', '<html lang=\"en-US\"><head>..</html>', 200, 'example query')","block_group":"c4c4624363304a04932801a192ba3f05","execution_count":12,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"eaaf737f5c4048258a60100fe87dde31","deepnote_cell_type":"text-cell-p"},"source":"define a database function to check for processed markdown","block_group":"5c894e5199a94f29a3984861b3de9b24"},{"cell_type":"code","metadata":{"source_hash":"1bb61aa3","execution_start":1710195738772,"execution_millis":600,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"8488807ff1454b9ab7065334b52b645d","deepnote_cell_type":"code"},"source":"def check_processed_markdown(url: str) -> bool:\n    conn = connection()\n    \"\"\"Check if the markdown for a given URL has already been processed.\"\"\"\n    c = conn.cursor()\n    c.execute(\"SELECT processed_markdown FROM google_search_results WHERE url = %s\", (url,))\n    result = c.fetchone()\n    if result and result[0]:\n        # If there's processed markdown, return True\n        return True\n    return False\n    if conn:\n        conn.close()\n\n# Example usage:\nurl = 'https://www.topbots.com/top-llm-research-papers-2023/'\ncheck_processed_markdown(url) ","block_group":"615a3e57d9ff43e6b6bcf28399282cac","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"False"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/42d9a5e4-a662-4657-80e3-52d875d57856"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"c50c7d0f062044749a10a0ce12f60140","deepnote_cell_type":"text-cell-p"},"source":"define a database function to insert processed markdown","block_group":"97d4ceb2ae3f431199e3f7c3c91da9f3"},{"cell_type":"code","metadata":{"source_hash":"1ef6728b","execution_start":1710195740521,"execution_millis":570,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"8004aacb12f74d1cac30c562d5857c0b","deepnote_cell_type":"code"},"source":"def insert_processed_markdown(url: str, processed_markdown: dict):  # processed_markdown should be a dict based on usage\n    try:\n        conn = connection()\n        c = conn.cursor()\n        \n        # First, check if the URL exists in the database\n        c.execute('SELECT COUNT(*) FROM google_search_results WHERE url = %s', (url,))\n        url_exists = c.fetchone()[0]\n        \n        if url_exists:\n            # Convert processed_markdown to a JSON string\n            processed_markdown_str = json.dumps(processed_markdown, indent=4)  # Assuming processed_markdown is always a dict based on your usage\n            \n            # Update the row where the URL matches, setting the processed_markdown column\n            c.execute('''\n                UPDATE google_search_results \n                SET processed_markdown = %s \n                WHERE url = %s;\n            ''', (processed_markdown_str, url))\n            conn.commit()\n            print(f\"Processed markdown inserted successfully for URL: {url}\")\n        else:\n            print(f\"No entry found in the database for URL: {url}. Update skipped.\")\n        \n    except Exception as e:\n        print(f\"An error occurred while inserting processed markdown: {e}\")\n    finally:\n        if conn:\n            conn.close()\n            \nurl = 'https://test.url'\nprocessed_markdown = {\n    \"Function call\": \"Page\",\n    \"args\": {\n        \"title\": \"Top academic papers on LLMs\",\n        \"summary\": \"A list of academic papers and resources related to Large Language Models (LLMs) and their applications.\",\n        \"paragraphs\": [\n            {\n                \"paper_title\": \"Awesome-LLM-hallucination\",\n                \"content\": \"LLM hallucination paper list.\",\n                \"links\": [\n                    {\"url\": \"https://github.com/LuckyyySTA/Awesome-LLM-hallucination\"}\n                ]\n            },\n            {\n                \"paper_title\": \"awesome-hallucination-detection\",\n                \"content\": \"List of papers on hallucination detection in LLMs.\",\n                \"links\": [\n                    {\"url\": \"https://github.com/EdinburghNLP/awesome-hallucination-detection\"}\n                ]\n            },\n            {\n                \"paper_title\": \"LLMsPracticalGuide\",\n                \"content\": \"A curated (still actively updated) list of practical guide resources of LLMs\",\n                \"links\": [\n                    {\"url\": \"https://github.com/Mooler0410/LLMsPracticalGuide\"}\n                ]\n            },\n            # Add other papers here in the same format\n        ]\n    }\n}\n\ninsert_processed_markdown(url, processed_markdown)","block_group":"65dc6a72009e4a5c94b853fdac5b0d0d","execution_count":14,"outputs":[{"name":"stdout","text":"No entry found in the database for URL: https://test.url. Update skipped.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/67e16d33-f464-4946-b631-0510e819dc0b"},{"cell_type":"markdown","metadata":{"id":"91Izl-tUsvYs","deepnote_app_block_visible":true,"cell_id":"289b7bd7049649fbad72fcaf3408d521","deepnote_cell_type":"markdown"},"source":"### Get metadata from arxiv for the paper","block_group":"a4e1182f717243be889675cfa4369496"},{"cell_type":"code","metadata":{"id":"QShBS-KWqTqD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"127f03ce-502d-4912-c123-b7b1129df144","source_hash":"356c3982","execution_start":1710195743039,"execution_millis":276,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"65fab026a41b438180be639a7ce83dee","deepnote_cell_type":"code"},"source":"import xml.etree.ElementTree as ET\n\ndef fetch_arxiv_paper_from_url(arxiv_url):\n    # Extract the arXiv ID from the provided URL\n    arxiv_id = arxiv_url.split('/')[-1]\n    # Ensure that .pdf is not part of the arXiv ID\n    arxiv_id = arxiv_id.replace('.pdf', '')  # Remove '.pdf' if it's part of the ID\n\n    print(\"Fetching information for arXiv ID:\", arxiv_id)\n\n    # Define the base URL for the arXiv API\n    base_url = 'http://export.arxiv.org/api/query?'\n    query_params = 'id_list={}&max_results=1'.format(arxiv_id)\n    final_url = base_url + query_params  # Construct the final URL\n    print(\"Final API Request URL:\", final_url)  # Debug: print the URL to be requested\n\n    # Make the request\n    response = requests.get(final_url)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        print(\"Raw XML response received\")\n        xml_data = response.text\n        root = ET.fromstring(xml_data)\n        ns = {'atom': 'http://www.w3.org/2005/Atom'}  # Namespace for parsing\n\n        # Extract paper details\n        link_element = root.find('.//atom:entry/atom:link[@rel=\"related\"]', ns)\n        if link_element is not None:\n            pdf_url = link_element.attrib['href']\n        else:\n            pdf_url = None\n        title = root.find('.//atom:entry/atom:title', ns).text.strip()\n        abstract = root.find('.//atom:entry/atom:summary', ns).text.strip()\n        published_date = root.find('.//atom:entry/atom:published', ns).text.strip()\n\n        # Extract authors\n        authors = [author.find('atom:name', ns).text for author in root.findall('.//atom:entry/atom:author', ns)]\n\n        # Generate a sanitized file name from the title\n        file_name = title.replace(':', '').replace(' ', '_') + '.pdf'\n\n        # Print extracted information for debugging\n        print(f\"PDF URL: {pdf_url}\")\n        print(f\"Title: {title}\")\n        print(f\"File Name: {file_name}\")\n        print(f\"Abstract: {abstract[:100]}...\" if len(abstract) > 100 else abstract)\n        print(f\"Published Date: {published_date}\")\n        print(f\"Authors: {', '.join(authors)}\")\n\n        # Return the collected information\n        return xml_data, pdf_url, title, file_name, abstract, published_date, authors\n    else:\n        print(\"Failed to fetch data from arXiv API. Status code:\", response.status_code)\n        return None, None, None, None, None, None, None\n\n# Example usage\narxiv_url = 'https://arxiv.org/abs/2302.13971'\nfetch_arxiv_paper_from_url(arxiv_url)","block_group":"3c4bf3e6017046bca5920f5fb49096e9","execution_count":15,"outputs":[{"name":"stdout","text":"Fetching information for arXiv ID: 2302.13971\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2302.13971&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2302.13971v1\nTitle: LLaMA: Open and Efficient Foundation Language Models\nFile Name: LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf\nAbstract: We introduce LLaMA, a collection of foundation language models ranging from\n7B to 65B parameters. We...\nPublished Date: 2023-02-27T17:11:15Z\nAuthors: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample\n","output_type":"stream"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2302.13971%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2302.13971&amp;start=0&amp;max_results=1</title>\\n  <id>http://arxiv.org/api/qJuhZNxbRqWajNrNkNtkRSmyBuQ</id>\\n  <updated>2024-03-11T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2302.13971v1</id>\\n    <updated>2023-02-27T17:11:15Z</updated>\\n    <published>2023-02-27T17:11:15Z</published>\\n    <title>LLaMA: Open and Efficient Foundation Language Models</title>\\n    <summary>  We introduce LLaMA, a collection of foundation language models ranging from\\n7B to 65B parameters. We train our models on trillions of tokens, and show that\\nit is possible to train state-of-the-art models using publicly available\\ndatasets exclusively, without resorting to proprietary and inaccessible\\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\\nPaLM-540B. We release all our models to the research community.\\n</summary>\\n    <author>\\n      <name>Hugo Touvron</name>\\n    </author>\\n    <author>\\n      <name>Thibaut Lavril</name>\\n    </author>\\n    <author>\\n      <name>Gautier Izacard</name>\\n    </author>\\n    <author>\\n      <name>Xavier Martinet</name>\\n    </author>\\n    <author>\\n      <name>Marie-Anne Lachaux</name>\\n    </author>\\n    <author>\\n      <name>Timothée Lacroix</name>\\n    </author>\\n    <author>\\n      <name>Baptiste Rozière</name>\\n    </author>\\n    <author>\\n      <name>Naman Goyal</name>\\n    </author>\\n    <author>\\n      <name>Eric Hambro</name>\\n    </author>\\n    <author>\\n      <name>Faisal Azhar</name>\\n    </author>\\n    <author>\\n      <name>Aurelien Rodriguez</name>\\n    </author>\\n    <author>\\n      <name>Armand Joulin</name>\\n    </author>\\n    <author>\\n      <name>Edouard Grave</name>\\n    </author>\\n    <author>\\n      <name>Guillaume Lample</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2302.13971v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2302.13971v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n',\n 'http://arxiv.org/pdf/2302.13971v1',\n 'LLaMA: Open and Efficient Foundation Language Models',\n 'LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf',\n 'We introduce LLaMA, a collection of foundation language models ranging from\\n7B to 65B parameters. We train our models on trillions of tokens, and show that\\nit is possible to train state-of-the-art models using publicly available\\ndatasets exclusively, without resorting to proprietary and inaccessible\\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\\nPaLM-540B. We release all our models to the research community.',\n '2023-02-27T17:11:15Z',\n ['Hugo Touvron',\n  'Thibaut Lavril',\n  'Gautier Izacard',\n  'Xavier Martinet',\n  'Marie-Anne Lachaux',\n  'Timothée Lacroix',\n  'Baptiste Rozière',\n  'Naman Goyal',\n  'Eric Hambro',\n  'Faisal Azhar',\n  'Aurelien Rodriguez',\n  'Armand Joulin',\n  'Edouard Grave',\n  'Guillaume Lample'])"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/7a6a58fe-08ef-41ed-a3be-36c8647d8135"},{"cell_type":"markdown","metadata":{"id":"i9A5UfGBstA0","deepnote_app_block_visible":true,"cell_id":"82738a7559f34ccdad8b031ba05fd7d5","deepnote_cell_type":"markdown"},"source":"### Download pdf of the paper","block_group":"9329080b7fb34539a821e604c0875e0c"},{"cell_type":"code","metadata":{"id":"fZwslypIsf-4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"01af4fd0-2b43-4886-e0ef-eac8ed71e1cf","source_hash":"de2044b4","execution_start":1710195745509,"execution_millis":240,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"2f493d40fd38496d90c90a57c8064805","deepnote_cell_type":"code"},"source":"def download_pdf(pdf_url, file_name):\n    # Create the \"papers\" directory if it doesn't exist\n    papers_dir = \"papers\"\n    if not os.path.exists(papers_dir):\n        os.makedirs(papers_dir)\n\n    # Construct the full file path\n    file_path = os.path.join(papers_dir, file_name)\n\n    # Check if the file already exists\n    if os.path.exists(file_path):\n        print(\"The paper already exists.\")\n        return file_path  # Return the file path\n\n    # Send a GET request to download the PDF\n    response = requests.get(pdf_url)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        # Write the PDF content to the file\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n        print(\"The paper has been downloaded successfully.\")\n        return file_path  # Return the file path\n    else:\n        # Return a status error message\n        error_message = f\"Failed to download the paper. Status code: {response.status_code}\"\n        return error_message\n\n# Example usage\ndownload_pdf('http://arxiv.org/pdf/2302.13971v1', 'LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf')\n\n","block_group":"221abd5c1c544a869145d9f714468673","execution_count":16,"outputs":[{"name":"stdout","text":"The paper already exists.\n","output_type":"stream"},{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"'papers/LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf'"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/815d7a93-5bae-4825-8453-6978d363dfd9"},{"cell_type":"markdown","metadata":{"id":"NlHg6Z20sqKH","deepnote_app_block_visible":true,"cell_id":"b7a1786b345c43c99f2b71a25a97c965","deepnote_cell_type":"markdown"},"source":"### Convert pdf into markdown","block_group":"a23ae86f5c844865af4511949a641b93"},{"cell_type":"code","metadata":{"id":"PCakTs60tTH1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ba2f97f5-42ce-4182-ea5e-4ebb15f13246","source_hash":"55b486db","execution_start":1710195747611,"execution_millis":1969,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"f53abd56a6894ade81db76791227ce9f","deepnote_cell_type":"code"},"source":"import nest_asyncio\nfrom llama_parse import LlamaParse\n\n# This function will convert a given PDF file to Markdown format using LlamaParse\ndef convert_pdf_to_markdown(file_name):\n    # Necessary for running async code in notebooks or scripts\n    nest_asyncio.apply()\n\n    # Initialize the LlamaParse parser\n    parser = LlamaParse(\n        api_key=llamaindex_api_key,\n        result_type=\"markdown\",  # Choose \"markdown\" as the output format\n        verbose=True,  # Enable verbose output to see detailed logs\n    )\n    \n    # Define the path to your PDF file\n    pdf_file_path = os.path.join(\"./papers/\", file_name)\n    print(pdf_file_path, \"type:\", type(pdf_file_path))\n    # Convert the PDF to Markdown\n    # This is a synchronous call, you can also use asynchronous calls as shown in the documentation\n    documents = parser.load_data(pdf_file_path)\n\n    # Return the converted documents\n    return documents\n\n# Define the path to your PDF file\nfile_name = \"Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks.pdf\"\ndocuments = convert_pdf_to_markdown(file_name)","block_group":"aa9b48dc38d0416486e6dd513632f311","execution_count":17,"outputs":[{"name":"stdout","text":"./papers/Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks.pdf type: <class 'str'>\nStarted parsing the file under job_id 967430bd-3d20-4c45-958d-88c13839c3d3\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/2076499c-16b5-45c1-a62b-829afef05a79"},{"cell_type":"code","metadata":{"id":"nTLmIr8AwpmG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"af063990-1cfb-4b52-c74a-9294984976fd","source_hash":"8d5f8090","execution_start":1710195750023,"execution_millis":300,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"c0e822e8b52e437c941781cb6af4050d","deepnote_cell_type":"code"},"source":"markdown_content = None\nif documents:\n    # Assuming the first document contains the content\n    # Use the get_text() method to retrieve the Markdown content\n    markdown_content = documents[0].get_text()\n    print(markdown_content)\n\n    # Optionally, write the markdown content to a file\n    with open('converted_markdown.md', 'w', encoding='utf-8') as markdown_file:\n        markdown_file.write(markdown_content)","block_group":"3d6ca6c2ca164b4c92462bdbaaded69e","execution_count":18,"outputs":[{"name":"stdout","text":"## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\nPatrick Lewis†‡, Ethan Perez⋆, Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†\n\narXiv:2005.11401v4 [cs.CL] 12 Apr 2021\n\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n\n†Facebook AI Research; ‡University College London; ⋆New York University;\n\nplewis@fb.com\n\n### Abstract\n\nLarge pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\n\n### Introduction\n\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data [47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52]. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results.\n---\n## Define \"middle ear\"(x)\n\nThe middle ear includes the tympanic cavity and the three ossicles.\n\n## Question Answering:\n\n|Question Query|Query|Retriever p|η|Document|Generator p|θ|\n|---|---|---|---|---|---|---|\n|Barack Obama was born in Hawaii.(x)|Encoder|(Non-Parametric)| |Index|(Parametric)|Answer Generation supports (y)|\n\n## Fact Verification: Fact Query\n\nThe Divine Comedy (x)\n\n## Jeopardy Question Generation:\n\nAnswer Query\n\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to find the top-K documents zi. For final prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents. but have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned. There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stack-augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training. Our results highlight the benefits of combining parametric and non-parametric memory with generation for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER [56] fact verification, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models’ knowledge as the world changes.\n\n## Methods\n\nWe explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pη(z|x) with parameters η that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pθ(yi|x, z, y1:i−1) parametrized\n\n1 Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transformers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\n---\n## by θ that generates a current token based on a context of the previous i − 1 tokens y1:i−1, the original input x and a retrieved passage z.\n\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pη and pθ components, as well as the training and decoding procedure.\n\n### Models\n\n|RAG-Sequence Model|The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,|\n|---|---|\n| |pRAG-Sequence(y|x) ≈ pη(z|x)pθ(y|x, z) = pη(z|x) Σ pθ(yi|x, z, y1:i−1) z∈top-k(p(·|x)) z∈top-k(p(·|x)) i|\n|RAG-Token Model|In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we define:|\n| |pRAG-Token(y|x) ≈ Σ z∈top-k(p(·|x)) pη(z|x)pθ(yi|x, z, y1:i−1)|\n\nFinally, we note that RAG can be used for sequence classification tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\n\n### Retriever: DPR\n\nThe retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pη(z|x) ∝ exp d(z)⊤q(x) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pη(·|x)), the list of k documents z with highest prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory.\n\n### Generator: BART\n\nThe generator component pθ(yi|x, z, y1:i−1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters θ as the parametric memory henceforth.\n\n### Training\n\nWe jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a fine-tuning training corpus of input/output pairs (xj, yj), we\n---\nminimize the negative marginal log-likelihood of each target, j −log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder BERTq and the BART generator.\n\n## Decoding\n\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate arg max y p(y|x).\n\n|RAG-Token|The RAG-Token model can be seen as a standard, autoregressive seq2seq generator with transition probability: p′ θ(yi|x, y1:i−1) = z∈top-k(p(·|x)) pη(zi|x)pθ(yi|x, zi, y1:i−1) To decode, we can plug p′ θ(yi|x, y1:i−1) into a standard beam decoder.|\n|---|---|\n|RAG-Sequence|For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pθ(yi|x, z, y1:i−1). This yields a set of hypotheses Y, some of which may not have appeared in the beams of all documents. To estimate the probability of a hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer output sequences, |Y| can become large, requiring many forward passes. For more efficient decoding, we can make a further approximation that pθ(y|x, zi) ≈ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as “Fast Decoding.”|\n\n## Experiments\n\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k ∈ {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task.\n\n### Open-domain Question Answering\n\nOpen-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book QA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\n\n### Abstractive Question Answering\n\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat\n---\nMSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as \"What is the weather in Volcano, CA?\" so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses.\n\nJeopardy Question Generation\n\nTo evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question generation. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, \"The World Cup\" is the answer to the question \"In 1986 Mexico scored as the first country to host this international sports competition twice.\" As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task.\n\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for specificity. We define factuality as whether a statement can be corroborated by trusted external sources, and specificity as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options—question A is better, question B is better, both are good, or neither is good.\n\nFact Verification\n\nFEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unverifiable from Wikipedia alone. FEVER is a retrieval problem coupled with a challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models’ ability to handle classification rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\n\nResults\n\nOpen-domain Question Answering\n\n|Task|RAG|State-of-the-Art Models|\n|---|---|---|\n|All four open-domain QA tasks|RAG sets a new state of the art (only on the T5-comparable split for TQA)|RAG combines the generation flexibility of the “closed-book” (parametric only) approaches and the performance of \"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance.|\n\nThere are several advantages to generating answers even when it is possible to extract them. Documents with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading\n---\n## Table 1: Open-Domain QA Test Scores\n\n|Model|NQ|TQA|WQ|CT|\n|---|---|---|---|---|\n|Closed Book T5-11B [52]|34.5|- /50.1|37.4|-|\n|Book T5-11B+SSM[52]|36.6|- /60.5|44.7|-|\n|Open REALM [20]|40.4|- / -|40.7|46.8|\n|Book DPR [26]|41.5|57.9/ -|41.1|50.6|\n|RAG-Token|44.1|55.2/66.1|45.5|50.0|\n|RAG-Seq.|44.5|56.8/68.0|45.2|52.2|\n\n## Table 2: Generation and classification Test Scores\n\n|Model|Jeopardy|MSMARCO|FVR3|FVR2|\n|---|---|---|---|---|\n|B-1|QB-1|R-L|B-1|Label Acc.|\n|BART|15.1|19.7|38.2|41.6|\n|RAG-Tok.|17.3|22.2|40.1|41.5|\n|RAG-Seq.|14.7|21.4|40.8|44.2|\n\n4.2 Abstractive Question Answering\n\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with specific information required to generate the reference answer, (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we find that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5).\n\n4.3 Jeopardy Question Generation\n\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also find RAG generations to be more specific by a large margin. Table 3 shows typical generations from each model.\n\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating “Sun”, the posterior is high for document 2 which mentions “The Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is generated. Intriguingly, after the first token of each book is generated, the document posterior flattens. This observation suggests that the generator can complete the titles without depending on specific documents. In other words, the model’s parametric knowledge is sufficient to complete the titles. We find evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The Sun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly, BART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A with \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows how parametric and non-parametric memories work together—the non-parametric component helps to guide the generation, drawing out specific knowledge stored in the parametric memory.\n\n4.4 Fact Verification\n\nTable 2 shows our results on FEVER. For 3-way classification, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-specific architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\n---\n## Document 1\n\nhis works are considered classics of American literature ... His wartime experiences formed the basis for his novel \"A Farewell to Arms\" (1929) ...\n\n## Document 2\n\n... artists of the 1920s \"Lost Generation\" expatriate community. His debut novel, \"The Sun Also Rises\", was published in 1926.\n\nFigure 2: RAG-Token document posterior p(zi|x, yi, y−i) for each generated token for input “Hemingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\n\n|Task|Input|Model|Generation|\n|---|---|---|---|\n|define middle ear|BART|?|The middle ear is the part of the ear between the middle ear and the nose.|\n| |RAG-T| |The middle ear is the portion of the ear internal to the eardrum.|\n|MS-MARCO|what currency needed in Scotland|BART|The currency needed in Scotland is Pound sterling.|\n| |RAG-T| |Pound is the currency needed in Scotland.|\n| |RAG-S| |The currency needed in Scotland is the pound sterling.|\n| |BART|?|This state has the largest number of counties in the U.S.|\n|Jeopardy Question Generation|Washington|RAG-T|It’s the only U.S. state named for a U.S. president|\n| |RAG-S| |It’s the state where you’ll find Mount Rainier National Park|\n| |BART|*|This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio|\n|The Divine Comedy|RAG-T| |Dante’s \"Inferno\" is the first part of this epic poem|\n| |RAG-S| |This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"|\n\nFor 2-way classification, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved by RAG and gold evidence annotations. We find that the top retrieved document is from a gold article in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n\n## Additional Results\n\nGeneration Diversity: Section 4.3 shows that RAG models are more factual and specific than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding, we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are more diverse than RAG-Token’s, and both are significantly more diverse than BART without needing any diversity-promoting decoding.\n\nRetrieval Ablations: A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks.\n\nWe compare RAG’s dense retriever to a word overlap-based BM25 retriever. Here, we replace RAG’s retriever with a fixed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.\n\nIndex hot-swapping: An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed\n---\n|Model|NQ|TQA|WQ|CT|Jeopardy-QGen|MSMarco|FVR-3|FVR-2|\n|---|---|---|---|---|---|---|---|---|\n|RAG-Token-BM25|29.7|41.5|32.1|33.1|17.5|22.3|55.5|48.4|75.1|91.6|\n|RAG-Sequence-BM25|31.8|44.1|36.6|33.8|11.1|19.5|56.5|46.9|\n|RAG-Token-Frozen|37.8|50.1|37.1|51.1|16.7|21.7|55.9|49.4|72.9|89.4|\n|RAG-Sequence-Frozen|41.2|52.1|41.8|52.6|11.8|19.6|56.7|47.3|\n|RAG-Token|43.5|54.8|46.5|51.9|17.9|22.6|56.2|49.4|74.5|90.6|\n|RAG-Sequence|44.0|55.8|44.9|53.4|15.3|21.5|57.2|47.5|\n\n|Content|Page Number|\n|---|---|\n|Table 4: Human assessments for the Jeopardy Question Generation Task.| |\n|Table 5: Ratio of distinct to total tri-grams for generation tasks.| |\n|Table 6: Ablations on the dev set. As FEVER is a classification task, both RAG models are equivalent.| |\n\nBetween these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”) to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for 2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders). This shows we can update RAG’s world knowledge by simply replacing its non-parametric memory.\n\nEffect of Retrieving more documents: Models are trained with either 5 or 10 retrieved latent documents, and we do not observe significant differences in performance between them. We have the flexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\n\n| |NQ Answer Recall @ K|\n|---|---|\n|NQ Exact Match|80|\n\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\n\nRelated Work: Single-Task Retrieval - Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering, fact checking, fact completion, long-form question answering, Wikipedia article generation, dialogue, translation, and language modeling. Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.\n---\n## General-Purpose Architectures for NLP\n\nPrior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks [60, 61] after fine-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models.\n\n## Learned Retrieval\n\nThere is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.\n\n## Memory-based Architectures\n\nOur document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval [9].\n\n## Retrieve-and-Edit approaches\n\nOur method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a final output. These approaches have proved successful in a number of domains including Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.\n\n## Discussion\n\nIn this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG’s generation over purely parametric BART, finding RAG more factual and specific. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.\n---\n## Broader Impact\n\nThis work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.\n\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [54]. Advanced language models may also lead to the automation of various jobs in the coming decades [16]. In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing.\n\n## Acknowledgments\n\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.\n\n## References\n\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http://arxiv.org/abs/1611.09268. arXiv: 1611.09268.\n[2] Petr Baudiš and Jan Šediv` y. Modeling of pe question answering task in pe yodaqa system. In International Conference of pe Cross-Language Evaluation Forum for European Languages, pages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20.\n[3] Jonapan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of pe 2013 Conference on Empirical Mepods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anpology/D13-1160.\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-ing&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159, 2020. URL https://arxiv.org/abs/2004.07159.\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anpology/P17-1171.\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonapan Berant. Coarse-to-fine question answering for long documents. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anpology/P17-1020.\n---\nChristopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Comprehension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723. arXiv: 1710.10723.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of pe 2019 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anpology/N19-1423.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\nMatpew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A New Q&A Dataset Augmented wip Context from a Search Engine. arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv: 1704.05179.\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of pe 56p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anpology/P18-1082.\nAngela Fan, Yacine Jernite, Epan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of pe 57p Annual Meeting of pe Association for Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/anpology/P19-1346.\nAngela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers wip KNN-based composite memory, 2020. URL https://openreview.net/forum?id=H1gx1CNKPH.\nThibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access wip entity supervision. ArXiv, abs/2004.07202, 2020. URL https://arxiv.org/abs/2004.07202.\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wentau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710.\nKatja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL http://arxiv.org/abs/1705.08807.\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018. 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018.\nKelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of pe Association for Computational Linguistics, 6:437–450, 2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anpology/Q18-1031.\n---\n## References\n\n[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https://arxiv.org/abs/2002.08909.\n[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 10052–10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf.\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-edit-rerank text generation. In Proceedings of pe 58p Annual Meeting of pe Association for Computational Linguistics, pages 2532–2538, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/anpology/2020.acl-main.228.\n[23] Jeff Johnson, Matpijs Douze, and Hervé Jégou. Billion-scale similarity search wip gpus. arXiv preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.\n[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anpology/P17-1147.\n[25] Armand Joulin and Tomas Mikolov. Inferring algoripmic patterns wip stack-augmented recurrent nets. In Proceedings of pe 28p International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, page 190–198, Cambridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/5857-inferring-algoripmic-patterns-wip-stack-augmented-recurrent-nets.\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization prough memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A mepod for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\n[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matpew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. Transactions of pe Association of Computational Linguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf.\n[30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers wip product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9061-large-memory-layers-wip-product-keys.pdf.\n[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of pe 57p Annual Meeting of pe Association\n---\n## References\n\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of pe 2016 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, pages 110–119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anpology/N16-1014.\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation wip optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL https://arxiv.org/abs/1909.03087.\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation wip joint textual and phonetic embedding. In Proceedings of pe 57p Annual Meeting of pe Association for Computational Linguistics, pages 3044–3049, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL https://www.aclweb.org/anpology/P19-1291.\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hyg0vbWC-.\n[37] Yury A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320.\n[38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect pe verifiability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https://arxiv.org/abs/1911.03587.\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\n[41] Nikita Moghe, Siddharpa Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploiting background knowledge for building conversation systems. In Proceedings of pe 2018 Conference on Empirical Mepods in Natural Language Processing, pages 2322–2332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URL https://www.aclweb.org/anpology/D18-1255.\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation systems. In Proceedings of pe 2018 Conference on Empirical Mepods in Natural Language Processing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/anpology/D18-1429.\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors, Proceedings of pe Workshop on Cognitive Computation: Integrating neural and symbolic.\n---\napproaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf.\n\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\n\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.org/anthology/N19-4009.\n\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\n\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://www.aclweb.org/anthology/D19-1250.\n\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models’ factual predictions. In Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum?id=025X0zPfn.\n\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training, 2018. URL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf.\n\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\n\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\n\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/2002.08910.\n\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL https://doi.org/10.1561/1500000019.\n\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019.\n\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\n---\n# References\n\n[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of pe 2018 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://www.aclweb.org/anpology/N18-1074.\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classification wip elastic weight consolidation. ArXiv, abs/2004.14366, 2020. URL https://arxiv.org/abs/2004.14366.\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanapan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaap Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329.\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of pe 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anpology/W18-5446.\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https://arxiv.org/abs/1905.00537.\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain question answering. In Sheila A. McIlraip and Kilian Q. Weinberger, editors, Proceedings of pe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), pe 30p innovative Applications of Artificial Intelligence (IAAI-18), and pe 8p AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712.\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in open-domain question answering. In ICLR, 2018. URL https://openreview.net/forum?id=rJl3yM-Ab.\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and refine: Improved sequence generation models for dialogue. In Proceedings of pe 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anpology/W18-5713.\n---\n## References\n\n[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anpony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers: State-of-pe-art natural language processing. ArXiv, abs/1910.03771, 2019.\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-supervised question answering. In Proceedings of pe 2019 Conference on Empirical Mepods in Natural Language Processing and pe 9p International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2495–2509, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anpology/D19-1253.\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019. URL https://arxiv.org/abs/1909.03745.\n---\n## Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\n### Implementation Details\n\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not find beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\n\n### Human Evaluation\n\n|Which sentence is more factually true?|Select option|\n|---|---|\n|Noje: Scna Guesucn?|snterzt \"The8r Nuso Rists|\n|IncicateFich|Farcncllic AM; on Im|\n|Iclbwng sentarces Is Mca luclualy Injb[ealecllo|ZLbko Uaino cnoc urdoco|\n\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \"view tool guide\".\n\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instructions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.\n\n### Training Setup Details\n\nWe train all RAG models and BART baselines using Fairseq. We train with mixed precision floating point arithmetic, distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We find that doing Maximum Inner Product Search with FAISS is sufficiently fast on CPU, so we store document index vectors on CPU, requiring approximately 100 GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers, which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS’s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/\n\n2. https://github.com/pytorch/fairseq\n\n3. https://github.com/huggingface/transformers\n---\n## Further Details on Open-Domain QA\n\nFor open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to find matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we filter out answer candidates if they do not occur in top 1000 documents for the query.\n\n## CuratedTrec preprocessing\n\nThe answers for CuratedTrec are given in the form of regular expressions, which has been suggested as a reason why it is unsuitable for answer-generation models [20]. To overcome this, we use a pre-processing step where we first retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\n\n## TriviaQA Evaluation setups\n\nThe open-domain QA community customarily uses public development datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading comprehension purposes. We report our results using the datasets splits used in DPR [26], which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA official Wikipedia test set instead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See appendix of [14]). We report results on both test sets to enable fair comparison to both approaches. We find that our performance is much higher using the official Wiki test set, rather than the more conventional open-domain test set, which we attribute to the official Wiki test set questions being simpler to answer from Wikipedia.\n\n## Further Details on FEVER\n\nFor FEVER classification, we follow the practice from [32], and first re-generate the claim, and then classify using the representation of the final hidden state, before finally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The first is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task we explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classification prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work.\n\n## Null Document Probabilities\n\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order to model cases where no useful information could be retrieved for a given input. Here, if k documents were retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null document, before marginalizing over k + 1 predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not find that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to benefit from retrieval, suggesting that null document mechanisms may not be necessary for RAG.\n\n## Parameters\n\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable.\n---\n|Task|Train|Development|Test|\n|---|---|---|---|\n|Natural Questions|79169|8758|3611|\n|TriviaQA|78786|8838|11314|\n|WebQuestions|3418|362|2033|\n|CuratedTrec|635|134|635|\n|Jeopardy Question Generation|97392|13714|26849|\n|MS-MARCO|153726|12468|101093*|\n|FEVER-3-way|145450|10000|10000|\n|FEVER-2-way|96966|6666|6666|\n\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit floating\npoint precision to manage memory and disk footprints.\n\n## Retrieval Collapse\n\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the\nretrieval component would “collapse” and learn to retrieve the same documents regardless of the\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\nin less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\n\n## Number of instances per dataset\n\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/774fbc0d-41a4-4ce4-9cbc-e739edc3996b"},{"cell_type":"markdown","metadata":{"id":"3tM61elq3hrj","deepnote_app_block_visible":true,"cell_id":"17ffcf9815084e5fa2002a2270ba5de0","deepnote_cell_type":"markdown"},"source":"### Get citations and number of versions from Google Scholar","block_group":"b8e3e9beee96482c8a18da4d133a0393"},{"cell_type":"code","metadata":{"id":"mwuv2gRw3mv-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bdfaf190-a448-4bc6-9bd7-5ae42a5e41dd","source_hash":"6bb279e6","execution_start":1710195751221,"execution_millis":147,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"3cbeef9dc096403499243ad238f9194e","deepnote_cell_type":"code"},"source":"from serpapi import GoogleSearch\n\ndef get_scholar_citations_versions(query_url):\n    params = {\n        \"api_key\": serp_api_key,  # Ensure serp_api_key is defined elsewhere\n        \"engine\": \"google_scholar\",\n        \"q\": query_url,\n        \"hl\": \"en\"\n    }\n\n    search = GoogleSearch(params)\n    results = search.get_dict()\n\n    # Initialize the return values\n    number_of_citations = None\n    number_of_versions = None\n\n    # Extracting number of citations and versions\n    if 'organic_results' in results:\n        if 'inline_links' in results['organic_results'][0]:\n            if 'cited_by' in results['organic_results'][0]['inline_links']:\n                number_of_citations = results[\"organic_results\"][0][\"inline_links\"][\"cited_by\"][\"total\"]\n\n            if 'versions' in results['organic_results'][0]['inline_links']:\n                number_of_versions = results[\"organic_results\"][0][\"inline_links\"][\"versions\"][\"total\"]\n\n    return number_of_citations, number_of_versions\n\nquery_url = 'https://arxiv.org/abs/2302.13971'\ncitations, versions = get_scholar_citations_versions(query_url)\nprint(\"Number of citations:\", citations)\nprint(\"Number of versions:\", versions)","block_group":"64707be5b9684185b05d52d700f0ecc3","execution_count":19,"outputs":[{"name":"stdout","text":"Number of citations: 4450\nNumber of versions: 13\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/f4c603f5-5a7a-45df-a7c0-e06b29f1eb83"},{"cell_type":"markdown","metadata":{"id":"u9eHATZLi5ZV","deepnote_app_block_visible":true,"cell_id":"7db90d63c5ee477fadc3fcaefa116c5e","deepnote_cell_type":"markdown"},"source":"### Gemini summary and relevance score","block_group":"303d9cd81b044716802ee7b71c614f28"},{"cell_type":"markdown","metadata":{"id":"-gQldNXOVd68","deepnote_app_block_visible":true,"cell_id":"8bd63746631d4fd888104de5728f7af1","deepnote_cell_type":"markdown"},"source":"Gemini Set up","block_group":"faa953eb4647427db8a2f9cea784cc67"},{"cell_type":"code","metadata":{"id":"GWLjB54KVgGb","colab":{"height":332,"base_uri":"https://localhost:8080/"},"outputId":"8b5b4290-7941-4489-9bab-6149fac379e6","source_hash":"c3b85edd","execution_start":1710195753346,"execution_millis":152,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"40c54655224746b7a4fd57cdcbaaa818","deepnote_cell_type":"code"},"source":"import google.generativeai as genai\ngenai.configure(api_key=gemini_api_key)\nmodel = genai.GenerativeModel('gemini-1.0-pro')","block_group":"8b3b32ec98e14047b104f2506295ba54","execution_count":20,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"d991cab6","execution_start":1710195755176,"execution_millis":77,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"4241ee38055245fab054980baf5462bc","deepnote_cell_type":"code"},"source":"model","block_group":"4a0d5e23b54a431a82ccbb0d4aca39b9","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"genai.GenerativeModel(\n    model_name='models/gemini-1.0-pro',\n    generation_config={},\n    safety_settings={},\n    tools=None,\n)"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/39e89d0e-cab1-4e4c-88f3-0cfaab62062a"},{"cell_type":"markdown","metadata":{"id":"nvyJkedJU2Pv","deepnote_app_block_visible":true,"cell_id":"e94a45f53692417d941389ae227e4b3e","deepnote_cell_type":"markdown"},"source":"Given `arxiv` structure, summarize and evaluate against user prompt. Give a heuritic score.","block_group":"fa2a223bf8034417a6d1fa9e5f8c8f52"},{"cell_type":"code","metadata":{"id":"SoqeHjBAihAD","source_hash":"389c1e7","execution_start":1710195756174,"execution_millis":421,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"bbdda948ae7447d59b2ab4dcbda5affb","deepnote_cell_type":"code"},"source":"import re\nimport json\n\ndef process_arxiv(mkdn, metdata, query):\n  # 1 - `arxiv` dict\n  def extract_markdown(markdown_text, pattern):\n    # Use re.findall to find all matches of the pattern in the markdown text\n    matches = re.findall(pattern, markdown_text, re.MULTILINE)\n\n    # Return the first match (if any)\n    if matches:\n        return matches[0]\n    else:\n        return None\n\n  paper_title = extract_markdown(mkdn, r'^##\\s+(.*)$')\n  if paper_title is None:\n    print(\"extract_markdown for paper_title isn't working\")\n\n  abstract = extract_markdown(mkdn, r'^Abstract(.*)#')\n  if abstract is None:\n    print(\"extract_markdown for abstract isn't working...hardcoding the abstract instead\")\n    abstract = '''We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.'''\n\n  arxiv = {'paper_title': paper_title, 'abstract': abstract, 'metadata': metadata, 'paper': mkdn}\n\n  # 2 - Summarizer\n  prompt = '''Please summarize the following paper in one sentence given the user query \"{query}\". The paper is provided in a structured format {paper_format} \\n\\nDocument: {document}'''.format(query=query, document=arxiv, paper_format={key: \"\" for key in arxiv.keys()})\n  print(prompt, \"\\nGenerating summarization............\")\n\n  if model.count_tokens(prompt).total_tokens > 28_000:\n    print(\"The prompt is too long, visiting https://aistudio.google.com/app/prompts/new_freeform to manually use Gemini 1.5 pro instead with the prompt above.\")\n  relevant_answer = model.generate_content(prompt).text\n\n  print(relevant_answer)\n\n  # 3 - Relevance scorer\n  prompt = '''From a scale of 1 to 5, rate how relevant the following paper is with the user query \"{query}\". The paper is provided in a structured format {paper_format}. Please provide the score in the format of a json object with one key, 'score'. Example: {{\"score\": 5}}. Also please provide reasoning why it doesn't have a higher or lower relevance score. \\n\\nDocument: {document}'''.format(query=query, document=arxiv, paper_format={key: \"\" for key in arxiv.keys()})\n  print(prompt, \"\\nGenerating............\")\n\n  if model.count_tokens(prompt).total_tokens > 28_000:\n    print(\"The prompt is too long, visiting https://aistudio.google.com/app/prompts/new_freeform to manually use Gemini 1.5 pro instead with the prompt above.\")\n\n  model_response = model.generate_content(prompt).text\n\n  re_match = re.search(r'\"score\": (\\d+)', model_response)\n  relevance_score = re_match.group(1)\n\n  print(\"relevance score: \" + relevance_score)\n\n  return {\n      'relevance_score': relevance_score,\n      'relevant_answer': relevant_answer\n  }\n\n  query\n\n  #@title `mkdn` and `metadata`\nmetadata = markdown_content #right now it's just the entire paper pdf\n\n#@title Extractors to process `mkdn` and `metadata` into `arxiv` dict\n\nimport re\n\ndef extract_markdown(markdown_text, pattern):\n  # Use re.findall to find all matches of the pattern in the markdown text\n  matches = re.findall(pattern, markdown_text, re.MULTILINE)\n\n  # Return the first match (if any)\n  if matches:\n      return matches[0]\n  else:\n      return None\n\npaper_title = extract_markdown(metadata, r'^##\\s+(.*)$')\nif paper_title is None:\n  print(\"extract_markdown for paper_title isn't working\")\n\nabstract = extract_markdown(metadata, r'^Abstract(.*)#')\nif abstract is None:\n  print(\"extract_markdown for abstract isn't working...hardcoding the abstract instead\")\n  abstract = '''We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.'''\n\narxiv = {'paper_title': paper_title, 'abstract': abstract, 'metadata': metadata, 'paper': metadata}\n\n\n\nimport json\n\n# Convert to JSON string with indentation for readability\npretty_arxiv_output = json.dumps(arxiv, indent=4, default=str)\n\n# Print with added line breaks\nprint(\"\\narxiv=\",)\nprint(pretty_arxiv_output)","block_group":"e3a2b46fdc674e5aa372ece84f9f2082","execution_count":22,"outputs":[{"name":"stdout","text":"extract_markdown for abstract isn't working...hardcoding the abstract instead\n\narxiv=\n{\n    \"paper_title\": \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\",\n    \"abstract\": \"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.\",\n    \"metadata\": \"## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\nPatrick Lewis\\u2020\\u2021, Ethan Perez\\u22c6, Aleksandra Piktus\\u2020, Fabio Petroni\\u2020, Vladimir Karpukhin\\u2020, Naman Goyal\\u2020, Heinrich K\\u00fcttler\\u2020\\n\\narXiv:2005.11401v4 [cs.CL] 12 Apr 2021\\n\\nMike Lewis\\u2020, Wen-tau Yih\\u2020, Tim Rockt\\u00e4schel\\u2020\\u2021, Sebastian Riedel\\u2020\\u2021, Douwe Kiela\\u2020\\n\\n\\u2020Facebook AI Research; \\u2021University College London; \\u22c6New York University;\\n\\nplewis@fb.com\\n\\n### Abstract\\n\\nLarge pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \\u2014 models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\\n\\n### Introduction\\n\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data [47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52]. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can\\u2019t straightforwardly provide insight into their predictions, and may produce \\u201challucinations\\u201d [38]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results.\\n---\\n## Define \\\"middle ear\\\"(x)\\n\\nThe middle ear includes the tympanic cavity and the three ossicles.\\n\\n## Question Answering:\\n\\n|Question Query|Query|Retriever p|\\u03b7|Document|Generator p|\\u03b8|\\n|---|---|---|---|---|---|---|\\n|Barack Obama was born in Hawaii.(x)|Encoder|(Non-Parametric)| |Index|(Parametric)|Answer Generation supports (y)|\\n\\n## Fact Verification: Fact Query\\n\\nThe Divine Comedy (x)\\n\\n## Jeopardy Question Generation:\\n\\nAnswer Query\\n\\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to find the top-K documents zi. For final prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents. but have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the \\u201cworkhorse of NLP,\\u201d i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned. There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stack-augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training. Our results highlight the benefits of combining parametric and non-parametric memory with generation for knowledge-intensive tasks\\u2014tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER [56] fact verification, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models\\u2019 knowledge as the world changes.\\n\\n## Methods\\n\\nWe explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever p\\u03b7(z|x) with parameters \\u03b7 that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator p\\u03b8(yi|x, z, y1:i\\u22121) parametrized\\n\\n1 Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transformers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n---\\n## by \\u03b8 that generates a current token based on a context of the previous i \\u2212 1 tokens y1:i\\u22121, the original input x and a retrieved passage z.\\n\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the p\\u03b7 and p\\u03b8 components, as well as the training and decoding procedure.\\n\\n### Models\\n\\n|RAG-Sequence Model|The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,|\\n|---|---|\\n| |pRAG-Sequence(y|x) \\u2248 p\\u03b7(z|x)p\\u03b8(y|x, z) = p\\u03b7(z|x) \\u03a3 p\\u03b8(yi|x, z, y1:i\\u22121) z\\u2208top-k(p(\\u00b7|x)) z\\u2208top-k(p(\\u00b7|x)) i|\\n|RAG-Token Model|In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we define:|\\n| |pRAG-Token(y|x) \\u2248 \\u03a3 z\\u2208top-k(p(\\u00b7|x)) p\\u03b7(z|x)p\\u03b8(yi|x, z, y1:i\\u22121)|\\n\\nFinally, we note that RAG can be used for sequence classification tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n\\n### Retriever: DPR\\n\\nThe retrieval component p\\u03b7(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: p\\u03b7(z|x) \\u221d exp d(z)\\u22a4q(x) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(p\\u03b7(\\u00b7|x)), the list of k documents z with highest prior probability p\\u03b7(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory.\\n\\n### Generator: BART\\n\\nThe generator component p\\u03b8(yi|x, z, y1:i\\u22121) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters \\u03b8 as the parametric memory henceforth.\\n\\n### Training\\n\\nWe jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a fine-tuning training corpus of input/output pairs (xj, yj), we\\n---\\nminimize the negative marginal log-likelihood of each target, j \\u2212log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder BERTq and the BART generator.\\n\\n## Decoding\\n\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate arg max y p(y|x).\\n\\n|RAG-Token|The RAG-Token model can be seen as a standard, autoregressive seq2seq generator with transition probability: p\\u2032 \\u03b8(yi|x, y1:i\\u22121) = z\\u2208top-k(p(\\u00b7|x)) p\\u03b7(zi|x)p\\u03b8(yi|x, zi, y1:i\\u22121) To decode, we can plug p\\u2032 \\u03b8(yi|x, y1:i\\u22121) into a standard beam decoder.|\\n|---|---|\\n|RAG-Sequence|For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using p\\u03b8(yi|x, z, y1:i\\u22121). This yields a set of hypotheses Y, some of which may not have appeared in the beams of all documents. To estimate the probability of a hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with p\\u03b7(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as \\u201cThorough Decoding.\\u201d For longer output sequences, |Y| can become large, requiring many forward passes. For more efficient decoding, we can make a further approximation that p\\u03b8(y|x, zi) \\u2248 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as \\u201cFast Decoding.\\u201d|\\n\\n## Experiments\\n\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k \\u2208 {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task.\\n\\n### Open-domain Question Answering\\n\\nOpen-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to \\u201cClosed-Book QA\\u201d approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\\n\\n### Abstractive Question Answering\\n\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG\\u2019s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n---\\nMSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as \\\"What is the weather in Volcano, CA?\\\" so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses.\\n\\nJeopardy Question Generation\\n\\nTo evaluate RAG\\u2019s generation abilities in a non-QA setting, we study open-domain question generation. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, \\\"The World Cup\\\" is the answer to the question \\\"In 1986 Mexico scored as the first country to host this international sports competition twice.\\\" As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task.\\n\\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for specificity. We define factuality as whether a statement can be corroborated by trusted external sources, and specificity as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options\\u2014question A is better, question B is better, both are good, or neither is good.\\n\\nFact Verification\\n\\nFEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unverifiable from Wikipedia alone. FEVER is a retrieval problem coupled with a challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models\\u2019 ability to handle classification rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren\\u2019t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n\\nResults\\n\\nOpen-domain Question Answering\\n\\n|Task|RAG|State-of-the-Art Models|\\n|---|---|---|\\n|All four open-domain QA tasks|RAG sets a new state of the art (only on the T5-comparable split for TQA)|RAG combines the generation flexibility of the \\u201cclosed-book\\u201d (parametric only) approaches and the performance of \\\"open-book\\\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized \\u201csalient span masking\\u201d pre-training [20]. It is worth noting that RAG\\u2019s retriever is initialized using DPR\\u2019s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based \\u201ccross-encoder\\u201d to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance.|\\n\\nThere are several advantages to generating answers even when it is possible to extract them. Documents with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading\\n---\\n## Table 1: Open-Domain QA Test Scores\\n\\n|Model|NQ|TQA|WQ|CT|\\n|---|---|---|---|---|\\n|Closed Book T5-11B [52]|34.5|- /50.1|37.4|-|\\n|Book T5-11B+SSM[52]|36.6|- /60.5|44.7|-|\\n|Open REALM [20]|40.4|- / -|40.7|46.8|\\n|Book DPR [26]|41.5|57.9/ -|41.1|50.6|\\n|RAG-Token|44.1|55.2/66.1|45.5|50.0|\\n|RAG-Seq.|44.5|56.8/68.0|45.2|52.2|\\n\\n## Table 2: Generation and classification Test Scores\\n\\n|Model|Jeopardy|MSMARCO|FVR3|FVR2|\\n|---|---|---|---|---|\\n|B-1|QB-1|R-L|B-1|Label Acc.|\\n|BART|15.1|19.7|38.2|41.6|\\n|RAG-Tok.|17.3|22.2|40.1|41.5|\\n|RAG-Seq.|14.7|21.4|40.8|44.2|\\n\\n4.2 Abstractive Question Answering\\n\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with specific information required to generate the reference answer, (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we find that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see \\u00a74.5).\\n\\n4.3 Jeopardy Question Generation\\n\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also find RAG generations to be more specific by a large margin. Table 3 shows typical generations from each model.\\n\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating \\u201cSun\\u201d, the posterior is high for document 2 which mentions \\u201cThe Sun Also Rises\\u201d. Similarly, document 1 dominates the posterior when \\u201cA Farewell to Arms\\u201d is generated. Intriguingly, after the first token of each book is generated, the document posterior flattens. This observation suggests that the generator can complete the titles without depending on specific documents. In other words, the model\\u2019s parametric knowledge is sufficient to complete the titles. We find evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \\\"The Sun. BART completes the generation \\\"The Sun Also Rises\\\" is a novel by this author of \\\"The Sun Also Rises\\\" indicating the title \\\"The Sun Also Rises\\\" is stored in BART\\u2019s parameters. Similarly, BART will complete the partial decoding \\\"The Sun Also Rises\\\" is a novel by this author of \\\"A with \\\"The Sun Also Rises\\\" is a novel by this author of \\\"A Farewell to Arms\\\". This example shows how parametric and non-parametric memories work together\\u2014the non-parametric component helps to guide the generation, drawing out specific knowledge stored in the parametric memory.\\n\\n4.4 Fact Verification\\n\\nTable 2 shows our results on FEVER. For 3-way classification, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-specific architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n---\\n## Document 1\\n\\nhis works are considered classics of American literature ... His wartime experiences formed the basis for his novel \\\"A Farewell to Arms\\\" (1929) ...\\n\\n## Document 2\\n\\n... artists of the 1920s \\\"Lost Generation\\\" expatriate community. His debut novel, \\\"The Sun Also Rises\\\", was published in 1926.\\n\\nFigure 2: RAG-Token document posterior p(zi|x, yi, y\\u2212i) for each generated token for input \\u201cHemingway\\\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating \\u201cA Farewell to Arms\\\" and for document 2 when generating \\u201cThe Sun Also Rises\\\".\\n\\n|Task|Input|Model|Generation|\\n|---|---|---|---|\\n|define middle ear|BART|?|The middle ear is the part of the ear between the middle ear and the nose.|\\n| |RAG-T| |The middle ear is the portion of the ear internal to the eardrum.|\\n|MS-MARCO|what currency needed in Scotland|BART|The currency needed in Scotland is Pound sterling.|\\n| |RAG-T| |Pound is the currency needed in Scotland.|\\n| |RAG-S| |The currency needed in Scotland is the pound sterling.|\\n| |BART|?|This state has the largest number of counties in the U.S.|\\n|Jeopardy Question Generation|Washington|RAG-T|It\\u2019s the only U.S. state named for a U.S. president|\\n| |RAG-S| |It\\u2019s the state where you\\u2019ll find Mount Rainier National Park|\\n| |BART|*|This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio|\\n|The Divine Comedy|RAG-T| |Dante\\u2019s \\\"Inferno\\\" is the first part of this epic poem|\\n| |RAG-S| |This 14th century work is divided into 3 sections: \\\"Inferno\\\", \\\"Purgatorio\\\" & \\\"Paradiso\\\"|\\n\\nFor 2-way classification, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved by RAG and gold evidence annotations. We find that the top retrieved document is from a gold article in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\\n\\n## Additional Results\\n\\nGeneration Diversity: Section 4.3 shows that RAG models are more factual and specific than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding, we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence\\u2019s generations are more diverse than RAG-Token\\u2019s, and both are significantly more diverse than BART without needing any diversity-promoting decoding.\\n\\nRetrieval Ablations: A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks.\\n\\nWe compare RAG\\u2019s dense retriever to a word overlap-based BM25 retriever. Here, we replace RAG\\u2019s retriever with a fixed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\n\\nIndex hot-swapping: An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n---\\n|Model|NQ|TQA|WQ|CT|Jeopardy-QGen|MSMarco|FVR-3|FVR-2|\\n|---|---|---|---|---|---|---|---|---|\\n|RAG-Token-BM25|29.7|41.5|32.1|33.1|17.5|22.3|55.5|48.4|75.1|91.6|\\n|RAG-Sequence-BM25|31.8|44.1|36.6|33.8|11.1|19.5|56.5|46.9|\\n|RAG-Token-Frozen|37.8|50.1|37.1|51.1|16.7|21.7|55.9|49.4|72.9|89.4|\\n|RAG-Sequence-Frozen|41.2|52.1|41.8|52.6|11.8|19.6|56.7|47.3|\\n|RAG-Token|43.5|54.8|46.5|51.9|17.9|22.6|56.2|49.4|74.5|90.6|\\n|RAG-Sequence|44.0|55.8|44.9|53.4|15.3|21.5|57.2|47.5|\\n\\n|Content|Page Number|\\n|---|---|\\n|Table 4: Human assessments for the Jeopardy Question Generation Task.| |\\n|Table 5: Ratio of distinct to total tri-grams for generation tasks.| |\\n|Table 6: Ablations on the dev set. As FEVER is a classification task, both RAG models are equivalent.| |\\n\\nBetween these dates and use a template \\u201cWho is {position}?\\u201d (e.g. \\u201cWho is the President of Peru?\\u201d) to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for 2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders). This shows we can update RAG\\u2019s world knowledge by simply replacing its non-parametric memory.\\n\\nEffect of Retrieving more documents: Models are trained with either 5 or 10 retrieved latent documents, and we do not observe significant differences in performance between them. We have the flexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n\\n| |NQ Answer Recall @ K|\\n|---|---|\\n|NQ Exact Match|80|\\n\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n\\nRelated Work: Single-Task Retrieval - Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering, fact checking, fact completion, long-form question answering, Wikipedia article generation, dialogue, translation, and language modeling. Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.\\n---\\n## General-Purpose Architectures for NLP\\n\\nPrior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks [60, 61] after fine-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models.\\n\\n## Learned Retrieval\\n\\nThere is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.\\n\\n## Memory-based Architectures\\n\\nOur document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model\\u2019s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval [9].\\n\\n## Retrieve-and-Edit approaches\\n\\nOur method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a final output. These approaches have proved successful in a number of domains including Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.\\n\\n## Discussion\\n\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG\\u2019s generation over purely parametric BART, finding RAG more factual and specific. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.\\n---\\n## Broader Impact\\n\\nThis work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it \\u201challucinate\\u201d less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.\\n\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [54]. Advanced language models may also lead to the automation of various jobs in the coming decades [16]. In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing.\\n\\n## Acknowledgments\\n\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.\\n\\n## References\\n\\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http://arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n[2] Petr Baudi\\u0161 and Jan \\u0160ediv` y. Modeling of pe question answering task in pe yodaqa system. In International Conference of pe Cross-Language Evaluation Forum for European Languages, pages 222\\u2013228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20.\\n[3] Jonapan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of pe 2013 Conference on Empirical Mepods in Natural Language Processing, pages 1533\\u20131544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anpology/D13-1160.\\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-ing&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159, 2020. URL https://arxiv.org/abs/2004.07159.\\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 1870\\u20131879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anpology/P17-1171.\\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonapan Berant. Coarse-to-fine question answering for long documents. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 209\\u2013220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anpology/P17-1020.\\n---\\nChristopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Comprehension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723. arXiv: 1710.10723.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of pe 2019 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anpology/N19-1423.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\\nMatpew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A New Q&A Dataset Augmented wip Context from a Search Engine. arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv: 1704.05179.\\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of pe 56p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 889\\u2013898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anpology/P18-1082.\\nAngela Fan, Yacine Jernite, Epan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of pe 57p Annual Meeting of pe Association for Computational Linguistics, pages 3558\\u20133567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/anpology/P19-1346.\\nAngela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers wip KNN-based composite memory, 2020. URL https://openreview.net/forum?id=H1gx1CNKPH.\\nThibault F\\u00e9vry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access wip entity supervision. ArXiv, abs/2004.07202, 2020. URL https://arxiv.org/abs/2004.07202.\\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wentau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710.\\nKatja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL http://arxiv.org/abs/1705.08807.\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pages 5133\\u20135140. AAAI press, 2018. 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018.\\nKelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of pe Association for Computational Linguistics, 6:437\\u2013450, 2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anpology/Q18-1031.\\n---\\n## References\\n\\n[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https://arxiv.org/abs/2002.08909.\\n[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 10052\\u201310062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf.\\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-edit-rerank text generation. In Proceedings of pe 58p Annual Meeting of pe Association for Computational Linguistics, pages 2532\\u20132538, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/anpology/2020.acl-main.228.\\n[23] Jeff Johnson, Matpijs Douze, and Herv\\u00e9 J\\u00e9gou. Billion-scale similarity search wip gpus. arXiv preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.\\n[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\\u20131611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anpology/P17-1147.\\n[25] Armand Joulin and Tomas Mikolov. Inferring algoripmic patterns wip stack-augmented recurrent nets. In Proceedings of pe 28p International Conference on Neural Information Processing Systems - Volume 1, NIPS\\u201915, page 190\\u2013198, Cambridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/5857-inferring-algoripmic-patterns-wip-stack-augmented-recurrent-nets.\\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization prough memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A mepod for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\\n[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matpew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. Transactions of pe Association of Computational Linguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf.\\n[30] Guillaume Lample, Alexandre Sablayrolles, Marc\\u2019 Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers wip product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\u2019 Alch\\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8548\\u20138559. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9061-large-memory-layers-wip-product-keys.pdf.\\n[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of pe 57p Annual Meeting of pe Association\\n---\\n## References\\n\\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of pe 2016 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, pages 110\\u2013119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anpology/N16-1014.\\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation wip optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL https://arxiv.org/abs/1909.03087.\\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation wip joint textual and phonetic embedding. In Proceedings of pe 57p Annual Meeting of pe Association for Computational Linguistics, pages 3044\\u20133049, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL https://www.aclweb.org/anpology/P19-1291.\\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hyg0vbWC-.\\n[37] Yury A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824\\u2013836, 2016. URL https://arxiv.org/abs/1603.09320.\\n[38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rockt\\u00e4schel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect pe verifiability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https://arxiv.org/abs/1911.03587.\\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\n[41] Nikita Moghe, Siddharpa Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploiting background knowledge for building conversation systems. In Proceedings of pe 2018 Conference on Empirical Mepods in Natural Language Processing, pages 2322\\u20132332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URL https://www.aclweb.org/anpology/D18-1255.\\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation systems. In Proceedings of pe 2018 Conference on Empirical Mepods in Natural Language Processing, pages 3950\\u20133959, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/anpology/D18-1429.\\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d\\u2019Avila Garcez, and Greg Wayne, editors, Proceedings of pe Workshop on Cognitive Computation: Integrating neural and symbolic.\\n---\\napproaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf.\\n\\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\\n\\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48\\u201353, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.org/anthology/N19-4009.\\n\\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402\\u20132411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\\n\\n[47] Fabio Petroni, Tim Rockt\\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\\u20132473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://www.aclweb.org/anthology/D19-1250.\\n\\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\\u00e4schel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models\\u2019 factual predictions. In Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum?id=025X0zPfn.\\n\\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training, 2018. URL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf.\\n\\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\\n\\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\\n\\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/2002.08910.\\n\\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333\\u2013389, April 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL https://doi.org/10.1561/1500000019.\\n\\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019.\\n\\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440\\u20132448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\\n---\\n# References\\n\\n[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of pe 2018 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809\\u2013819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://www.aclweb.org/anpology/N18-1074.\\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classification wip elastic weight consolidation. ArXiv, abs/2004.14366, 2020. URL https://arxiv.org/abs/2004.14366.\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanapan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998\\u20136008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaap Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329.\\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of pe 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\\u2013355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anpology/W18-5446.\\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3261\\u20133275. Curran Associates, Inc., 2019. URL https://arxiv.org/abs/1905.00537.\\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain question answering. In Sheila A. McIlraip and Kilian Q. Weinberger, editors, Proceedings of pe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), pe 30p innovative Applications of Artificial Intelligence (IAAI-18), and pe 8p AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5981\\u20135988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712.\\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in open-domain question answering. In ICLR, 2018. URL https://openreview.net/forum?id=rJl3yM-Ab.\\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.\\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and refine: Improved sequence generation models for dialogue. In Proceedings of pe 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 87\\u201392, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anpology/W18-5713.\\n---\\n## References\\n\\n[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anpony Moi, Pierric Cistac, Tim Rault, R\\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface\\u2019s transformers: State-of-pe-art natural language processing. ArXiv, abs/1910.03771, 2019.\\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-supervised question answering. In Proceedings of pe 2019 Conference on Empirical Mepods in Natural Language Processing and pe 9p International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2495\\u20132509, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anpology/D19-1253.\\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019. URL https://arxiv.org/abs/1909.03745.\\n---\\n## Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\n### Implementation Details\\n\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not find beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\n\\n### Human Evaluation\\n\\n|Which sentence is more factually true?|Select option|\\n|---|---|\\n|Noje: Scna Guesucn?|snterzt \\\"The8r Nuso Rists|\\n|IncicateFich|Farcncllic AM; on Im|\\n|Iclbwng sentarces Is Mca luclualy Injb[ealecllo|ZLbko Uaino cnoc urdoco|\\n\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \\\"view tool guide\\\".\\n\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instructions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.\\n\\n### Training Setup Details\\n\\nWe train all RAG models and BART baselines using Fairseq. We train with mixed precision floating point arithmetic, distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We find that doing Maximum Inner Product Search with FAISS is sufficiently fast on CPU, so we store document index vectors on CPU, requiring approximately 100 GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers, which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS\\u2019s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/\\n\\n2. https://github.com/pytorch/fairseq\\n\\n3. https://github.com/huggingface/transformers\\n---\\n## Further Details on Open-Domain QA\\n\\nFor open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to find matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we filter out answer candidates if they do not occur in top 1000 documents for the query.\\n\\n## CuratedTrec preprocessing\\n\\nThe answers for CuratedTrec are given in the form of regular expressions, which has been suggested as a reason why it is unsuitable for answer-generation models [20]. To overcome this, we use a pre-processing step where we first retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\n\\n## TriviaQA Evaluation setups\\n\\nThe open-domain QA community customarily uses public development datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading comprehension purposes. We report our results using the datasets splits used in DPR [26], which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA official Wikipedia test set instead. F\\u00e9vry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See appendix of [14]). We report results on both test sets to enable fair comparison to both approaches. We find that our performance is much higher using the official Wiki test set, rather than the more conventional open-domain test set, which we attribute to the official Wiki test set questions being simpler to answer from Wikipedia.\\n\\n## Further Details on FEVER\\n\\nFor FEVER classification, we follow the practice from [32], and first re-generate the claim, and then classify using the representation of the final hidden state, before finally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The first is to classify the claim as either \\\"Supported\\\", \\\"Refuted\\\" or \\\"Not Enough Info\\\", which is the task we explore in the main paper. FEVER\\u2019s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classification prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work.\\n\\n## Null Document Probabilities\\n\\nWe experimented with adding \\\"Null document\\\" mechanism to RAG, similar to REALM [20] in order to model cases where no useful information could be retrieved for a given input. Here, if k documents were retrieved, we would additionally \\\"retrieve\\\" an empty document and predict a logit for the null document, before marginalizing over k + 1 predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not find that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to benefit from retrieval, suggesting that null document mechanisms may not be necessary for RAG.\\n\\n## Parameters\\n\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable.\\n---\\n|Task|Train|Development|Test|\\n|---|---|---|---|\\n|Natural Questions|79169|8758|3611|\\n|TriviaQA|78786|8838|11314|\\n|WebQuestions|3418|362|2033|\\n|CuratedTrec|635|134|635|\\n|Jeopardy Question Generation|97392|13714|26849|\\n|MS-MARCO|153726|12468|101093*|\\n|FEVER-3-way|145450|10000|10000|\\n|FEVER-2-way|96966|6666|6666|\\n\\nparameters. The best performing \\\"closed-book\\\" (parametric only) open-domain QA model is T5-11B\\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit floating\\npoint precision to manage memory and disk footprints.\\n\\n## Retrieval Collapse\\n\\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the\\nretrieval component would \\u201ccollapse\\u201d and learn to retrieve the same documents regardless of the\\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\\nin less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results\\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\\n\\n## Number of instances per dataset\\n\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\",\n    \"paper\": \"## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\nPatrick Lewis\\u2020\\u2021, Ethan Perez\\u22c6, Aleksandra Piktus\\u2020, Fabio Petroni\\u2020, Vladimir Karpukhin\\u2020, Naman Goyal\\u2020, Heinrich K\\u00fcttler\\u2020\\n\\narXiv:2005.11401v4 [cs.CL] 12 Apr 2021\\n\\nMike Lewis\\u2020, Wen-tau Yih\\u2020, Tim Rockt\\u00e4schel\\u2020\\u2021, Sebastian Riedel\\u2020\\u2021, Douwe Kiela\\u2020\\n\\n\\u2020Facebook AI Research; \\u2021University College London; \\u22c6New York University;\\n\\nplewis@fb.com\\n\\n### Abstract\\n\\nLarge pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \\u2014 models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\\n\\n### Introduction\\n\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data [47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52]. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can\\u2019t straightforwardly provide insight into their predictions, and may produce \\u201challucinations\\u201d [38]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results.\\n---\\n## Define \\\"middle ear\\\"(x)\\n\\nThe middle ear includes the tympanic cavity and the three ossicles.\\n\\n## Question Answering:\\n\\n|Question Query|Query|Retriever p|\\u03b7|Document|Generator p|\\u03b8|\\n|---|---|---|---|---|---|---|\\n|Barack Obama was born in Hawaii.(x)|Encoder|(Non-Parametric)| |Index|(Parametric)|Answer Generation supports (y)|\\n\\n## Fact Verification: Fact Query\\n\\nThe Divine Comedy (x)\\n\\n## Jeopardy Question Generation:\\n\\nAnswer Query\\n\\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to find the top-K documents zi. For final prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents. but have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the \\u201cworkhorse of NLP,\\u201d i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned. There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stack-augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training. Our results highlight the benefits of combining parametric and non-parametric memory with generation for knowledge-intensive tasks\\u2014tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER [56] fact verification, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models\\u2019 knowledge as the world changes.\\n\\n## Methods\\n\\nWe explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever p\\u03b7(z|x) with parameters \\u03b7 that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator p\\u03b8(yi|x, z, y1:i\\u22121) parametrized\\n\\n1 Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transformers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n---\\n## by \\u03b8 that generates a current token based on a context of the previous i \\u2212 1 tokens y1:i\\u22121, the original input x and a retrieved passage z.\\n\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the p\\u03b7 and p\\u03b8 components, as well as the training and decoding procedure.\\n\\n### Models\\n\\n|RAG-Sequence Model|The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,|\\n|---|---|\\n| |pRAG-Sequence(y|x) \\u2248 p\\u03b7(z|x)p\\u03b8(y|x, z) = p\\u03b7(z|x) \\u03a3 p\\u03b8(yi|x, z, y1:i\\u22121) z\\u2208top-k(p(\\u00b7|x)) z\\u2208top-k(p(\\u00b7|x)) i|\\n|RAG-Token Model|In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we define:|\\n| |pRAG-Token(y|x) \\u2248 \\u03a3 z\\u2208top-k(p(\\u00b7|x)) p\\u03b7(z|x)p\\u03b8(yi|x, z, y1:i\\u22121)|\\n\\nFinally, we note that RAG can be used for sequence classification tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n\\n### Retriever: DPR\\n\\nThe retrieval component p\\u03b7(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: p\\u03b7(z|x) \\u221d exp d(z)\\u22a4q(x) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(p\\u03b7(\\u00b7|x)), the list of k documents z with highest prior probability p\\u03b7(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory.\\n\\n### Generator: BART\\n\\nThe generator component p\\u03b8(yi|x, z, y1:i\\u22121) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters \\u03b8 as the parametric memory henceforth.\\n\\n### Training\\n\\nWe jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a fine-tuning training corpus of input/output pairs (xj, yj), we\\n---\\nminimize the negative marginal log-likelihood of each target, j \\u2212log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder BERTq and the BART generator.\\n\\n## Decoding\\n\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate arg max y p(y|x).\\n\\n|RAG-Token|The RAG-Token model can be seen as a standard, autoregressive seq2seq generator with transition probability: p\\u2032 \\u03b8(yi|x, y1:i\\u22121) = z\\u2208top-k(p(\\u00b7|x)) p\\u03b7(zi|x)p\\u03b8(yi|x, zi, y1:i\\u22121) To decode, we can plug p\\u2032 \\u03b8(yi|x, y1:i\\u22121) into a standard beam decoder.|\\n|---|---|\\n|RAG-Sequence|For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using p\\u03b8(yi|x, z, y1:i\\u22121). This yields a set of hypotheses Y, some of which may not have appeared in the beams of all documents. To estimate the probability of a hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with p\\u03b7(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as \\u201cThorough Decoding.\\u201d For longer output sequences, |Y| can become large, requiring many forward passes. For more efficient decoding, we can make a further approximation that p\\u03b8(y|x, zi) \\u2248 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as \\u201cFast Decoding.\\u201d|\\n\\n## Experiments\\n\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k \\u2208 {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task.\\n\\n### Open-domain Question Answering\\n\\nOpen-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to \\u201cClosed-Book QA\\u201d approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\\n\\n### Abstractive Question Answering\\n\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG\\u2019s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n---\\nMSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as \\\"What is the weather in Volcano, CA?\\\" so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses.\\n\\nJeopardy Question Generation\\n\\nTo evaluate RAG\\u2019s generation abilities in a non-QA setting, we study open-domain question generation. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, \\\"The World Cup\\\" is the answer to the question \\\"In 1986 Mexico scored as the first country to host this international sports competition twice.\\\" As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task.\\n\\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for specificity. We define factuality as whether a statement can be corroborated by trusted external sources, and specificity as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options\\u2014question A is better, question B is better, both are good, or neither is good.\\n\\nFact Verification\\n\\nFEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unverifiable from Wikipedia alone. FEVER is a retrieval problem coupled with a challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models\\u2019 ability to handle classification rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren\\u2019t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n\\nResults\\n\\nOpen-domain Question Answering\\n\\n|Task|RAG|State-of-the-Art Models|\\n|---|---|---|\\n|All four open-domain QA tasks|RAG sets a new state of the art (only on the T5-comparable split for TQA)|RAG combines the generation flexibility of the \\u201cclosed-book\\u201d (parametric only) approaches and the performance of \\\"open-book\\\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized \\u201csalient span masking\\u201d pre-training [20]. It is worth noting that RAG\\u2019s retriever is initialized using DPR\\u2019s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based \\u201ccross-encoder\\u201d to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance.|\\n\\nThere are several advantages to generating answers even when it is possible to extract them. Documents with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading\\n---\\n## Table 1: Open-Domain QA Test Scores\\n\\n|Model|NQ|TQA|WQ|CT|\\n|---|---|---|---|---|\\n|Closed Book T5-11B [52]|34.5|- /50.1|37.4|-|\\n|Book T5-11B+SSM[52]|36.6|- /60.5|44.7|-|\\n|Open REALM [20]|40.4|- / -|40.7|46.8|\\n|Book DPR [26]|41.5|57.9/ -|41.1|50.6|\\n|RAG-Token|44.1|55.2/66.1|45.5|50.0|\\n|RAG-Seq.|44.5|56.8/68.0|45.2|52.2|\\n\\n## Table 2: Generation and classification Test Scores\\n\\n|Model|Jeopardy|MSMARCO|FVR3|FVR2|\\n|---|---|---|---|---|\\n|B-1|QB-1|R-L|B-1|Label Acc.|\\n|BART|15.1|19.7|38.2|41.6|\\n|RAG-Tok.|17.3|22.2|40.1|41.5|\\n|RAG-Seq.|14.7|21.4|40.8|44.2|\\n\\n4.2 Abstractive Question Answering\\n\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with specific information required to generate the reference answer, (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we find that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see \\u00a74.5).\\n\\n4.3 Jeopardy Question Generation\\n\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also find RAG generations to be more specific by a large margin. Table 3 shows typical generations from each model.\\n\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating \\u201cSun\\u201d, the posterior is high for document 2 which mentions \\u201cThe Sun Also Rises\\u201d. Similarly, document 1 dominates the posterior when \\u201cA Farewell to Arms\\u201d is generated. Intriguingly, after the first token of each book is generated, the document posterior flattens. This observation suggests that the generator can complete the titles without depending on specific documents. In other words, the model\\u2019s parametric knowledge is sufficient to complete the titles. We find evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \\\"The Sun. BART completes the generation \\\"The Sun Also Rises\\\" is a novel by this author of \\\"The Sun Also Rises\\\" indicating the title \\\"The Sun Also Rises\\\" is stored in BART\\u2019s parameters. Similarly, BART will complete the partial decoding \\\"The Sun Also Rises\\\" is a novel by this author of \\\"A with \\\"The Sun Also Rises\\\" is a novel by this author of \\\"A Farewell to Arms\\\". This example shows how parametric and non-parametric memories work together\\u2014the non-parametric component helps to guide the generation, drawing out specific knowledge stored in the parametric memory.\\n\\n4.4 Fact Verification\\n\\nTable 2 shows our results on FEVER. For 3-way classification, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-specific architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n---\\n## Document 1\\n\\nhis works are considered classics of American literature ... His wartime experiences formed the basis for his novel \\\"A Farewell to Arms\\\" (1929) ...\\n\\n## Document 2\\n\\n... artists of the 1920s \\\"Lost Generation\\\" expatriate community. His debut novel, \\\"The Sun Also Rises\\\", was published in 1926.\\n\\nFigure 2: RAG-Token document posterior p(zi|x, yi, y\\u2212i) for each generated token for input \\u201cHemingway\\\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating \\u201cA Farewell to Arms\\\" and for document 2 when generating \\u201cThe Sun Also Rises\\\".\\n\\n|Task|Input|Model|Generation|\\n|---|---|---|---|\\n|define middle ear|BART|?|The middle ear is the part of the ear between the middle ear and the nose.|\\n| |RAG-T| |The middle ear is the portion of the ear internal to the eardrum.|\\n|MS-MARCO|what currency needed in Scotland|BART|The currency needed in Scotland is Pound sterling.|\\n| |RAG-T| |Pound is the currency needed in Scotland.|\\n| |RAG-S| |The currency needed in Scotland is the pound sterling.|\\n| |BART|?|This state has the largest number of counties in the U.S.|\\n|Jeopardy Question Generation|Washington|RAG-T|It\\u2019s the only U.S. state named for a U.S. president|\\n| |RAG-S| |It\\u2019s the state where you\\u2019ll find Mount Rainier National Park|\\n| |BART|*|This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio|\\n|The Divine Comedy|RAG-T| |Dante\\u2019s \\\"Inferno\\\" is the first part of this epic poem|\\n| |RAG-S| |This 14th century work is divided into 3 sections: \\\"Inferno\\\", \\\"Purgatorio\\\" & \\\"Paradiso\\\"|\\n\\nFor 2-way classification, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved by RAG and gold evidence annotations. We find that the top retrieved document is from a gold article in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\\n\\n## Additional Results\\n\\nGeneration Diversity: Section 4.3 shows that RAG models are more factual and specific than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding, we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence\\u2019s generations are more diverse than RAG-Token\\u2019s, and both are significantly more diverse than BART without needing any diversity-promoting decoding.\\n\\nRetrieval Ablations: A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks.\\n\\nWe compare RAG\\u2019s dense retriever to a word overlap-based BM25 retriever. Here, we replace RAG\\u2019s retriever with a fixed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\n\\nIndex hot-swapping: An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n---\\n|Model|NQ|TQA|WQ|CT|Jeopardy-QGen|MSMarco|FVR-3|FVR-2|\\n|---|---|---|---|---|---|---|---|---|\\n|RAG-Token-BM25|29.7|41.5|32.1|33.1|17.5|22.3|55.5|48.4|75.1|91.6|\\n|RAG-Sequence-BM25|31.8|44.1|36.6|33.8|11.1|19.5|56.5|46.9|\\n|RAG-Token-Frozen|37.8|50.1|37.1|51.1|16.7|21.7|55.9|49.4|72.9|89.4|\\n|RAG-Sequence-Frozen|41.2|52.1|41.8|52.6|11.8|19.6|56.7|47.3|\\n|RAG-Token|43.5|54.8|46.5|51.9|17.9|22.6|56.2|49.4|74.5|90.6|\\n|RAG-Sequence|44.0|55.8|44.9|53.4|15.3|21.5|57.2|47.5|\\n\\n|Content|Page Number|\\n|---|---|\\n|Table 4: Human assessments for the Jeopardy Question Generation Task.| |\\n|Table 5: Ratio of distinct to total tri-grams for generation tasks.| |\\n|Table 6: Ablations on the dev set. As FEVER is a classification task, both RAG models are equivalent.| |\\n\\nBetween these dates and use a template \\u201cWho is {position}?\\u201d (e.g. \\u201cWho is the President of Peru?\\u201d) to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for 2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders). This shows we can update RAG\\u2019s world knowledge by simply replacing its non-parametric memory.\\n\\nEffect of Retrieving more documents: Models are trained with either 5 or 10 retrieved latent documents, and we do not observe significant differences in performance between them. We have the flexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n\\n| |NQ Answer Recall @ K|\\n|---|---|\\n|NQ Exact Match|80|\\n\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n\\nRelated Work: Single-Task Retrieval - Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering, fact checking, fact completion, long-form question answering, Wikipedia article generation, dialogue, translation, and language modeling. Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.\\n---\\n## General-Purpose Architectures for NLP\\n\\nPrior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks [60, 61] after fine-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models.\\n\\n## Learned Retrieval\\n\\nThere is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.\\n\\n## Memory-based Architectures\\n\\nOur document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model\\u2019s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval [9].\\n\\n## Retrieve-and-Edit approaches\\n\\nOur method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a final output. These approaches have proved successful in a number of domains including Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.\\n\\n## Discussion\\n\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG\\u2019s generation over purely parametric BART, finding RAG more factual and specific. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.\\n---\\n## Broader Impact\\n\\nThis work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it \\u201challucinate\\u201d less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.\\n\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [54]. Advanced language models may also lead to the automation of various jobs in the coming decades [16]. In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing.\\n\\n## Acknowledgments\\n\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.\\n\\n## References\\n\\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http://arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n[2] Petr Baudi\\u0161 and Jan \\u0160ediv` y. Modeling of pe question answering task in pe yodaqa system. In International Conference of pe Cross-Language Evaluation Forum for European Languages, pages 222\\u2013228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20.\\n[3] Jonapan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of pe 2013 Conference on Empirical Mepods in Natural Language Processing, pages 1533\\u20131544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anpology/D13-1160.\\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-ing&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159, 2020. URL https://arxiv.org/abs/2004.07159.\\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 1870\\u20131879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anpology/P17-1171.\\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonapan Berant. Coarse-to-fine question answering for long documents. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 209\\u2013220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anpology/P17-1020.\\n---\\nChristopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Comprehension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723. arXiv: 1710.10723.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of pe 2019 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anpology/N19-1423.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\\nMatpew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A New Q&A Dataset Augmented wip Context from a Search Engine. arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv: 1704.05179.\\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of pe 56p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 889\\u2013898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anpology/P18-1082.\\nAngela Fan, Yacine Jernite, Epan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of pe 57p Annual Meeting of pe Association for Computational Linguistics, pages 3558\\u20133567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/anpology/P19-1346.\\nAngela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers wip KNN-based composite memory, 2020. URL https://openreview.net/forum?id=H1gx1CNKPH.\\nThibault F\\u00e9vry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access wip entity supervision. ArXiv, abs/2004.07202, 2020. URL https://arxiv.org/abs/2004.07202.\\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wentau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710.\\nKatja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL http://arxiv.org/abs/1705.08807.\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pages 5133\\u20135140. AAAI press, 2018. 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018.\\nKelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of pe Association for Computational Linguistics, 6:437\\u2013450, 2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anpology/Q18-1031.\\n---\\n## References\\n\\n[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https://arxiv.org/abs/2002.08909.\\n[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 10052\\u201310062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf.\\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-edit-rerank text generation. In Proceedings of pe 58p Annual Meeting of pe Association for Computational Linguistics, pages 2532\\u20132538, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/anpology/2020.acl-main.228.\\n[23] Jeff Johnson, Matpijs Douze, and Herv\\u00e9 J\\u00e9gou. Billion-scale similarity search wip gpus. arXiv preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.\\n[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\\u20131611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anpology/P17-1147.\\n[25] Armand Joulin and Tomas Mikolov. Inferring algoripmic patterns wip stack-augmented recurrent nets. In Proceedings of pe 28p International Conference on Neural Information Processing Systems - Volume 1, NIPS\\u201915, page 190\\u2013198, Cambridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/5857-inferring-algoripmic-patterns-wip-stack-augmented-recurrent-nets.\\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization prough memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A mepod for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\\n[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matpew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. Transactions of pe Association of Computational Linguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf.\\n[30] Guillaume Lample, Alexandre Sablayrolles, Marc\\u2019 Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers wip product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\u2019 Alch\\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8548\\u20138559. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9061-large-memory-layers-wip-product-keys.pdf.\\n[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of pe 57p Annual Meeting of pe Association\\n---\\n## References\\n\\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of pe 2016 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, pages 110\\u2013119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anpology/N16-1014.\\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation wip optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL https://arxiv.org/abs/1909.03087.\\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation wip joint textual and phonetic embedding. In Proceedings of pe 57p Annual Meeting of pe Association for Computational Linguistics, pages 3044\\u20133049, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL https://www.aclweb.org/anpology/P19-1291.\\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hyg0vbWC-.\\n[37] Yury A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824\\u2013836, 2016. URL https://arxiv.org/abs/1603.09320.\\n[38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rockt\\u00e4schel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect pe verifiability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https://arxiv.org/abs/1911.03587.\\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\n[41] Nikita Moghe, Siddharpa Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploiting background knowledge for building conversation systems. In Proceedings of pe 2018 Conference on Empirical Mepods in Natural Language Processing, pages 2322\\u20132332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URL https://www.aclweb.org/anpology/D18-1255.\\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation systems. In Proceedings of pe 2018 Conference on Empirical Mepods in Natural Language Processing, pages 3950\\u20133959, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/anpology/D18-1429.\\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d\\u2019Avila Garcez, and Greg Wayne, editors, Proceedings of pe Workshop on Cognitive Computation: Integrating neural and symbolic.\\n---\\napproaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf.\\n\\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\\n\\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48\\u201353, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.org/anthology/N19-4009.\\n\\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402\\u20132411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\\n\\n[47] Fabio Petroni, Tim Rockt\\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\\u20132473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://www.aclweb.org/anthology/D19-1250.\\n\\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\\u00e4schel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models\\u2019 factual predictions. In Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum?id=025X0zPfn.\\n\\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training, 2018. URL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf.\\n\\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\\n\\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\\n\\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/2002.08910.\\n\\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333\\u2013389, April 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL https://doi.org/10.1561/1500000019.\\n\\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019.\\n\\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440\\u20132448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\\n---\\n# References\\n\\n[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of pe 2018 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809\\u2013819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://www.aclweb.org/anpology/N18-1074.\\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classification wip elastic weight consolidation. ArXiv, abs/2004.14366, 2020. URL https://arxiv.org/abs/2004.14366.\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanapan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998\\u20136008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaap Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329.\\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of pe 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\\u2013355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anpology/W18-5446.\\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3261\\u20133275. Curran Associates, Inc., 2019. URL https://arxiv.org/abs/1905.00537.\\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain question answering. In Sheila A. McIlraip and Kilian Q. Weinberger, editors, Proceedings of pe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), pe 30p innovative Applications of Artificial Intelligence (IAAI-18), and pe 8p AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5981\\u20135988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712.\\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in open-domain question answering. In ICLR, 2018. URL https://openreview.net/forum?id=rJl3yM-Ab.\\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.\\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and refine: Improved sequence generation models for dialogue. In Proceedings of pe 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 87\\u201392, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anpology/W18-5713.\\n---\\n## References\\n\\n[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anpony Moi, Pierric Cistac, Tim Rault, R\\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface\\u2019s transformers: State-of-pe-art natural language processing. ArXiv, abs/1910.03771, 2019.\\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-supervised question answering. In Proceedings of pe 2019 Conference on Empirical Mepods in Natural Language Processing and pe 9p International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2495\\u20132509, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anpology/D19-1253.\\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019. URL https://arxiv.org/abs/1909.03745.\\n---\\n## Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\n### Implementation Details\\n\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not find beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\n\\n### Human Evaluation\\n\\n|Which sentence is more factually true?|Select option|\\n|---|---|\\n|Noje: Scna Guesucn?|snterzt \\\"The8r Nuso Rists|\\n|IncicateFich|Farcncllic AM; on Im|\\n|Iclbwng sentarces Is Mca luclualy Injb[ealecllo|ZLbko Uaino cnoc urdoco|\\n\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \\\"view tool guide\\\".\\n\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instructions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.\\n\\n### Training Setup Details\\n\\nWe train all RAG models and BART baselines using Fairseq. We train with mixed precision floating point arithmetic, distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We find that doing Maximum Inner Product Search with FAISS is sufficiently fast on CPU, so we store document index vectors on CPU, requiring approximately 100 GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers, which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS\\u2019s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/\\n\\n2. https://github.com/pytorch/fairseq\\n\\n3. https://github.com/huggingface/transformers\\n---\\n## Further Details on Open-Domain QA\\n\\nFor open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to find matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we filter out answer candidates if they do not occur in top 1000 documents for the query.\\n\\n## CuratedTrec preprocessing\\n\\nThe answers for CuratedTrec are given in the form of regular expressions, which has been suggested as a reason why it is unsuitable for answer-generation models [20]. To overcome this, we use a pre-processing step where we first retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\n\\n## TriviaQA Evaluation setups\\n\\nThe open-domain QA community customarily uses public development datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading comprehension purposes. We report our results using the datasets splits used in DPR [26], which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA official Wikipedia test set instead. F\\u00e9vry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See appendix of [14]). We report results on both test sets to enable fair comparison to both approaches. We find that our performance is much higher using the official Wiki test set, rather than the more conventional open-domain test set, which we attribute to the official Wiki test set questions being simpler to answer from Wikipedia.\\n\\n## Further Details on FEVER\\n\\nFor FEVER classification, we follow the practice from [32], and first re-generate the claim, and then classify using the representation of the final hidden state, before finally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The first is to classify the claim as either \\\"Supported\\\", \\\"Refuted\\\" or \\\"Not Enough Info\\\", which is the task we explore in the main paper. FEVER\\u2019s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classification prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work.\\n\\n## Null Document Probabilities\\n\\nWe experimented with adding \\\"Null document\\\" mechanism to RAG, similar to REALM [20] in order to model cases where no useful information could be retrieved for a given input. Here, if k documents were retrieved, we would additionally \\\"retrieve\\\" an empty document and predict a logit for the null document, before marginalizing over k + 1 predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not find that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to benefit from retrieval, suggesting that null document mechanisms may not be necessary for RAG.\\n\\n## Parameters\\n\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable.\\n---\\n|Task|Train|Development|Test|\\n|---|---|---|---|\\n|Natural Questions|79169|8758|3611|\\n|TriviaQA|78786|8838|11314|\\n|WebQuestions|3418|362|2033|\\n|CuratedTrec|635|134|635|\\n|Jeopardy Question Generation|97392|13714|26849|\\n|MS-MARCO|153726|12468|101093*|\\n|FEVER-3-way|145450|10000|10000|\\n|FEVER-2-way|96966|6666|6666|\\n\\nparameters. The best performing \\\"closed-book\\\" (parametric only) open-domain QA model is T5-11B\\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit floating\\npoint precision to manage memory and disk footprints.\\n\\n## Retrieval Collapse\\n\\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the\\nretrieval component would \\u201ccollapse\\u201d and learn to retrieve the same documents regardless of the\\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\\nin less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results\\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\\n\\n## Number of instances per dataset\\n\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\"\n}\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/a0427540-f7d2-44b8-af25-dc3e9b79a166"},{"cell_type":"markdown","metadata":{"is_collapsed":false,"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"133e41a62e904dd2b46508abcfd947ae","deepnote_cell_type":"text-cell-h3"},"source":"### OpenAI summary","block_group":"d1aeac78675841e2a325e4344648d91a"},{"cell_type":"code","metadata":{"source_hash":"d8524201","execution_start":1710286181686,"execution_millis":3678,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"9acb24a26620429f9947432265c0a73c","deepnote_cell_type":"code"},"source":"import openai\nimport aiopg\nfrom openai import AsyncOpenAI\n\nasync def query_info_with_gpt(paper_id, arxiv_paper_markdown, arxiv_metadata, user_query):\n\n    MAX_CONTEXT_LENGTH = 15500 \n\n    # Initial context setup and trimming\n    context = f\"Context: {arxiv_metadata}\\n{arxiv_paper_markdown}\"\n    while count_tokens(context) > MAX_CONTEXT_LENGTH:\n        char_to_token_ratio = len(context) / count_tokens(context)\n        max_char_length = int(MAX_CONTEXT_LENGTH * char_to_token_ratio)\n        context = context[:max_char_length]\n\n    # Constructing the prompt\n    prompt = (\n        f\"Summarize this in 100 characters based on the user query {user_query}: {context}\"\n    )\n\n    # Setting up OpenAI client\n    client = AsyncOpenAI(api_key=openai_api_key) # Replace 'your_openai_api_key' with your actual OpenAI API key\n\n    # Making an asynchronous API call\n    try:\n        response = await client.chat.completions.create(\n            messages=[\n                {\"role\": \"system\", \"content\": prompt}\n            ],\n            model=\"gpt-4-turbo-preview\"  # You can switch to other models if needed\n        )\n        answer = response.choices[0].message.content  # Extracting the response   answer = response.choices[0].message.content  # Extracting the response\n        DSN = (\n            f\"dbname={os.environ['MY_INTEGRATION_DATABASE']} \"\n            f\"user={os.environ['MY_INTEGRATION_USER']} \"\n            f\"password={os.environ['MY_INTEGRATION_PASSWORD']} \"\n            f\"host={os.environ['MY_INTEGRATION_HOST']} \"\n            f\"port={os.environ['MY_INTEGRATION_PORT']}\"\n        )\n        # Establish a new database connection\n        async with aiopg.create_pool(DSN) as pool:\n            async with pool.acquire() as conn:\n                async with conn.cursor() as cur:\n                    # Update the database with the answer\n                    await cur.execute(\n                        \"UPDATE Query_Papers SET relevant_answer = %s WHERE id = %s\",\n                        (answer, paper_id)\n                    )\n\n        return answer  # You might still return the answer for logging or other purposes\n    except openai.error.InvalidRequestError as e:  # Catching specific errors related to invalid requests, like NotFoundError\n        if e.code == 404:  # Checking if it's a NotFoundError\n            print(f\"Model not found or not accessible: {e}\")\n        else:\n            print(f\"Invalid request error: {e}\")\n    except openai.error.OpenAIError as e:  # Catching general OpenAI errors\n        print(f\"OpenAI API error: {e}\")\n    except Exception as e:  # Catching all other exceptions\n        print(f\"Unexpected error: {e}\")\n\n# Define the main operation\nasync def test():\n    # Connect to the SQLite database\n    conn = connection()\n    c = conn.cursor()\n\n    # Fetch data from the Papers table\n    c.execute(\"SELECT * FROM Papers WHERE arxiv_metadata IS NOT NULL LIMIT 1\")\n    row = c.fetchone()\n\n    # Check if data exists\n    if row:\n        _, _, _, _, _, _, _, arxiv_metadata, _, arxiv_paper_markdown, _, _ = row\n        user_query = \"Insert your query here\"  # Define the user query as needed\n\n        # Call the GPT function and handle response\n        response = await query_info_with_gpt(paper_id, arxiv_paper_markdown, arxiv_metadata, user_query)\n        print(\"LLM response:\", response)\n\n        # Here, insert the response into the database if needed, or handle it as necessary\n\n    else:\n        print(\"No data found in the Papers table.\")\n    if conn is not None:\n        conn.close()  # Ensure the connection is closed if it's not None\n\n# Example\nuser_query = \"Top academic papers on LLMs\" \npaper_id = 801\narxiv_paper_markdown = \"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.\",\narxiv_metadata = \"## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\nPatrick Lewis\\u2020\\u2021, Ethan Perez\\u22c6, Aleksandra Piktus\\u2020, Fabio Petroni\\u2020, Vladimir Karpukhin\\u2020, Naman Goyal\\u2020, Heinrich K\\u00fcttler\\u2020\\n\\narXiv:2005.11401v4 [cs.CL] 12 Apr 2021\\n\\nMike Lewis\\u2020, Wen-tau Yih\\u2020, Tim Rockt\\u00e4schel\\u2020\\u2021, Sebastian Riedel\\u2020\\u2021, Douwe Kiela\\u2020\\n\\n\\u2020Facebook AI Research; \\u2021University College London; \\u22c6New York University;\\n\\nplewis@fb.com\\n\\n### Abstract\\n\\nLarge pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \\u2014 models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\\n\\n### Introduction\\n\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data [47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52]. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can\\u2019t straightforwardly provide insight into their predictions, and may produce \\u201challucinations\\u201d [38]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results.\\n---\\n## Define \\\"middle ear\\\"(x)\\n\\nThe middle ear includes the tympanic cavity and the three ossicles.\\n\\n## Question Answering:\\n\\n|Question Query|Query|Retriever p|\\u03b7|Document|Generator p|\\u03b8|\\n|---|---|---|---|---|---|---|\\n|Barack Obama was born in Hawaii.(x)|Encoder|(Non-Parametric)| |Index|(Parametric)|Answer Generation supports (y)|\\n\\n## Fact Verification: Fact Query\\n\\nThe Divine Comedy (x)\\n\\n## Jeopardy Question Generation:\\n\\nAnswer Query\\n\\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to find the top-K documents zi. For final prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents. but have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the \\u201cworkhorse of NLP,\\u201d i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned. There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stack-augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training. Our results highlight the benefits of combining parametric and non-parametric memory with generation for knowledge-intensive tasks\\u2014tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER [56] fact verification, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models\\u2019 knowledge as the world changes.\\n\\n## Methods\\n\\nWe explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever p\\u03b7(z|x) with parameters \\u03b7 that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator p\\u03b8(yi|x, z, y1:i\\u22121) parametrized\\n\\n1 Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transformers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n---\\n## by \\u03b8 that generates a current token based on a context of the previous i \\u2212 1 tokens y1:i\\u22121, the original input x and a retrieved passage z.\\n\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the p\\u03b7 and p\\u03b8 components, as well as the training and decoding procedure.\\n\\n### Models\\n\\n|RAG-Sequence Model|The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,|\\n|---|---|\\n| |pRAG-Sequence(y|x) \\u2248 p\\u03b7(z|x)p\\u03b8(y|x, z) = p\\u03b7(z|x) \\u03a3 p\\u03b8(yi|x, z, y1:i\\u22121) z\\u2208top-k(p(\\u00b7|x)) z\\u2208top-k(p(\\u00b7|x)) i|\\n|RAG-Token Model|In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we define:|\\n| |pRAG-Token(y|x) \\u2248 \\u03a3 z\\u2208top-k(p(\\u00b7|x)) p\\u03b7(z|x)p\\u03b8(yi|x, z, y1:i\\u22121)|\\n\\nFinally, we note that RAG can be used for sequence classification tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n\\n### Retriever: DPR\\n\\nThe retrieval component p\\u03b7(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: p\\u03b7(z|x) \\u221d exp d(z)\\u22a4q(x) d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(p\\u03b7(\\u00b7|x)), the list of k documents z with highest prior probability p\\u03b7(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory.\\n\\n### Generator: BART\\n\\nThe generator component p\\u03b8(yi|x, z, y1:i\\u22121) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters \\u03b8 as the parametric memory henceforth.\\n\\n### Training\\n\\nWe jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a fine-tuning training corpus of input/output pairs (xj, yj), we\\n---\\nminimize the negative marginal log-likelihood of each target, j \\u2212log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder BERTq and the BART generator.\\n\\n## Decoding\\n\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate arg max y p(y|x).\\n\\n|RAG-Token|The RAG-Token model can be seen as a standard, autoregressive seq2seq generator with transition probability: p\\u2032 \\u03b8(yi|x, y1:i\\u22121) = z\\u2208top-k(p(\\u00b7|x)) p\\u03b7(zi|x)p\\u03b8(yi|x, zi, y1:i\\u22121) To decode, we can plug p\\u2032 \\u03b8(yi|x, y1:i\\u22121) into a standard beam decoder.|\\n|---|---|\\n|RAG-Sequence|For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using p\\u03b8(yi|x, z, y1:i\\u22121). This yields a set of hypotheses Y, some of which may not have appeared in the beams of all documents. To estimate the probability of a hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with p\\u03b7(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as \\u201cThorough Decoding.\\u201d For longer output sequences, |Y| can become large, requiring many forward passes. For more efficient decoding, we can make a further approximation that p\\u03b8(y|x, zi) \\u2248 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as \\u201cFast Decoding.\\u201d|\\n\\n## Experiments\\n\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k \\u2208 {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task.\\n\\n### Open-domain Question Answering\\n\\nOpen-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to \\u201cClosed-Book QA\\u201d approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\\n\\n### Abstractive Question Answering\\n\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG\\u2019s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n---\\nMSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as \\\"What is the weather in Volcano, CA?\\\" so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses.\\n\\nJeopardy Question Generation\\n\\nTo evaluate RAG\\u2019s generation abilities in a non-QA setting, we study open-domain question generation. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, \\\"The World Cup\\\" is the answer to the question \\\"In 1986 Mexico scored as the first country to host this international sports competition twice.\\\" As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task.\\n\\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for specificity. We define factuality as whether a statement can be corroborated by trusted external sources, and specificity as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options\\u2014question A is better, question B is better, both are good, or neither is good.\\n\\nFact Verification\\n\\nFEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unverifiable from Wikipedia alone. FEVER is a retrieval problem coupled with a challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models\\u2019 ability to handle classification rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren\\u2019t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n\\nResults\\n\\nOpen-domain Question Answering\\n\\n|Task|RAG|State-of-the-Art Models|\\n|---|---|---|\\n|All four open-domain QA tasks|RAG sets a new state of the art (only on the T5-comparable split for TQA)|RAG combines the generation flexibility of the \\u201cclosed-book\\u201d (parametric only) approaches and the performance of \\\"open-book\\\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized \\u201csalient span masking\\u201d pre-training [20]. It is worth noting that RAG\\u2019s retriever is initialized using DPR\\u2019s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based \\u201ccross-encoder\\u201d to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance.|\\n\\nThere are several advantages to generating answers even when it is possible to extract them. Documents with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading\\n---\\n## Table 1: Open-Domain QA Test Scores\\n\\n|Model|NQ|TQA|WQ|CT|\\n|---|---|---|---|---|\\n|Closed Book T5-11B [52]|34.5|- /50.1|37.4|-|\\n|Book T5-11B+SSM[52]|36.6|- /60.5|44.7|-|\\n|Open REALM [20]|40.4|- / -|40.7|46.8|\\n|Book DPR [26]|41.5|57.9/ -|41.1|50.6|\\n|RAG-Token|44.1|55.2/66.1|45.5|50.0|\\n|RAG-Seq.|44.5|56.8/68.0|45.2|52.2|\\n\\n## Table 2: Generation and classification Test Scores\\n\\n|Model|Jeopardy|MSMARCO|FVR3|FVR2|\\n|---|---|---|---|---|\\n|B-1|QB-1|R-L|B-1|Label Acc.|\\n|BART|15.1|19.7|38.2|41.6|\\n|RAG-Tok.|17.3|22.2|40.1|41.5|\\n|RAG-Seq.|14.7|21.4|40.8|44.2|\\n\\n4.2 Abstractive Question Answering\\n\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with specific information required to generate the reference answer, (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we find that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see \\u00a74.5).\\n\\n4.3 Jeopardy Question Generation\\n\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also find RAG generations to be more specific by a large margin. Table 3 shows typical generations from each model.\\n\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating \\u201cSun\\u201d, the posterior is high for document 2 which mentions \\u201cThe Sun Also Rises\\u201d. Similarly, document 1 dominates the posterior when \\u201cA Farewell to Arms\\u201d is generated. Intriguingly, after the first token of each book is generated, the document posterior flattens. This observation suggests that the generator can complete the titles without depending on specific documents. In other words, the model\\u2019s parametric knowledge is sufficient to complete the titles. We find evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \\\"The Sun. BART completes the generation \\\"The Sun Also Rises\\\" is a novel by this author of \\\"The Sun Also Rises\\\" indicating the title \\\"The Sun Also Rises\\\" is stored in BART\\u2019s parameters. Similarly, BART will complete the partial decoding \\\"The Sun Also Rises\\\" is a novel by this author of \\\"A with \\\"The Sun Also Rises\\\" is a novel by this author of \\\"A Farewell to Arms\\\". This example shows how parametric and non-parametric memories work together\\u2014the non-parametric component helps to guide the generation, drawing out specific knowledge stored in the parametric memory.\\n\\n4.4 Fact Verification\\n\\nTable 2 shows our results on FEVER. For 3-way classification, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-specific architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n---\\n## Document 1\\n\\nhis works are considered classics of American literature ... His wartime experiences formed the basis for his novel \\\"A Farewell to Arms\\\" (1929) ...\\n\\n## Document 2\\n\\n... artists of the 1920s \\\"Lost Generation\\\" expatriate community. His debut novel, \\\"The Sun Also Rises\\\", was published in 1926.\\n\\nFigure 2: RAG-Token document posterior p(zi|x, yi, y\\u2212i) for each generated token for input \\u201cHemingway\\\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating \\u201cA Farewell to Arms\\\" and for document 2 when generating \\u201cThe Sun Also Rises\\\".\\n\\n|Task|Input|Model|Generation|\\n|---|---|---|---|\\n|define middle ear|BART|?|The middle ear is the part of the ear between the middle ear and the nose.|\\n| |RAG-T| |The middle ear is the portion of the ear internal to the eardrum.|\\n|MS-MARCO|what currency needed in Scotland|BART|The currency needed in Scotland is Pound sterling.|\\n| |RAG-T| |Pound is the currency needed in Scotland.|\\n| |RAG-S| |The currency needed in Scotland is the pound sterling.|\\n| |BART|?|This state has the largest number of counties in the U.S.|\\n|Jeopardy Question Generation|Washington|RAG-T|It\\u2019s the only U.S. state named for a U.S. president|\\n| |RAG-S| |It\\u2019s the state where you\\u2019ll find Mount Rainier National Park|\\n| |BART|*|This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio|\\n|The Divine Comedy|RAG-T| |Dante\\u2019s \\\"Inferno\\\" is the first part of this epic poem|\\n| |RAG-S| |This 14th century work is divided into 3 sections: \\\"Inferno\\\", \\\"Purgatorio\\\" & \\\"Paradiso\\\"|\\n\\nFor 2-way classification, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved by RAG and gold evidence annotations. We find that the top retrieved document is from a gold article in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\\n\\n## Additional Results\\n\\nGeneration Diversity: Section 4.3 shows that RAG models are more factual and specific than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding, we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence\\u2019s generations are more diverse than RAG-Token\\u2019s, and both are significantly more diverse than BART without needing any diversity-promoting decoding.\\n\\nRetrieval Ablations: A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks.\\n\\nWe compare RAG\\u2019s dense retriever to a word overlap-based BM25 retriever. Here, we replace RAG\\u2019s retriever with a fixed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\n\\nIndex hot-swapping: An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n---\\n|Model|NQ|TQA|WQ|CT|Jeopardy-QGen|MSMarco|FVR-3|FVR-2|\\n|---|---|---|---|---|---|---|---|---|\\n|RAG-Token-BM25|29.7|41.5|32.1|33.1|17.5|22.3|55.5|48.4|75.1|91.6|\\n|RAG-Sequence-BM25|31.8|44.1|36.6|33.8|11.1|19.5|56.5|46.9|\\n|RAG-Token-Frozen|37.8|50.1|37.1|51.1|16.7|21.7|55.9|49.4|72.9|89.4|\\n|RAG-Sequence-Frozen|41.2|52.1|41.8|52.6|11.8|19.6|56.7|47.3|\\n|RAG-Token|43.5|54.8|46.5|51.9|17.9|22.6|56.2|49.4|74.5|90.6|\\n|RAG-Sequence|44.0|55.8|44.9|53.4|15.3|21.5|57.2|47.5|\\n\\n|Content|Page Number|\\n|---|---|\\n|Table 4: Human assessments for the Jeopardy Question Generation Task.| |\\n|Table 5: Ratio of distinct to total tri-grams for generation tasks.| |\\n|Table 6: Ablations on the dev set. As FEVER is a classification task, both RAG models are equivalent.| |\\n\\nBetween these dates and use a template \\u201cWho is {position}?\\u201d (e.g. \\u201cWho is the President of Peru?\\u201d) to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for 2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders). This shows we can update RAG\\u2019s world knowledge by simply replacing its non-parametric memory.\\n\\nEffect of Retrieving more documents: Models are trained with either 5 or 10 retrieved latent documents, and we do not observe significant differences in performance between them. We have the flexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n\\n| |NQ Answer Recall @ K|\\n|---|---|\\n|NQ Exact Match|80|\\n\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n\\nRelated Work: Single-Task Retrieval - Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering, fact checking, fact completion, long-form question answering, Wikipedia article generation, dialogue, translation, and language modeling. Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.\\n---\\n## General-Purpose Architectures for NLP\\n\\nPrior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks [60, 61] after fine-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models.\\n\\n## Learned Retrieval\\n\\nThere is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.\\n\\n## Memory-based Architectures\\n\\nOur document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model\\u2019s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval [9].\\n\\n## Retrieve-and-Edit approaches\\n\\nOur method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a final output. These approaches have proved successful in a number of domains including Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.\\n\\n## Discussion\\n\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG\\u2019s generation over purely parametric BART, finding RAG more factual and specific. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.\\n---\\n## Broader Impact\\n\\nThis work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it \\u201challucinate\\u201d less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.\\n\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [54]. Advanced language models may also lead to the automation of various jobs in the coming decades [16]. In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing.\\n\\n## Acknowledgments\\n\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.\\n\\n## References\\n\\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http://arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n[2] Petr Baudi\\u0161 and Jan \\u0160ediv` y. Modeling of pe question answering task in pe yodaqa system. In International Conference of pe Cross-Language Evaluation Forum for European Languages, pages 222\\u2013228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20.\\n[3] Jonapan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of pe 2013 Conference on Empirical Mepods in Natural Language Processing, pages 1533\\u20131544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anpology/D13-1160.\\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-ing&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159, 2020. URL https://arxiv.org/abs/2004.07159.\\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 1870\\u20131879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anpology/P17-1171.\\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonapan Berant. Coarse-to-fine question answering for long documents. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 209\\u2013220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anpology/P17-1020.\\n---\\nChristopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Comprehension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723. arXiv: 1710.10723.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of pe 2019 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anpology/N19-1423.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\\nMatpew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A New Q&A Dataset Augmented wip Context from a Search Engine. arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv: 1704.05179.\\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of pe 56p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 889\\u2013898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anpology/P18-1082.\\nAngela Fan, Yacine Jernite, Epan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of pe 57p Annual Meeting of pe Association for Computational Linguistics, pages 3558\\u20133567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/anpology/P19-1346.\\nAngela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers wip KNN-based composite memory, 2020. URL https://openreview.net/forum?id=H1gx1CNKPH.\\nThibault F\\u00e9vry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access wip entity supervision. ArXiv, abs/2004.07202, 2020. URL https://arxiv.org/abs/2004.07202.\\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wentau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710.\\nKatja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL http://arxiv.org/abs/1705.08807.\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, 32nd AAAI Conference on Artificial Intelligence, AAAI 2018, pages 5133\\u20135140. AAAI press, 2018. 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018.\\nKelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of pe Association for Computational Linguistics, 6:437\\u2013450, 2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anpology/Q18-1031.\\n---\\n## References\\n\\n[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https://arxiv.org/abs/2002.08909.\\n[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 10052\\u201310062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf.\\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-edit-rerank text generation. In Proceedings of pe 58p Annual Meeting of pe Association for Computational Linguistics, pages 2532\\u20132538, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/anpology/2020.acl-main.228.\\n[23] Jeff Johnson, Matpijs Douze, and Herv\\u00e9 J\\u00e9gou. Billion-scale similarity search wip gpus. arXiv preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.\\n[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of pe 55p Annual Meeting of pe Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\\u20131611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anpology/P17-1147.\\n[25] Armand Joulin and Tomas Mikolov. Inferring algoripmic patterns wip stack-augmented recurrent nets. In Proceedings of pe 28p International Conference on Neural Information Processing Systems - Volume 1, NIPS\\u201915, page 190\\u2013198, Cambridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/5857-inferring-algoripmic-patterns-wip-stack-augmented-recurrent-nets.\\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization prough memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A mepod for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\\n[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matpew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. Transactions of pe Association of Computational Linguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf.\\n[30] Guillaume Lample, Alexandre Sablayrolles, Marc\\u2019 Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers wip product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\u2019 Alch\\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8548\\u20138559. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9061-large-memory-layers-wip-product-keys.pdf.\\n[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of pe 57p Annual Meeting of pe Association\\n---\\n## References\\n\\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of pe 2016 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, pages 110\\u2013119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anpology/N16-1014.\\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation wip optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL https://arxiv.org/abs/1909.03087.\\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation wip joint textual and phonetic embedding. In Proceedings of pe 57p Annual Meeting of pe Association for Computational Linguistics, pages 3044\\u20133049, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL https://www.aclweb.org/anpology/P19-1291.\\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=Hyg0vbWC-.\\n[37] Yury A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824\\u2013836, 2016. URL https://arxiv.org/abs/1603.09320.\\n[38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rockt\\u00e4schel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect pe verifiability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https://arxiv.org/abs/1911.03587.\\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\n[41] Nikita Moghe, Siddharpa Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploiting background knowledge for building conversation systems. In Proceedings of pe 2018 Conference on Empirical Mepods in Natural Language Processing, pages 2322\\u20132332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URL https://www.aclweb.org/anpology/D18-1255.\\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation systems. In Proceedings of pe 2018 Conference on Empirical Mepods in Natural Language Processing, pages 3950\\u20133959, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/anpology/D18-1429.\\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d\\u2019Avila Garcez, and Greg Wayne, editors, Proceedings of pe Workshop on Cognitive Computation: Integrating neural and symbolic.\\n---\\napproaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf.\\n\\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\\n\\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48\\u201353, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.org/anthology/N19-4009.\\n\\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402\\u20132411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\\n\\n[47] Fabio Petroni, Tim Rockt\\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\\u20132473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://www.aclweb.org/anthology/D19-1250.\\n\\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\\u00e4schel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models\\u2019 factual predictions. In Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum?id=025X0zPfn.\\n\\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training, 2018. URL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf.\\n\\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\\n\\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\\n\\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/2002.08910.\\n\\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333\\u2013389, April 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL https://doi.org/10.1561/1500000019.\\n\\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019.\\n\\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440\\u20132448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\\n---\\n# References\\n\\n[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of pe 2018 Conference of pe Norp American Chapter of pe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809\\u2013819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://www.aclweb.org/anpology/N18-1074.\\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classification wip elastic weight consolidation. ArXiv, abs/2004.14366, 2020. URL https://arxiv.org/abs/2004.14366.\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanapan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998\\u20136008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaap Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. AAAI Conference on Artificial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329.\\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of pe 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\\u2013355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anpology/W18-5446.\\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3261\\u20133275. Curran Associates, Inc., 2019. URL https://arxiv.org/abs/1905.00537.\\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain question answering. In Sheila A. McIlraip and Kilian Q. Weinberger, editors, Proceedings of pe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), pe 30p innovative Applications of Artificial Intelligence (IAAI-18), and pe 8p AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5981\\u20135988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712.\\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in open-domain question answering. In ICLR, 2018. URL https://openreview.net/forum?id=rJl3yM-Ab.\\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.\\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and refine: Improved sequence generation models for dialogue. In Proceedings of pe 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 87\\u201392, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anpology/W18-5713.\\n---\\n## References\\n\\n[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anpony Moi, Pierric Cistac, Tim Rault, R\\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface\\u2019s transformers: State-of-pe-art natural language processing. ArXiv, abs/1910.03771, 2019.\\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-supervised question answering. In Proceedings of pe 2019 Conference on Empirical Mepods in Natural Language Processing and pe 9p International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2495\\u20132509, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anpology/D19-1253.\\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019. URL https://arxiv.org/abs/1909.03745.\\n---\\n## Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n\\n### Implementation Details\\n\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not find beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\n\\n### Human Evaluation\\n\\n|Which sentence is more factually true?|Select option|\\n|---|---|\\n|Noje: Scna Guesucn?|snterzt \\\"The8r Nuso Rists|\\n|IncicateFich|Farcncllic AM; on Im|\\n|Iclbwng sentarces Is Mca luclualy Injb[ealecllo|ZLbko Uaino cnoc urdoco|\\n\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \\\"view tool guide\\\".\\n\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instructions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.\\n\\n### Training Setup Details\\n\\nWe train all RAG models and BART baselines using Fairseq. We train with mixed precision floating point arithmetic, distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We find that doing Maximum Inner Product Search with FAISS is sufficiently fast on CPU, so we store document index vectors on CPU, requiring approximately 100 GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers, which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS\\u2019s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/\\n\\n2. https://github.com/pytorch/fairseq\\n\\n3. https://github.com/huggingface/transformers\\n---\\n## Further Details on Open-Domain QA\\n\\nFor open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to find matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we filter out answer candidates if they do not occur in top 1000 documents for the query.\\n\\n## CuratedTrec preprocessing\\n\\nThe answers for CuratedTrec are given in the form of regular expressions, which has been suggested as a reason why it is unsuitable for answer-generation models [20]. To overcome this, we use a pre-processing step where we first retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\n\\n## TriviaQA Evaluation setups\\n\\nThe open-domain QA community customarily uses public development datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading comprehension purposes. We report our results using the datasets splits used in DPR [26], which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA official Wikipedia test set instead. F\\u00e9vry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See appendix of [14]). We report results on both test sets to enable fair comparison to both approaches. We find that our performance is much higher using the official Wiki test set, rather than the more conventional open-domain test set, which we attribute to the official Wiki test set questions being simpler to answer from Wikipedia.\\n\\n## Further Details on FEVER\\n\\nFor FEVER classification, we follow the practice from [32], and first re-generate the claim, and then classify using the representation of the final hidden state, before finally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The first is to classify the claim as either \\\"Supported\\\", \\\"Refuted\\\" or \\\"Not Enough Info\\\", which is the task we explore in the main paper. FEVER\\u2019s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classification prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work.\\n\\n## Null Document Probabilities\\n\\nWe experimented with adding \\\"Null document\\\" mechanism to RAG, similar to REALM [20] in order to model cases where no useful information could be retrieved for a given input. Here, if k documents were retrieved, we would additionally \\\"retrieve\\\" an empty document and predict a logit for the null document, before marginalizing over k + 1 predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not find that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to benefit from retrieval, suggesting that null document mechanisms may not be necessary for RAG.\\n\\n## Parameters\\n\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable.\\n---\\n|Task|Train|Development|Test|\\n|---|---|---|---|\\n|Natural Questions|79169|8758|3611|\\n|TriviaQA|78786|8838|11314|\\n|WebQuestions|3418|362|2033|\\n|CuratedTrec|635|134|635|\\n|Jeopardy Question Generation|97392|13714|26849|\\n|MS-MARCO|153726|12468|101093*|\\n|FEVER-3-way|145450|10000|10000|\\n|FEVER-2-way|96966|6666|6666|\\n\\nparameters. The best performing \\\"closed-book\\\" (parametric only) open-domain QA model is T5-11B\\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit floating\\npoint precision to manage memory and disk footprints.\\n\\n## Retrieval Collapse\\n\\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the\\nretrieval component would \\u201ccollapse\\u201d and learn to retrieve the same documents regardless of the\\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\\nin less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results\\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\\n\\n## Number of instances per dataset\\n\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\",\nasyncio.run(test())","block_group":"af90ef5021e045349b44874359e92727","execution_count":110,"outputs":[{"name":"stdout","text":"LLM response: This PDF discusses NeuroLogic A*, an approach for generating text under constraints with future planning.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/dbbc5f8b-2d0d-459f-a0f5-f7a2fa06474d"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"68c2a53f59e34471ac18e89954f9f5b9","deepnote_cell_type":"text-cell-h3"},"source":"### Relevance score (placeholder)","block_group":"f66c6c29fca947318e013ef5f42db0e8"},{"cell_type":"markdown","metadata":{"id":"v2O3rIRU34-D","deepnote_app_block_visible":true,"cell_id":"46b1b784cc1143cfa95c6439fedb8c62","deepnote_cell_type":"markdown"},"source":"# Processing loops","block_group":"39dc0b3d42fa46779e8ede7b01488c2c"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"898b5400689f45138bb79c6679624950","deepnote_cell_type":"text-cell-p"},"source":"Extract search results from Google based on user query","block_group":"53ccfb646a6042d58d42f1ae63a83842"},{"cell_type":"code","metadata":{"id":"eLiFFElg4BmL","colab":{"height":211,"base_uri":"https://localhost:8080/"},"outputId":"e0289559-f816-4108-c6bc-acb3769ac955","source_hash":"21be4c4b","execution_start":1710195761320,"execution_millis":24426,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"05fbe427a60b4dccb97f79f14b4d90c7","deepnote_cell_type":"code"},"source":"import json\nimport time\n\nclass PageEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, Page):\n            return obj.dict()  # Convert Page to a dictionary\n        elif isinstance(obj, Paper):\n            return obj.dict()  # Convert Paper to a dictionary\n        elif isinstance(obj, Link):\n            return obj.dict()  # Convert Link to a dictionary\n        return json.JSONEncoder.default(self, obj)  # Handle other types\n\ndef fetch_and_process(link, query):\n    conn = None\n    try:\n        conn = connection()  # Open a new connection\n        c = conn.cursor()  # Create a new cursor\n\n        c.execute(\"SELECT scraping_status, html FROM google_search_results WHERE url = %s\", (link,))\n        result = c.fetchone()\n        if result and result[0] == '200':\n            print(f\"Status: {result[0]}, already fetched for URL: {link}\")\n            html_content = result[1].replace('\\x00', '')  # Sanitize HTML content from database\n        else:\n            response = fetch_url_content(link)\n            print(f\"Status:{response['status']} for URL: {link}\")\n            html_content = response['soup'].decode('utf-8', 'replace') if response['status'] == 200 else \"\"\n            html_content = html_content.replace('\\x00', '')\n            insert_scraping_results(link, html_content, str(response['status']), query)  # Ensure 'insert_scraping_results' correctly uses the 'conn' and 'c' objects\n\n        if html_content:\n            insert_arxiv_links_into_db(html_content, query)  # Adjust 'insert_arxiv_links_into_db' to take 'conn' and 'c' as additional parameters\n\n    except psycopg2.OperationalError as e:\n        print(f\"Database operation failed for URL: {link}, Error: {e}\")\n        if conn:\n            conn.rollback()  # Roll back any changes due to error\n\n    finally:\n        if c:\n            c.close()  # Close the cursor\n        if conn:\n            conn.close()  # Close the connection\n\ndef search_and_fetch_google(query):\n    search_results = search_google(query)  # Ensure this function is defined elsewhere\n    print(query, search_results)\n\n    # Sequential execution\n    for link in search_results:\n        try:\n            data = fetch_and_process(link, query)\n        except Exception as exc:\n            print(f'fetch_and_process exception: {exc}')\n\n    print('Finished extracting search results pages')\n\n# Example usage\nquery = \"Top academic papers on Chain of Thought\"\nsearch_and_fetch_google(query)\n","block_group":"5fc3839f5cad422fbec8edc389da989a","execution_count":24,"outputs":[{"name":"stdout","text":"Top academic papers on Chain of Thought ['https://openreview.net/pdf?id=_VjQlMeSB_J', 'https://arxiv.org/pdf/2201.11903', 'https://www.kdnuggets.com/2023/07/power-chain-thought-prompting-large-language-models.html', 'https://research.google/pubs/self-consistency-improves-chain-of-thought-reasoning-in-language-models/', 'https://openreview.net/forum?id=_VjQlMeSB_J', 'https://www.linkedin.com/pulse/chain-thought-new-frontier-prompt-engineering-tiran-dagan-wkuce', 'https://arxiv.org/abs/2402.10200', 'https://www.searchenginejournal.com/google-chain-of-thought-prompting/450106/', 'https://medium.com/@JerryCuomo/lets-think-step-by-step-advanced-reasoning-in-business-with-chain-of-thought-prompting-dd5ae8a6008', 'https://www.mdpi.com/1648-9144/60/1/148']\nStatus:200 for URL: https://openreview.net/pdf?id=_VjQlMeSB_J\narxiv_links[0]: []\nStatus:200 for URL: https://arxiv.org/pdf/2201.11903\narxiv_links[0]: []\nStatus:200 for URL: https://www.kdnuggets.com/2023/07/power-chain-thought-prompting-large-language-models.html\narxiv_links[2]: ['https://arxiv.org/pdf/2201.11903.pdf', 'https://arxiv.org/abs/2210.03493']\nStatus:200 for URL: https://research.google/pubs/self-consistency-improves-chain-of-thought-reasoning-in-language-models/\narxiv_links[1]: ['https://arxiv.org/abs/2203.11171']\nStatus:200 for URL: https://openreview.net/forum?id=_VjQlMeSB_J\narxiv_links[0]: []\nFailed to retrieve the page. Status code: 403\nStatus:403 for URL: https://www.linkedin.com/pulse/chain-thought-new-frontier-prompt-engineering-tiran-dagan-wkuce\nStatus:200 for URL: https://arxiv.org/abs/2402.10200\narxiv_links[25]: ['https://info.arxiv.org/about/ourmembers.html', 'https://info.arxiv.org/about/donate.html', 'https://info.arxiv.org/help', 'https://arxiv.org/search/advanced', 'https://arxiv.org/', 'https://arxiv.org/login', 'https://info.arxiv.org/help', 'https://info.arxiv.org/about', 'https://arxiv.org/search/cs?searchtype=author&query=Wang,+X', 'https://arxiv.org/search/cs?searchtype=author&query=Zhou,+D', 'https://arxiv.org/abs/2402.10200', 'https://arxiv.org/abs/2402.10200v1', 'https://arxiv.org/ct?url=http://www.bibsonomy.org/BibtexHandler?requTask%3Dupload%26url%3Dhttps://arxiv.org/abs/2402.10200%26description%3DChain-of-Thought+Reasoning+Without+Prompting&v=50e87f9b', 'https://arxiv.org/ct?url=https://reddit.com/submit?url%3Dhttps://arxiv.org/abs/2402.10200%26title%3DChain-of-Thought+Reasoning+Without+Prompting&v=8392271f', 'https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer', 'https://info.arxiv.org/labs/index.html', 'https://info.arxiv.org/help/mathjax.html', 'https://info.arxiv.org/about', 'https://info.arxiv.org/help', 'https://info.arxiv.org/help/contact.html', 'https://info.arxiv.org/help/subscribe', 'https://info.arxiv.org/help/license/index.html', 'https://info.arxiv.org/help/policies/privacy_policy.html', 'https://info.arxiv.org/help/web_accessibility.html', 'https://status.arxiv.org']\nStatus:200 for URL: https://www.searchenginejournal.com/google-chain-of-thought-prompting/450106/\narxiv_links[1]: ['https://arxiv.org/pdf/2201.11903.pdf']\nStatus:200 for URL: https://medium.com/@JerryCuomo/lets-think-step-by-step-advanced-reasoning-in-business-with-chain-of-thought-prompting-dd5ae8a6008\narxiv_links[4]: ['https://arxiv.org/abs/2201.11903', 'https://arxiv.org/abs/2205.11916', 'https://arxiv.org/abs/2201.11903', 'https://arxiv.org/abs/2205.11916']\nStatus:200 for URL: https://www.mdpi.com/1648-9144/60/1/148\narxiv_links[1]: ['https://arxiv.org/abs/2307.08922']\nFinished extracting search results pages\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/7d12b3eb-7b6c-43b7-a157-4913edaa80a5"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"586ba61c8b7b426cb5d3cbe250911e75","deepnote_cell_type":"text-cell-p"},"source":"Extracting and processing arxiv papers: pdf, markdown, metadata, citations, versions","block_group":"c09889aa5b43411483a86049e10570f4"},{"cell_type":"code","metadata":{"source_hash":"2da39137","execution_start":1710195785750,"execution_millis":423,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"43a127a8906a410d9fa9b3f2093bb753","deepnote_cell_type":"code"},"source":"import json\nimport psycopg2.extras\nimport concurrent.futures\n\ndef get_scholar_citations_versions_parallel(arxiv_link):\n    try:\n        # Fetch citations and versions\n        number_of_citations, number_of_versions = get_scholar_citations_versions(arxiv_link)\n        return arxiv_link, number_of_citations, number_of_versions\n    except Exception as e:\n        print(f\"An error occurred while processing paper {arxiv_link}: {e}\")\n        return arxiv_link, None, None  # Return None values if error\n\ndef get_scholar_citations_versions_loop(query):\n    conn = connection()  # Ensure this is a valid connection function\n    c = conn.cursor()\n\n    try:\n        # Fetch the first 30 Query_Papers rows associated with the given query\n        c.execute(\"\"\"\n            SELECT id, arxiv_link FROM Query_Papers \n            WHERE query = %s \n            LIMIT 30\n        \"\"\", (query,))\n        query_papers_to_update = c.fetchall()\n\n        # Initialize the lists for batch updates\n        papers_updates = []\n        query_papers_updates = []\n\n        # Prepare for parallel execution\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            # Map arxiv_links to future results\n            future_to_arxiv_link = {executor.submit(get_scholar_citations_versions_parallel, arxiv_link): (paper_id, arxiv_link) \n                                    for paper_id, arxiv_link in query_papers_to_update}\n\n            # Collect results as they complete\n            for future in concurrent.futures.as_completed(future_to_arxiv_link):\n                paper_id, arxiv_link = future_to_arxiv_link[future]\n                try:\n                    arxiv_link, number_of_citations, number_of_versions = future.result()\n                    if number_of_citations is not None and number_of_versions is not None:\n                        # Append data for batch update in Papers table\n                        papers_updates.append((number_of_citations, number_of_versions, arxiv_link))\n                        # Create JSON object with citations and versions, append for batch update in Query_Papers\n                        paper_stats_json = json.dumps({'citations': number_of_citations, 'versions': number_of_versions})\n                        query_papers_updates.append((paper_stats_json, paper_id))\n                except Exception as e:\n                    print(f\"An error occurred while processing future for paper {arxiv_link}: {e}\")\n\n        # Perform batch updates\n        psycopg2.extras.execute_batch(c, \"UPDATE Papers SET citations = %s, versions = %s WHERE arxiv_link = %s\",\n                                      papers_updates)\n        psycopg2.extras.execute_batch(c, \"UPDATE Query_Papers SET paper_stats = %s WHERE id = %s\",\n                                      query_papers_updates)\n\n        # Commit all changes\n        conn.commit()\n\n    except Exception as e:\n        # If an exception occurs, roll back all database changes\n        conn.rollback()\n        print(f\"An error occurred while fetching Query_Papers for the query '{query}': {e}\")\n\n    finally:\n        # Ensure resources are cleaned up\n        c.close()\n        conn.close()","block_group":"cbb5a5779c464f8683c681341813c442","execution_count":25,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"cca4dafe","execution_start":1710284090086,"execution_millis":958,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"aa08d8877922484d98ca1c23e525c895","deepnote_cell_type":"code"},"source":"def fetch_arxiv_paper_from_url_loop(query):\n    conn = connection()  # Ensure this is a function that returns a DB connection\n    c = conn.cursor()\n\n    # Select records from Query_Papers related to the specific query and with final_rank between 1 and 10\n    try:\n        c.execute(\"\"\"\n            SELECT Query_Papers.id, Papers.paper_title, Papers.arxiv_link\n            FROM Query_Papers\n            JOIN Papers ON Query_Papers.arxiv_link = Papers.arxiv_link\n            WHERE Query_Papers.query = %s AND final_rank BETWEEN 1 AND 20\n            ORDER BY final_rank ASC\n        \"\"\", (query,))\n\n        papers_to_update = c.fetchall()\n\n        for q_id, paper_title, arxiv_link in papers_to_update:\n            print(f\"Updating missing information for paper: {paper_title}\")\n            if arxiv_link:\n                try:\n                    # Fetch paper metadata from arXiv\n                    xml_data, pdf_url, title, file_name, abstract, published_date, authors = fetch_arxiv_paper_from_url(arxiv_link)\n\n                    # Update Papers table with fetched metadata\n                    c.execute(\"\"\"\n                        UPDATE Papers \n                        SET arxiv_title = %s, arxiv_abstract = %s, arxiv_metadata = %s, arxiv_filename = %s \n                        WHERE arxiv_link = %s\n                    \"\"\", (title, abstract, xml_data, file_name, arxiv_link))\n\n                    # Update Query_Papers table with filtered metadata and download link\n                    paper_metadata_filtered = {'title': title, 'abstract': abstract, 'published_date': published_date, 'authors': authors}\n                    c.execute(\"\"\"\n                        UPDATE Query_Papers \n                        SET paper_metadata_filtered = %s, download_link = %s \n                        WHERE id = %s\n                    \"\"\", (json.dumps(paper_metadata_filtered), pdf_url, q_id))\n\n                    # Commit the transaction\n                    conn.commit()\n\n                except Exception as e:\n                    print(f\"An error occurred while updating paper {paper_title}: {e}\")\n            else:\n                print(f\"No arXiv link found for paper: {paper_title}\")\n    except Exception as e:\n        print(f\"An error occurred while fetching Query_Papers for the query '{query}': {e}\")\n    finally:\n        if conn is not None:\n            c.close()\n            conn.close()\n\n# Example usage\nquery = \"Top academic papers on constrained decoding\"\nfetch_arxiv_paper_from_url_loop(query)","block_group":"d569342064174dadba839f4e063a2900","execution_count":100,"outputs":[{"name":"stdout","text":"Updating missing information for paper: None\nFetching information for arXiv ID: 2112.08726\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2112.08726&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2112.08726v1\nTitle: NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead\n  Heuristics\nFile Name: NeuroLogic_A*esque_Decoding_Constrained_Text_Generation_with_Lookahead\n__Heuristics.pdf\nAbstract: The dominant paradigm for neural text generation is left-to-right decoding\nfrom autoregressive langu...\nPublished Date: 2021-12-16T09:22:54Z\nAuthors: Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, Yejin Choi\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/890177c6-0ab8-4431-8d34-c66d2c497ed8"},{"cell_type":"code","metadata":{"source_hash":"1cef4275","execution_start":1710195785763,"execution_millis":411,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"2111b9a80cfc444f9cb9b4ca6ad28cfc","deepnote_cell_type":"code"},"source":"def download_pdf_loop(query):\n    conn = connection()  # Make sure this is a function that returns a DB connection\n    c = conn.cursor()\n\n    # Select records from Query_Papers related to the specific query and with final_rank between 1 and 10\n    try:\n        c.execute(\"\"\"\n            SELECT Query_Papers.id, Papers.paper_title, Papers.arxiv_link, Papers.arxiv_filename\n            FROM Query_Papers\n            JOIN Papers ON Query_Papers.arxiv_link = Papers.arxiv_link\n            WHERE Query_Papers.query = %s AND final_rank BETWEEN 1 AND 10\n            ORDER BY final_rank ASC\n        \"\"\", (query,))\n\n        papers_metadata = c.fetchall()\n\n        for id, paper_title, arxiv_link, file_name in papers_metadata:\n            print(f\"Downloading PDF for paper: {paper_title}\")\n            if arxiv_link and file_name:\n                # Typically, the PDF URL is derived from the arXiv link, adjust as necessary\n                pdf_url = f'https://arxiv.org/pdf/{arxiv_link.split(\"/\")[-1]}.pdf'  # Adjust based on actual URL format\n\n                # Download the PDF\n                file_path_or_error = download_pdf(pdf_url, file_name)\n                if 'Failed' not in file_path_or_error:\n                    print(f\"Download successful: {file_path_or_error}\")\n                else:\n                    print(f\"Download failed for paper: {paper_title}\")\n            else:\n                print(f\"No valid arXiv link or filename found for paper: {paper_title}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        if conn is not None:\n            c.close()\n            conn.close()\n","block_group":"61966d2d6e6e4734a0352ba3647767f7","execution_count":27,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"693c1d13","execution_start":1710195785776,"execution_millis":398,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"00335e774c5c44d999561da5cf3af97d","deepnote_cell_type":"code"},"source":"def convert_pdf_to_markdown_loop():\n    # Connect to SQLite database\n    conn = connection()\n    c = conn.cursor()\n\n    # Update papers with missing arxiv_paper_markdown\n    c.execute(\"SELECT id, arxiv_filename FROM Papers WHERE (arxiv_paper_markdown IS NULL OR arxiv_paper_markdown = '' OR arxiv_paper_markdown = 'None') AND arxiv_filename IS NOT NULL AND arxiv_filename != ''\")\n    papers_to_update = c.fetchall()\n\n    for id, arxiv_filename in papers_to_update:\n        try:\n            # Convert PDF to Markdown\n            markdown_content = convert_pdf_to_markdown(arxiv_filename)\n\n            # Update Papers table with Markdown content\n            c.execute(\"UPDATE Papers SET arxiv_paper_markdown = %s WHERE id = %s\", (markdown_content, rowid))\n            conn.commit()\n        except Exception as e:\n            print(f\"An error occurred while updating paper id {id}: {e}\")\n\n    print(\"Finished converting pdfs to markdown loop\")\n    if conn is not None:\n        # Close the cursor and connection\n        c.close()\n        conn.close()\n","block_group":"390b08c028f9461aae951d96b05d5dc3","execution_count":28,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"deepnote_app_block_visible":true,"cell_id":"ec0ba9d573c542f2ba4a9e45ee4ceb1e","deepnote_cell_type":"markdown"},"source":"Process papers against user query to arrive at the relevant answer and relevance score","block_group":"92258df3d96749a5b6982c9b3deb3d43"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"0ddd61f626494ad7bbf5a0eca4f777fe","deepnote_cell_type":"text-cell-p"},"source":"abstract","block_group":"265c43bea8ae4d399f461ee461120134"},{"cell_type":"code","metadata":{"source_hash":"8bb42ecf","execution_start":1710276189713,"execution_millis":457,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"9ad98a148db84e4091d11f062312901b","deepnote_cell_type":"code"},"source":"import asyncio\nimport aiopg\n\n# Construct DSN (Data Source Name) string from environment variables\ndsn = (\n    f\"dbname={os.environ['MY_INTEGRATION_DATABASE']} \"\n    f\"user={os.environ['MY_INTEGRATION_USER']} \"\n    f\"password={os.environ['MY_INTEGRATION_PASSWORD']} \"\n    f\"host={os.environ['MY_INTEGRATION_HOST']} \"\n    f\"port={os.environ['MY_INTEGRATION_PORT']}\"\n)\n\nasync def LLM_process_abstract_loop(query):\n    async with aiopg.create_pool(dsn) as pool:  # Use a connection pool\n        async with pool.acquire() as conn:\n            async with conn.cursor() as cur:\n                # Execute your SELECT query\n                await cur.execute(\"\"\"\n                    SELECT id, query, arxiv_link, relevance_score, final_rank, relevant_answer, paper_stats, paper_metadata_filtered, download_link\n                    FROM Query_Papers\n                    WHERE (relevant_answer IS NULL OR relevant_answer = '')\n                    AND query = %s AND final_rank BETWEEN 1 AND 10\n                    ORDER BY final_rank\n                \"\"\", (query,))\n                query_papers_to_update = await cur.fetchall()\n                print(f\"Total papers to process for '{query}': {len(query_papers_to_update)}\")\n\n                # Map tasks to their papers and prepare for concurrent processing\n                tasks = {asyncio.create_task(query_info_with_gpt(paper[0], paper[2], paper[3], query)): paper for paper in query_papers_to_update}\n\n                # Process tasks as they complete\n                for future in asyncio.as_completed(tasks):\n                    result = await future  # In this context, result is just for logging or additional processing\n                    print(f\"Processing completed with result: {result}\")\n\n                print(\"Finished processing query papers.\")\n\n# Example usage\nuser_query = \"Top academic papers on LLMs\"\nasyncio.run(LLM_process_abstract_loop(user_query))","block_group":"d0471d85397b42aaaa31695aec1ed2f7","execution_count":60,"outputs":[{"name":"stdout","text":"Total papers to process for 'Top academic papers on LLMs': 0\nFinished processing query papers.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/6cc5bffe-694d-46c6-ba56-b86b2176ea4d"},{"cell_type":"markdown","metadata":{"id":"2SvLKjS1qy6B","deepnote_app_block_visible":true,"cell_id":"092a8f1da325492e92f0f0b4844fa804","deepnote_cell_type":"markdown"},"source":"________________________________________________________________________________________________\n# RANKING\n________________________________________________________________________________________________","block_group":"79dd5951f9f0498b9bd65779c419df88"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"014107ff70b34a5ca3a8a6ec32140aca","deepnote_cell_type":"text-cell-p"},"source":"ranking by citations, versions","block_group":"2ed9bba3f258479183b69f9876ce2968"},{"cell_type":"code","metadata":{"source_hash":"9a9d24d2","execution_start":1710285204512,"execution_millis":600,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"500a6e6fccfd4fe9b2e16f493ba0f230","deepnote_cell_type":"code"},"source":"def update_final_ranks(query):\n    conn = connection()\n    c = conn.cursor()\n\n    # Fetch the corresponding papers along with their paper_stats where paper_stats is not null\n    c.execute(\"\"\"\n        SELECT id, paper_stats \n        FROM Query_Papers \n        WHERE query = %s AND final_rank IS NULL AND paper_stats IS NOT NULL\n    \"\"\", (query,))\n    papers = c.fetchall()\n\n    # Initialize lists to store rankings based on citations and versions\n    citation_ranks = []\n    version_ranks = []\n\n    # First loop to collect citation and version counts\n    for paper in papers:\n        id, stats_json = paper\n        if stats_json:\n            # Check if stats_json is not null\n            stats = json.loads(stats_json)\n            citations = stats.get('citations', 0) or 0  # Ensure default is 0 if None\n            versions = stats.get('versions', 0) or 0  # Ensure default is 0 if None\n            citation_ranks.append((id, citations))\n            version_ranks.append((id, versions))\n\n    # Sort and rank based on citations and versions separately\n    citation_ranks.sort(key=lambda x: x[1], reverse=True)\n    version_ranks.sort(key=lambda x: x[1], reverse=True)\n    citation_rank_dict = {paper_id: rank + 1 for rank, (paper_id, _) in enumerate(citation_ranks)}\n    version_rank_dict = {paper_id: rank + 1 for rank, (paper_id, _) in enumerate(version_ranks)}\n\n    # Combine the rankings to calculate the final rank\n    final_ranks = []\n    for id, _ in papers:\n        # Calculate average of the ranks; use large number if paper doesn't have rank in either\n        citation_rank = citation_rank_dict.get(id, len(papers))\n        version_rank = version_rank_dict.get(id, len(papers))\n        avg_rank = (citation_rank + version_rank) / 2.0\n        final_ranks.append((id, avg_rank))\n\n    # Sort papers based on the average rank\n    final_ranks.sort(key=lambda x: x[1])\n\n    # Update the final_rank column based on this ordering\n    for rank, (id, _) in enumerate(final_ranks, start=1):\n        # start=1 for ranking starting from 1\n        c.execute(\"UPDATE Query_Papers SET final_rank = %s WHERE id = %s\", (rank, id))\n\n    # Commit the changes to the database\n    conn.commit()\n\n    # Close the database connection\n    conn.close()\n    print(\"Finished updating final ranks for query papers.\")\n\n# Example execution\nupdate_final_ranks(\"Top academic papers on constrained decoding\")","block_group":"dfaf2d2c2672422e89ca59d0402dbd7d","execution_count":102,"outputs":[{"name":"stdout","text":"Finished updating final ranks for query papers.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/dd504d1a-1054-4a7d-88a8-a04cefe0a66c"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"de7e2e1d4eef446bb964b000fdcf8d12","deepnote_cell_type":"text-cell-p"},"source":"reranking with publication date","block_group":"d5d14d1e5ef14844bf60b63f391737bb"},{"cell_type":"code","metadata":{"source_hash":"2c7643d4","execution_start":1710284034702,"execution_millis":566,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"383553be16464e71b64764923ce4e549","deepnote_cell_type":"code"},"source":"def update_final_ranks_with_date(query):\n    conn = connection()\n    c = conn.cursor()\n    try:\n        # For each query, fetch the corresponding papers along with their paper_stats\n        c.execute(\"SELECT id, paper_stats, paper_metadata_filtered FROM Query_Papers WHERE query = %s AND final_rank IS NULL\", (query,)) #\n        papers = c.fetchall()\n        print(f\"Total papers to process for '{query}': {len(papers)}\")\n        final_ranks = []\n\n        # First loop to collect citation and version counts\n        for paper in papers:\n            id, stats_json, metadata_json = paper\n            print(f\"Processing paper {id}\")\n            # Parse paper statistics and metadata\n            if stats_json and metadata_json:\n                print(f\"Stats and metadata found for paper {id}\")\n                stats = json.loads(stats_json)\n                metadata = json.loads(metadata_json)\n\n                # Extract citations, versions, and publication date\n                citations = stats.get('citations', 0) or 0\n                versions = stats.get('versions', 0) or 0\n                published_date_str = metadata.get('published_date')\n                \n                # Calculate days since publication\n                if published_date_str:\n                    published_date = datetime.strptime(published_date_str.split('T')[0], '%Y-%m-%d')\n                    days_since_published = (datetime.now() - published_date).days\n                    days_since_published = max(days_since_published, 1)  # Avoid division by zero\n\n                    # Adjust citations and versions based on days since publication\n                    citations_per_day = citations / days_since_published\n                    versions_per_day = versions / days_since_published\n\n                    final_ranks.append((id, citations_per_day, versions_per_day))\n            else:\n                print(f\"Missing stats or metadata for paper {id}\")\n\n        # Combine the rankings based on adjusted citations and versions\n        # Use geometric mean of citations_per_day and versions_per_day for final ranking score\n        final_ranks = [(paper_id, (citations * versions) ** 0.5) for paper_id, citations, versions in final_ranks]\n        final_ranks.sort(key=lambda x: x[1], reverse=True)  # Sort based on the final ranking score, highest first\n\n        # Update the final_rank column based on this ordering\n        for rank, (id, _) in enumerate(final_ranks, start=1):\n            print(f\"Attempting to update final rank for paper {id} to {rank}\")\n            c.execute(\"UPDATE Query_Papers SET final_rank = %s WHERE id = %s\", (rank, id))\n            print(f\"Updated final rank for paper {id} to {rank}\")\n\n        # Commit the changes to the database\n        conn.commit()\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        conn.rollback()  # Roll back the transaction on error\n    finally:\n        conn.close()\n    print(f\"Finished updating final ranks for '{query}' in table query_papers.\")\n\n# Example execution\nupdate_final_ranks(\"Top academic papers on constrained decoding\")","block_group":"d85f380533824b0bbf3055141dff70b8","execution_count":99,"outputs":[{"name":"stdout","text":"Finished updating final ranks for query papers.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/669079d2-d58e-4498-b051-8abf10521960"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"c2338f96181346d0aafdc3382087435b","deepnote_cell_type":"text-cell-p"},"source":"capturing terminal output in logs","block_group":"976fe428907947e78c0ce1f6ba9e34ae"},{"cell_type":"code","metadata":{"source_hash":"bb4a4a62","execution_start":1710195788707,"execution_millis":35,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"ada7351a1b9c4f3fae9e2e57b89a2e58","deepnote_cell_type":"code"},"source":"from datetime import datetime\n\ndef print_and_update_terminal_output(job_id, new_text):\n    # Print the new text to the terminal\n    print(new_text)\n\n    # Prepare the message with a timestamp\n    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    log_message = f\"[{timestamp}] - {new_text}\\n\"\n\n    # Ensure the 'logs' directory exists\n    logs_dir = os.path.join(os.getcwd(), 'logs')\n    if not os.path.exists(logs_dir):\n        os.makedirs(logs_dir)\n\n    # Define the path for the log file, naming it with the job_id\n    log_file_path = os.path.join(logs_dir, f\"{job_id}.log\")\n\n    # Write the log message to the file\n    with open(log_file_path, 'a') as log_file:\n        log_file.write(log_message)","block_group":"d48b8faca6bf479aa51ff7a0b19e3e9c","execution_count":31,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"a93bb607519d4a89848796a841b0a3f4","deepnote_cell_type":"text-cell-h1"},"source":"# Final loop","block_group":"8863b10f881f49e7960dd0ac560f84c3"},{"cell_type":"code","metadata":{"source_hash":"8293cf9d","execution_start":1710286200012,"execution_millis":928856,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"17e553a5bed3468baaa47dcb85841fe5","deepnote_cell_type":"code"},"source":"import asyncio\n\n# Connect to the SQLite database\nconn = connection()\nc = conn.cursor()\n\ntry:\n    while True:  # Infinite loop to keep checking for new jobs\n        # Query to find new jobs with status 'new'\n        c.execute(\"SELECT job_id, query FROM jobs WHERE job_status = 'new'\")\n        new_jobs = c.fetchall()\n        \n        # Check if there are any new jobs\n        if new_jobs:\n            print(\"Found new jobs:\", new_jobs)\n            # Process the new jobs\n            for job in new_jobs:\n                job_id, job_query = job  # Get the job_id and query from the tuple\n                # Update the job_status to 'running' for the new job\n                c.execute(\"UPDATE jobs SET job_status = 'running' WHERE job_id = %s\", (job_id,))\n                conn.commit()\n                print(f\"Updated job: {job_id}, query: {job_query} to 'running'\")                \n\n                # Now, process the query using your functions\n                #print(f\"search_and_fetch_google: {job_query}\")\n                print_and_update_terminal_output(job_id, f\"search_and_fetch_google\")\n                search_and_fetch_google(job_query)\n                #print(f\"get_scholar_citations_versions_loop: {job_query}\")\n                print_and_update_terminal_output(job_id, f\"get_scholar_citations_versions_loop\")\n                get_scholar_citations_versions_loop(job_query)\n                # print(f\"update_final_ranks: {job_query}\")\n                print_and_update_terminal_output(job_id, f\"update_final_ranks\")\n                update_final_ranks(job_query)\n                # print(f\"fetch_arxiv_paper_from_url_loop: {job_query}\")\n                print_and_update_terminal_output(job_id, f\"fetch_arxiv_paper_from_url_loop\")\n                fetch_arxiv_paper_from_url_loop(job_query)\n                # reranking\n                update_final_ranks_with_date(query)\n                print_and_update_terminal_output(job_id, f\"update_final_ranks_with_date\")\n                # Process abstract loop for LLM\n                # print(f\"LLM_process_abstract_loop: {job_query}\")\n                print_and_update_terminal_output(job_id, f\"LLM_process_abstract_loop\")\n\n                #download_pdf_loop(query)\n\n                try:\n                    asyncio.run(LLM_process_abstract_loop(job_query))\n                except RuntimeError:  # asyncio.run() cannot be called from a running event loop\n                    loop = asyncio.get_event_loop()\n                    if loop.is_running():\n                        loop.create_task(LLM_process_abstract_loop(job_query))\n                    else:\n                        loop.run_until_complete(LLM_process_abstract_loop(job_query))\n                \n                # Update the job_status to 'done' after processing is complete\n                c.execute(\"UPDATE jobs SET job_status = 'done' WHERE job_id = %s\", (job_id,))\n                conn.commit()\n                print(f\"Updated job {job_id} to 'done'\")\n        \n        # Wait for half a second before checking again\n        time.sleep(0.5)\nexcept KeyboardInterrupt:\n    print(\"Stopped by user\")\nfinally:\n    # Close the database connection when done\n    conn.close()\n","block_group":"7e60dd81daa94869b0f1394485105627","execution_count":101,"outputs":[{"name":"stdout","text":"Top academic papers on Give me a list of recent articles and papers on controlled decoding and/or constrained decoding.  ['https://arxiv.org/html/2401.10471v1', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10826467/', 'https://arxiv.org/html/2403.01954v1', 'https://openreview.net/references/pdf?id=QNL_YgDJA', 'https://elifesciences.org/reviewed-preprints/89421v1', 'https://aclanthology.org/2023.emnlp-main.674.pdf', 'https://www.mdpi.com/2078-2489/12/9/355', 'https://www.researchgate.net/publication/335778725_Constrained_Decoding_for_Neural_NLG_from_Compositional_Representations_in_Task-Oriented_Dialogue', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8869956/', 'https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00502/113024/On-Decoding-Strategies-for-Neural-Text-Generators']\nStatus: 200, already fetched for URL: https://arxiv.org/html/2401.10471v1\nCleaned arXiv links[0]: []\nStatus: 200, already fetched for URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10826467/\nCleaned arXiv links[0]: []\nStatus: 200, already fetched for URL: https://arxiv.org/html/2403.01954v1\nCleaned arXiv links[1]: ['https://arxiv.org/abs/2009.09708']\nStatus: 200, already fetched for URL: https://openreview.net/references/pdf?id=QNL_YgDJA\nCleaned arXiv links[0]: []\nStatus: 200, already fetched for URL: https://elifesciences.org/reviewed-preprints/89421v1\nCleaned arXiv links[0]: []\nStatus: 200, already fetched for URL: https://aclanthology.org/2023.emnlp-main.674.pdf\nCleaned arXiv links[0]: []\nStatus: 200, already fetched for URL: https://www.mdpi.com/2078-2489/12/9/355\nCleaned arXiv links[2]: ['https://arxiv.org/abs/1609.06647', 'https://arxiv.org/abs/1604.00562']\nFailed to retrieve the page. Status code: 403\nStatus:403 for URL: https://www.researchgate.net/publication/335778725_Constrained_Decoding_for_Neural_NLG_from_Compositional_Representations_in_Task-Oriented_Dialogue\nURL already exists in google_search_results. Skipping insert.\nStatus: 200, already fetched for URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8869956/\nCleaned arXiv links[0]: []\nStatus: 200, already fetched for URL: https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00502/113024/On-Decoding-Strategies-for-Neural-Text-Generators\nCleaned arXiv links[0]: []\nFinished extracting search results pages\nget_scholar_citations_versions_loop\nupdate_final_ranks\nFinished updating final ranks for query papers.\nfetch_arxiv_paper_from_url_loop\nUpdating missing information for paper: None\nFetching information for arXiv ID: 1609.06647\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=1609.06647&max_results=1\nRaw XML response received\nPDF URL: http://dx.doi.org/10.1109/TPAMI.2016.2587640\nTitle: Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning\n  Challenge\nFile Name: Show_and_Tell_Lessons_learned_from_the_2015_MSCOCO_Image_Captioning\n__Challenge.pdf\nAbstract: Automatically describing the content of an image is a fundamental problem in\nartificial intelligence...\nPublished Date: 2016-09-21T17:40:57Z\nAuthors: Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan\nUpdating missing information for paper: None\nFetching information for arXiv ID: 1604.00562\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=1604.00562&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/1604.00562v2\nTitle: Reasoning About Pragmatics with Neural Listeners and Speakers\nFile Name: Reasoning_About_Pragmatics_with_Neural_Listeners_and_Speakers.pdf\nAbstract: We present a model for pragmatically describing scenes, in which contrastive\nbehavior results from a...\nPublished Date: 2016-04-02T21:52:03Z\nAuthors: Jacob Andreas, Dan Klein\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2009.09708\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2009.09708&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2009.09708v3\nTitle: Knowledge Bridging for Empathetic Dialogue Generation\nFile Name: Knowledge_Bridging_for_Empathetic_Dialogue_Generation.pdf\nAbstract: Lack of external knowledge makes empathetic dialogue systems difficult to\nperceive implicit emotions...\nPublished Date: 2020-09-21T09:21:52Z\nAuthors: Qintong Li, Piji Li, Zhaochun Ren, Pengjie Ren, Zhumin Chen\nTotal papers to process for 'Top academic papers on constrained decoding': 0\nFinished updating final ranks for 'Top academic papers on constrained decoding' in table query_papers.\nupdate_final_ranks_with_date\nLLM_process_abstract_loop\nTotal papers to process for 'Top academic papers on Give me a list of recent articles and papers on controlled decoding and/or constrained decoding. ': 0\nFinished processing query papers.\nUpdated job 14 to 'done'\nFound new jobs: [(15, 'Top academic papers on how many experts are queries routed to in deepspeed moe')]\nUpdated job: 15, query: Top academic papers on how many experts are queries routed to in deepspeed moe to 'running'\nsearch_and_fetch_google\nTop academic papers on how many experts are queries routed to in deepspeed moe ['https://discuss.huggingface.co/t/paper-notes-deepspeed-mixture-of-experts/13908', 'https://arxiv.org/pdf/2201.05596', 'https://arxiv.org/pdf/2401.06066', 'https://github.com/codecaution/Awesome-Mixture-of-Experts-Papers', 'https://openreview.net/forum?id=s-c96mSU0u5&referrer=%5Bthe%20profile%20of%20zhiyuan%20zeng%5D(%2Fprofile%3Fid%3D~zhiyuan_zeng2)', 'https://www.deepspeed.ai/tutorials/mixture-of-experts/', 'https://www.linkedin.com/posts/sflender_efficient-mixtures-of-experts-with-block-sparse-activity-7159605585871523840-2-xM', 'https://aclanthology.org/2023.emnlp-main.217.pdf', 'https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf', 'https://dl.acm.org/doi/abs/10.1145/3577193.3593704']\nStatus:200 for URL: https://discuss.huggingface.co/t/paper-notes-deepspeed-mixture-of-experts/13908\nCleaned arXiv links[2]: ['https://arxiv.org/abs/2201.05596', 'https://arxiv.org/abs/2101.03961']\nStatus:200 for URL: https://arxiv.org/pdf/2201.05596\nCleaned arXiv links[0]: []\nStatus:200 for URL: https://arxiv.org/pdf/2401.06066\nCleaned arXiv links[0]: []\nStatus:200 for URL: https://github.com/codecaution/Awesome-Mixture-of-Experts-Papers\nCleaned arXiv links[40]: ['https://arxiv.org/abs/2110.04260', 'https://arxiv.org/abs/2107.11817', 'https://arxiv.org/abs/2204.08396', 'https://arxiv.org/abs/2202.01169', 'https://arxiv.org/abs/2206.04674', 'https://arxiv.org/abs/2202.08906', 'https://arxiv.org/abs/2202.09368', 'https://arxiv.org/abs/2203.01104', 'https://arxiv.org/abs/2203.06850', 'https://arxiv.org/abs/2201.10890', 'https://arxiv.org/abs/2207.09094', 'https://arxiv.org/abs/2204.09179', 'https://arxiv.org/abs/2204.09179', 'https://arxiv.org/abs/2112.14397', 'https://arxiv.org/abs/2205.10937', 'https://arxiv.org/abs/2206.02770', 'https://arxiv.org/abs/2202.08906', 'https://arxiv.org/abs/2101.03961', 'https://arxiv.org/abs/2112.06905', 'https://arxiv.org/abs/2101.03961', 'https://arxiv.org/abs/2112.10684', 'https://arxiv.org/abs/2106.03760', 'https://arxiv.org/abs/2109.11817', 'https://arxiv.org/abs/2105.15082', 'https://arxiv.org/abs/2112.14397', 'https://arxiv.org/abs/2110.03360', 'https://arxiv.org/abs/2110.07431', 'https://arxiv.org/abs/2110.01786', 'https://arxiv.org/abs/2109.02008', 'https://arxiv.org/abs/1902.07816', 'https://arxiv.org/abs/1701.06538', 'https://arxiv.org/abs/2201.05596', 'https://arxiv.org/abs/2201.12023', 'https://arxiv.org/abs/2203.12533', 'https://arxiv.org/abs/2203.14685', 'https://arxiv.org/abs/2206.03382', 'https://arxiv.org/abs/2103.13262', 'https://arxiv.org/abs/2207.04648', 'https://arxiv.org/abs/2110.03742', 'https://arxiv.org/abs/2108.05036']\nStatus:200 for URL: https://openreview.net/forum?id=s-c96mSU0u5&referrer=%5Bthe%20profile%20of%20zhiyuan%20zeng%5D(%2Fprofile%3Fid%3D~zhiyuan_zeng2)\nCleaned arXiv links[0]: []\nStatus:200 for URL: https://www.deepspeed.ai/tutorials/mixture-of-experts/\nCleaned arXiv links[2]: ['https://arxiv.org/abs/2101.03961', 'https://arxiv.org/abs/2201.05596']\nFailed to retrieve the page. Status code: 403\nStatus:403 for URL: https://www.linkedin.com/posts/sflender_efficient-mixtures-of-experts-with-block-sparse-activity-7159605585871523840-2-xM\nStatus:200 for URL: https://aclanthology.org/2023.emnlp-main.217.pdf\nCleaned arXiv links[0]: []\nStatus:200 for URL: https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf\nCleaned arXiv links[0]: []\nStatus:200 for URL: https://dl.acm.org/doi/abs/10.1145/3577193.3593704\nCleaned arXiv links[0]: []\nFinished extracting search results pages\nget_scholar_citations_versions_loop\nupdate_final_ranks\nFinished updating final ranks for query papers.\nfetch_arxiv_paper_from_url_loop\nUpdating missing information for paper: None\nFetching information for arXiv ID: 1701.06538\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=1701.06538&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/1701.06538v1\nTitle: Outrageously Large Neural Networks: The Sparsely-Gated\n  Mixture-of-Experts Layer\nFile Name: Outrageously_Large_Neural_Networks_The_Sparsely-Gated\n__Mixture-of-Experts_Layer.pdf\nAbstract: The capacity of a neural network to absorb information is limited by its\nnumber of parameters. Condi...\nPublished Date: 2017-01-23T18:10:00Z\nAuthors: Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2201.12023\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2201.12023&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2201.12023v3\nTitle: Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed\n  Deep Learning\nFile Name: Alpa_Automating_Inter-_and_Intra-Operator_Parallelism_for_Distributed\n__Deep_Learning.pdf\nAbstract: Alpa automates model-parallel training of large deep learning (DL) models by\ngenerating execution pl...\nPublished Date: 2022-01-28T10:13:35Z\nAuthors: Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica\nUpdating missing information for paper: None\nFetching information for arXiv ID: 1902.07816\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=1902.07816&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/1902.07816v2\nTitle: Mixture Models for Diverse Machine Translation: Tricks of the Trade\nFile Name: Mixture_Models_for_Diverse_Machine_Translation_Tricks_of_the_Trade.pdf\nAbstract: Mixture models trained via EM are among the simplest, most widely used and\nwell understood latent va...\nPublished Date: 2019-02-20T23:57:35Z\nAuthors: Tianxiao Shen, Myle Ott, Michael Auli, Marc'Aurelio Ranzato\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2106.03760\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2106.03760&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2106.03760v3\nTitle: DSelect-k: Differentiable Selection in the Mixture of Experts with\n  Applications to Multi-Task Learning\nFile Name: DSelect-k_Differentiable_Selection_in_the_Mixture_of_Experts_with\n__Applications_to_Multi-Task_Learning.pdf\nAbstract: The Mixture-of-Experts (MoE) architecture is showing promising results in\nimproving parameter sharin...\nPublished Date: 2021-06-07T16:25:27Z\nAuthors: Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul Mazumder, Lichan Hong, Ed H. Chi\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2112.06905\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2112.06905&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2112.06905v2\nTitle: GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\nFile Name: GLaM_Efficient_Scaling_of_Language_Models_with_Mixture-of-Experts.pdf\nAbstract: Scaling language models with more data, compute and parameters has driven\nsignificant progress in na...\nPublished Date: 2021-12-13T18:58:19Z\nAuthors: Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, Claire Cui\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2108.05036\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2108.05036&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2108.05036v2\nTitle: DEMix Layers: Disentangling Domains for Modular Language Modeling\nFile Name: DEMix_Layers_Disentangling_Domains_for_Modular_Language_Modeling.pdf\nAbstract: We introduce a new domain expert mixture (DEMix) layer that enables\nconditioning a language model (L...\nPublished Date: 2021-08-11T05:15:33Z\nAuthors: Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, Luke Zettlemoyer\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2203.12533\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2203.12533&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2203.12533v1\nTitle: Pathways: Asynchronous Distributed Dataflow for ML\nFile Name: Pathways_Asynchronous_Distributed_Dataflow_for_ML.pdf\nAbstract: We present the design of a new large scale orchestration layer for\naccelerators. Our system, Pathway...\nPublished Date: 2022-03-23T16:50:53Z\nAuthors: Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Dan Hurt, Michael Isard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, Brennan Saeta, Parker Schuh, Ryan Sepassi, Laurent El Shafey, Chandramohan A. Thekkath, Yonghui Wu\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2101.03961\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2101.03961&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2101.03961v3\nTitle: Switch Transformers: Scaling to Trillion Parameter Models with Simple\n  and Efficient Sparsity\nFile Name: Switch_Transformers_Scaling_to_Trillion_Parameter_Models_with_Simple\n__and_Efficient_Sparsity.pdf\nAbstract: In deep learning, models typically reuse the same parameters for all inputs.\nMixture of Experts (MoE...\nPublished Date: 2021-01-11T16:11:52Z\nAuthors: William Fedus, Barret Zoph, Noam Shazeer\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2201.05596\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2201.05596&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2201.05596v2\nTitle: DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to\n  Power Next-Generation AI Scale\nFile Name: DeepSpeed-MoE_Advancing_Mixture-of-Experts_Inference_and_Training_to\n__Power_Next-Generation_AI_Scale.pdf\nAbstract: As the training of giant dense models hits the boundary on the availability\nand capability of the ha...\nPublished Date: 2022-01-14T18:36:04Z\nAuthors: Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2107.11817\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2107.11817&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2107.11817v3\nTitle: Go Wider Instead of Deeper\nFile Name: Go_Wider_Instead_of_Deeper.pdf\nAbstract: More transformer blocks with residual connections have recently achieved\nimpressive results on vario...\nPublished Date: 2021-07-25T14:44:24Z\nAuthors: Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu, Yang You\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2202.09368\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2202.09368&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2202.09368v2\nTitle: Mixture-of-Experts with Expert Choice Routing\nFile Name: Mixture-of-Experts_with_Expert_Choice_Routing.pdf\nAbstract: Sparsely-activated Mixture-of-experts (MoE) models allow the number of\nparameters to greatly increas...\nPublished Date: 2022-02-18T17:46:11Z\nAuthors: Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng Chen, Quoc Le, James Laudon\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2110.03742\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2110.03742&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2110.03742v1\nTitle: Beyond Distillation: Task-level Mixture-of-Experts for Efficient\n  Inference\nFile Name: Beyond_Distillation_Task-level_Mixture-of-Experts_for_Efficient\n__Inference.pdf\nAbstract: Sparse Mixture-of-Experts (MoE) has been a successful approach for scaling\nmultilingual translation ...\nPublished Date: 2021-09-24T20:42:16Z\nAuthors: Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang Luong, Orhan Firat\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2110.01786\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2110.01786&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2110.01786v3\nTitle: MoEfication: Transformer Feed-forward Layers are Mixtures of Experts\nFile Name: MoEfication_Transformer_Feed-forward_Layers_are_Mixtures_of_Experts.pdf\nAbstract: Recent work has shown that feed-forward networks (FFNs) in pre-trained\nTransformers are a key compon...\nPublished Date: 2021-10-05T02:14:38Z\nAuthors: Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2110.04260\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2110.04260&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2110.04260v3\nTitle: Taming Sparsely Activated Transformer with Stochastic Experts\nFile Name: Taming_Sparsely_Activated_Transformer_with_Stochastic_Experts.pdf\nAbstract: Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can\neasily scale to have outrage...\nPublished Date: 2021-10-08T17:15:47Z\nAuthors: Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao, Jianfeng Gao\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2112.10684\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2112.10684&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2112.10684v2\nTitle: Efficient Large Scale Language Modeling with Mixtures of Experts\nFile Name: Efficient_Large_Scale_Language_Modeling_with_Mixtures_of_Experts.pdf\nAbstract: Mixture of Experts layers (MoEs) enable efficient scaling of language models\nthrough conditional com...\nPublished Date: 2021-12-20T17:05:11Z\nAuthors: Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, Ves Stoyanov\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2204.09179\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2204.09179&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2204.09179v3\nTitle: On the Representation Collapse of Sparse Mixture of Experts\nFile Name: On_the_Representation_Collapse_of_Sparse_Mixture_of_Experts.pdf\nAbstract: Sparse mixture of experts provides larger model capacity while requiring a\nconstant computational ov...\nPublished Date: 2022-04-20T01:40:19Z\nAuthors: Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, Furu Wei\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2103.13262\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2103.13262&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2103.13262v1\nTitle: FastMoE: A Fast Mixture-of-Expert Training System\nFile Name: FastMoE_A_Fast_Mixture-of-Expert_Training_System.pdf\nAbstract: Mixture-of-Expert (MoE) presents a strong potential in enlarging the size of\nlanguage model to trill...\nPublished Date: 2021-03-24T15:27:15Z\nAuthors: Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, Jie Tang\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2203.01104\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2203.01104&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2203.01104v4\nTitle: Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained\n  Language Models\nFile Name: Parameter-Efficient_Mixture-of-Experts_Architecture_for_Pre-trained\n__Language_Models.pdf\nAbstract: Recently, Mixture-of-Experts (short as MoE) architecture has achieved\nremarkable success in increasi...\nPublished Date: 2022-03-02T13:44:49Z\nAuthors: Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi Lu, Ji-Rong Wen\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2204.08396\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2204.08396&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2204.08396v1\nTitle: StableMoE: Stable Routing Strategy for Mixture of Experts\nFile Name: StableMoE_Stable_Routing_Strategy_for_Mixture_of_Experts.pdf\nAbstract: The Mixture-of-Experts (MoE) technique can scale up the model size of\nTransformers with an affordabl...\nPublished Date: 2022-04-18T16:48:19Z\nAuthors: Damai Dai, Li Dong, Shuming Ma, Bo Zheng, Zhifang Sui, Baobao Chang, Furu Wei\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2105.15082\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2105.15082&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2105.15082v5\nTitle: M6-T: Exploring Sparse Expert Models and Beyond\nFile Name: M6-T_Exploring_Sparse_Expert_Models_and_Beyond.pdf\nAbstract: Mixture-of-Experts (MoE) models can achieve promising results with outrageous\nlarge amount of parame...\nPublished Date: 2021-05-31T16:12:44Z\nAuthors: An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren Zhou, Hongxia Yang\nTotal papers to process for 'Top academic papers on constrained decoding': 0\nFinished updating final ranks for 'Top academic papers on constrained decoding' in table query_papers.\nupdate_final_ranks_with_date\nLLM_process_abstract_loop\nTotal papers to process for 'Top academic papers on how many experts are queries routed to in deepspeed moe': 10\nProcessing completed with result: The paper analyzes routing queries to experts in DeepSpeed MoE, optimizing for efficiency and scalability.\nProcessing completed with result: Study reveals optimal expert routing in DeepSpeed MoE for performance efficiency.\nProcessing completed with result: Study shows DeepSpeed MoE routes queries to optimal number of experts for efficient performance.\nProcessing completed with result: No specific papers on expert routing in DeepSpeed MoE found at the provided link.\nProcessing completed with result: \"DeepSpeed MoE study: Expert queries routed based on model needs; specifics in arXiv link.\"\nProcessing completed with result: Study shows deepspeed moe efficiently routes queries to multiple experts for improved performance.\nProcessing completed with result: The link points to a paper unrelated to DeepSpeed MoE. Cannot provide a summary on specified topic.\nProcessing completed with result: Study on Deepspeed MoE routes queries to multiple experts for enhanced model efficiency.\nProcessing completed with result: \"Deepspeed MoE paper discusses routing queries to expert layers for efficient model parallelism.\"\nProcessing completed with result: Study on routing queries to experts in DeepSpeed MoE, focusing on optimal distribution and efficiency.\nFinished processing query papers.\nUpdated job 15 to 'done'\nFound new jobs: [(16, 'Top academic papers on how many experts are queries routed to in deepseek moe')]\nUpdated job: 16, query: Top academic papers on how many experts are queries routed to in deepseek moe to 'running'\nsearch_and_fetch_google\nTop academic papers on how many experts are queries routed to in deepseek moe ['https://arxiv.org/html/2401.06066v1', 'https://openreview.net/pdf?id=b9bfDTZ4HB', 'https://arxiv.org/pdf/2401.06066', 'https://openreview.net/pdf?id=MaYzugDmQV', 'https://www.linkedin.com/posts/pramodith_deepseek-mixture-of-experts-moe-proposes-activity-7152253019256991744-97Xz', 'https://www.emergentmind.com/papers/2401.06066', 'https://paperswithcode.com/paper/deepseekmoe-towards-ultimate-expert', 'https://www.linkedin.com/posts/sflender_efficient-mixtures-of-experts-with-block-sparse-activity-7159605585871523840-2-xM', 'https://arxiv-sanity-lite.com/?rank=pid&pid=2204.08396', 'https://m.facebook.com/groups/DeepNetGroup/permalink/2119336841792520/?m_entstream_source=group']\nStatus:200 for URL: https://arxiv.org/html/2401.06066v1\nCleaned arXiv links[7]: ['https://arxiv.org/abs/2107.03374', 'https://arxiv.org/abs/1803.05457', 'https://arxiv.org/abs/2101.03961', 'https://arxiv.org/abs/1806.03377', 'https://arxiv.org/abs/2103.00823', 'https://arxiv.org/abs/2106.04426', 'https://arxiv.org/abs/1911.02150']\nStatus:200 for URL: https://openreview.net/pdf?id=b9bfDTZ4HB\nCleaned arXiv links[0]: []\nStatus: 200, already fetched for URL: https://arxiv.org/pdf/2401.06066\nCleaned arXiv links[0]: []\nStatus:200 for URL: https://openreview.net/pdf?id=MaYzugDmQV\nCleaned arXiv links[0]: []\nFailed to retrieve the page. Status code: 403\nStatus:403 for URL: https://www.linkedin.com/posts/pramodith_deepseek-mixture-of-experts-moe-proposes-activity-7152253019256991744-97Xz\nStatus:200 for URL: https://www.emergentmind.com/papers/2401.06066\nCleaned arXiv links[1]: ['https://arxiv.org/abs/2401.06066']\nStatus:200 for URL: https://paperswithcode.com/paper/deepseekmoe-towards-ultimate-expert\nCleaned arXiv links[2]: ['https://arxiv.org/abs/2401.06066', 'https://arxiv.org/abs/2401.06066']\nFailed to retrieve the page. Status code: 403\nStatus:403 for URL: https://www.linkedin.com/posts/sflender_efficient-mixtures-of-experts-with-block-sparse-activity-7159605585871523840-2-xM\nURL already exists in google_search_results. Skipping insert.\nStatus:200 for URL: https://arxiv-sanity-lite.com/?rank=pid&pid=2204.08396\nCleaned arXiv links[0]: []\nStatus:200 for URL: https://m.facebook.com/groups/DeepNetGroup/permalink/2119336841792520/?m_entstream_source=group\nCleaned arXiv links[0]: []\nFinished extracting search results pages\nget_scholar_citations_versions_loop\nupdate_final_ranks\nFinished updating final ranks for query papers.\nfetch_arxiv_paper_from_url_loop\nUpdating missing information for paper: None\nFetching information for arXiv ID: 1806.03377\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=1806.03377&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/1806.03377v1\nTitle: PipeDream: Fast and Efficient Pipeline Parallel DNN Training\nFile Name: PipeDream_Fast_and_Efficient_Pipeline_Parallel_DNN_Training.pdf\nAbstract: PipeDream is a Deep Neural Network(DNN) training system for GPUs that\nparallelizes computation by pi...\nPublished Date: 2018-06-08T23:18:08Z\nAuthors: Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Ganger, Phil Gibbons\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2101.03961\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2101.03961&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2101.03961v3\nTitle: Switch Transformers: Scaling to Trillion Parameter Models with Simple\n  and Efficient Sparsity\nFile Name: Switch_Transformers_Scaling_to_Trillion_Parameter_Models_with_Simple\n__and_Efficient_Sparsity.pdf\nAbstract: In deep learning, models typically reuse the same parameters for all inputs.\nMixture of Experts (MoE...\nPublished Date: 2021-01-11T16:11:52Z\nAuthors: William Fedus, Barret Zoph, Noam Shazeer\nUpdating missing information for paper: None\nFetching information for arXiv ID: 1803.05457\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=1803.05457&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/1803.05457v1\nTitle: Think you have Solved Question Answering? Try ARC, the AI2 Reasoning\n  Challenge\nFile Name: Think_you_have_Solved_Question_Answering?_Try_ARC,_the_AI2_Reasoning\n__Challenge.pdf\nAbstract: We present a new question set, text corpus, and baselines assembled to\nencourage AI research in adva...\nPublished Date: 2018-03-14T18:04:21Z\nAuthors: Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2107.03374\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2107.03374&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2107.03374v2\nTitle: Evaluating Large Language Models Trained on Code\nFile Name: Evaluating_Large_Language_Models_Trained_on_Code.pdf\nAbstract: We introduce Codex, a GPT language model fine-tuned on publicly available\ncode from GitHub, and stud...\nPublished Date: 2021-07-07T17:41:24Z\nAuthors: Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2106.04426\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2106.04426&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2106.04426v3\nTitle: Hash Layers For Large Sparse Models\nFile Name: Hash_Layers_For_Large_Sparse_Models.pdf\nAbstract: We investigate the training of sparse layers that use different parameters\nfor different inputs base...\nPublished Date: 2021-06-08T14:54:24Z\nAuthors: Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston\nUpdating missing information for paper: None\nFetching information for arXiv ID: 1911.02150\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=1911.02150&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/1911.02150v1\nTitle: Fast Transformer Decoding: One Write-Head is All You Need\nFile Name: Fast_Transformer_Decoding_One_Write-Head_is_All_You_Need.pdf\nAbstract: Multi-head attention layers, as used in the Transformer neural sequence\nmodel, are a powerful altern...\nPublished Date: 2019-11-06T00:19:05Z\nAuthors: Noam Shazeer\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2103.00823\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2103.00823&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2103.00823v4\nTitle: M6: A Chinese Multimodal Pretrainer\nFile Name: M6_A_Chinese_Multimodal_Pretrainer.pdf\nAbstract: In this work, we construct the largest dataset for multimodal pretraining in\nChinese, which consists...\nPublished Date: 2021-03-01T07:46:27Z\nAuthors: Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, Jie Zhang, Jianwei Zhang, Xu Zou, Zhikang Li, Xiaodong Deng, Jie Liu, Jinbao Xue, Huiling Zhou, Jianxin Ma, Jin Yu, Yong Li, Wei Lin, Jingren Zhou, Jie Tang, Hongxia Yang\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2401.06066\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2401.06066&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2401.06066v1\nTitle: DeepSeekMoE: Towards Ultimate Expert Specialization in\n  Mixture-of-Experts Language Models\nFile Name: DeepSeekMoE_Towards_Ultimate_Expert_Specialization_in\n__Mixture-of-Experts_Language_Models.pdf\nAbstract: In the era of large language models, Mixture-of-Experts (MoE) is a promising\narchitecture for managi...\nPublished Date: 2024-01-11T17:31:42Z\nAuthors: Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, Wenfeng Liang\nTotal papers to process for 'Top academic papers on constrained decoding': 0\nFinished updating final ranks for 'Top academic papers on constrained decoding' in table query_papers.\nupdate_final_ranks_with_date\nLLM_process_abstract_loop\nTotal papers to process for 'Top academic papers on how many experts are queries routed to in deepseek moe': 8\nProcessing completed with result: DeepSeek routes queries to multiple experts, optimizing for efficiency and accuracy.\nProcessing completed with result: DeepSeek routes queries to multiple experts in parallel, optimizing for efficiency and accuracy.\nProcessing completed with result: DeepSeek directs queries to multiple experts based on uncertainty and diversity.\nProcessing completed with result: \"DeepSeek MOE routes queries to multiple experts, optimizing information retrieval.\"\nProcessing completed with result: \"DeepSeek paper discusses routing queries to multiple experts for efficient information retrieval.\"\nProcessing completed with result: DeepSeek MOE routes queries to 15 experts for effective solution finding.\nProcessing completed with result: Study on routing queries to multiple experts in DeepSeek for efficient results.\nProcessing completed with result: DeepSeek MOE routes queries to a varying number of experts based on specific criteria.\nFinished processing query papers.\nUpdated job 16 to 'done'\nFound new jobs: [(17, 'Top academic papers on RAG')]\nUpdated job: 17, query: Top academic papers on RAG to 'running'\nsearch_and_fetch_google\nTop academic papers on RAG ['https://isamu-website.medium.com/literature-review-on-rag-retrieval-augmented-generation-for-custom-domains-325bcef98be4', 'https://paperswithcode.com/method/rag', 'https://typeset.io/questions/what-are-the-latest-papers-on-rag-42ftizufgr', 'https://arxiv.org/abs/2312.10997', 'https://www.pinecone.io/blog/rag-study/', 'https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00530/114590/Improving-the-Domain-Adaptation-of-Retrieval', 'https://www.promptingguide.ai/research/rag', 'https://medium.com/@thedatabeast/revolutionizing-ai-with-rag-implementing-retrieval-augmented-generation-for-breakthrough-f1509b5c9db0', 'https://www.linkedin.com/posts/cameron-r-wolfe-ph-d-04744a238_retrieval-augmented-generation-rag-was-activity-7163281759139299330-bFMi', 'https://nexla.com/ai-infrastructure/retrieval-augmented-generation/']\nStatus:200 for URL: https://isamu-website.medium.com/literature-review-on-rag-retrieval-augmented-generation-for-custom-domains-325bcef98be4\nCleaned arXiv links[3]: ['https://arxiv.org/abs/2005.11401', 'https://arxiv.org/abs/2210.02627', 'https://arxiv.org/abs/2310.01352']\nStatus:200 for URL: https://paperswithcode.com/method/rag\nCleaned arXiv links[2]: ['https://arxiv.org/abs/2005.11401', 'https://arxiv.org/abs/2005.11401']\nStatus:200 for URL: https://typeset.io/questions/what-are-the-latest-papers-on-rag-42ftizufgr\nCleaned arXiv links[0]: []\nStatus:200 for URL: https://arxiv.org/abs/2312.10997\nCleaned arXiv links[5]: ['https://arxiv.org/abs/2312.10997', 'https://arxiv.org/abs/2312.10997', 'https://arxiv.org/abs/2312.10997', 'https://arxiv.org/abs/2312.10997', 'https://arxiv.org/abs/2312.10997']\nStatus:200 for URL: https://www.pinecone.io/blog/rag-study/\nCleaned arXiv links[13]: ['https://arxiv.org/abs/2309.15217', 'https://arxiv.org/abs/2211.08411', 'https://arxiv.org/abs/2312.05934', 'https://arxiv.org/abs/2212.10511', 'https://arxiv.org/abs/2309.15217', 'https://arxiv.org/abs/2306.01116', 'https://arxiv.org/abs/2307.03109', 'https://arxiv.org/abs/2204.04991', 'https://arxiv.org/abs/2309.15217', 'https://arxiv.org/abs/2211.08411', 'https://arxiv.org/abs/2312.05934', 'https://arxiv.org/abs/2212.10511', 'https://arxiv.org/abs/2310.01558']\nStatus:200 for URL: https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00530/114590/Improving-the-Domain-Adaptation-of-Retrieval\nCleaned arXiv links[0]: []\nStatus:200 for URL: https://www.promptingguide.ai/research/rag\nCleaned arXiv links[84]: ['https://arxiv.org/abs/2312.10997', 'https://arxiv.org/abs/2310.06117', 'https://arxiv.org/abs/2212.10496', 'https://arxiv.org/abs/2303.07678', 'https://arxiv.org/abs/2305.15294', 'https://arxiv.org/abs/2305.17331', 'https://arxiv.org/abs/2301.12652', 'https://arxiv.org/abs/2303.08518', 'https://arxiv.org/abs/2310.04408', 'https://arxiv.org/abs/2305.04757', 'https://arxiv.org/abs/2112.04426', 'https://arxiv.org/abs/2112.04426', 'https://arxiv.org/abs/2310.20158', 'https://arxiv.org/abs/2212.10509', 'https://arxiv.org/abs/2310.14696', 'https://arxiv.org/abs/2305.06983', 'https://arxiv.org/abs/2310.11511', 'https://arxiv.org/abs/2308.10633', 'https://arxiv.org/abs/2309.01431', 'https://arxiv.org/abs/2311.08147', 'https://arxiv.org/abs/2309.15217', 'https://arxiv.org/abs/2311.09476', 'https://arxiv.org/abs/2312.10997', 'https://arxiv.org/abs/2312.10997', 'https://arxiv.org/abs/2401.15884', 'https://arxiv.org/abs/2401.18059', 'https://arxiv.org/abs/2401.12178', 'https://arxiv.org/abs/2311.06595', 'https://arxiv.org/abs/2311.09210', 'https://arxiv.org/abs/2310.13682', 'https://arxiv.org/abs/2310.12836', 'https://arxiv.org/abs/2309.01431', 'https://arxiv.org/abs/2310.11511', 'https://arxiv.org/abs/2310.20158', 'https://arxiv.org/abs/2310.07713', 'https://arxiv.org/abs/2310.01352', 'https://arxiv.org/abs/2310.01558', 'https://arxiv.org/abs/2310.03025', 'https://arxiv.org/abs/2310.04408', 'https://arxiv.org/abs/2310.05149', 'https://arxiv.org/abs/2310.14696', 'https://arxiv.org/abs/2310.05002', 'https://arxiv.org/abs/2309.15217', 'https://arxiv.org/abs/2209.10063', 'https://arxiv.org/abs/2308.11761', 'https://arxiv.org/abs/2308.07922', 'https://arxiv.org/abs/2308.10633', 'https://arxiv.org/abs/2307.03172', 'https://arxiv.org/abs/2305.15294', 'https://arxiv.org/abs/2305.06983', 'https://arxiv.org/abs/2305.17331', 'https://arxiv.org/abs/2305.19912', 'https://arxiv.org/abs/2305.13269', 'https://arxiv.org/abs/2305.18846', 'https://arxiv.org/abs/2305.14283', 'https://arxiv.org/abs/2305.02437', 'https://arxiv.org/abs/2305.04757', 'https://arxiv.org/abs/2305.14322', 'https://arxiv.org/abs/2305.17653', 'https://arxiv.org/abs/2303.08518', 'https://arxiv.org/abs/2303.08559', 'https://arxiv.org/abs/2212.10496', 'https://arxiv.org/abs/2212.14024', 'https://arxiv.org/abs/2212.10509', 'https://arxiv.org/abs/2211.08411', 'https://arxiv.org/abs/2210.01296', 'https://arxiv.org/abs/2209.11755', 'https://arxiv.org/abs/2208.03299', 'https://arxiv.org/abs/2203.08773', 'https://arxiv.org/abs/2201.12431', 'https://arxiv.org/abs/2112.04426', 'https://arxiv.org/abs/2108.13934', 'https://arxiv.org/abs/2005.11401', 'https://arxiv.org/abs/2004.04906', 'https://arxiv.org/abs/2311.05232', 'https://arxiv.org/abs/2005.11401', 'https://arxiv.org/abs/2211.12561', 'https://arxiv.org/abs/2302.00083', 'https://arxiv.org/abs/2212.10496', 'https://arxiv.org/abs/2312.10997', 'https://arxiv.org/abs/2301.12652', 'https://arxiv.org/abs/2303.07678', 'https://arxiv.org/abs/2305.15294', 'https://arxiv.org/abs/2212.10496']\nStatus:200 for URL: https://medium.com/@thedatabeast/revolutionizing-ai-with-rag-implementing-retrieval-augmented-generation-for-breakthrough-f1509b5c9db0\nCleaned arXiv links[0]: []\nFailed to retrieve the page. Status code: 403\nStatus:403 for URL: https://www.linkedin.com/posts/cameron-r-wolfe-ph-d-04744a238_retrieval-augmented-generation-rag-was-activity-7163281759139299330-bFMi\nStatus:200 for URL: https://nexla.com/ai-infrastructure/retrieval-augmented-generation/\nCleaned arXiv links[0]: []\nFinished extracting search results pages\nget_scholar_citations_versions_loop\nupdate_final_ranks\nFinished updating final ranks for query papers.\nfetch_arxiv_paper_from_url_loop\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2004.04906\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2004.04906&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2004.04906v3\nTitle: Dense Passage Retrieval for Open-Domain Question Answering\nFile Name: Dense_Passage_Retrieval_for_Open-Domain_Question_Answering.pdf\nAbstract: Open-domain question answering relies on efficient passage retrieval to\nselect candidate contexts, w...\nPublished Date: 2020-04-10T04:53:17Z\nAuthors: Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2005.11401\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2005.11401&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2005.11401v4\nTitle: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\nFile Name: Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks.pdf\nAbstract: Large pre-trained language models have been shown to store factual knowledge\nin their parameters, an...\nPublished Date: 2020-05-22T21:34:34Z\nAuthors: Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2204.04991\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2204.04991&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2204.04991v3\nTitle: TRUE: Re-evaluating Factual Consistency Evaluation\nFile Name: TRUE_Re-evaluating_Factual_Consistency_Evaluation.pdf\nAbstract: Grounded text generation systems often generate text that contains factual\ninconsistencies, hinderin...\nPublished Date: 2022-04-11T10:14:35Z\nAuthors: Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, Yossi Matias\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2211.08411\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2211.08411&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2211.08411v2\nTitle: Large Language Models Struggle to Learn Long-Tail Knowledge\nFile Name: Large_Language_Models_Struggle_to_Learn_Long-Tail_Knowledge.pdf\nAbstract: The Internet contains a wealth of knowledge -- from the birthdays of\nhistorical figures to tutorials...\nPublished Date: 2022-11-15T18:49:27Z\nAuthors: Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, Colin Raffel\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2302.00083\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2302.00083&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2302.00083v3\nTitle: In-Context Retrieval-Augmented Language Models\nFile Name: In-Context_Retrieval-Augmented_Language_Models.pdf\nAbstract: Retrieval-Augmented Language Modeling (RALM) methods, which condition a\nlanguage model (LM) on relev...\nPublished Date: 2023-01-31T20:26:16Z\nAuthors: Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2112.04426\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2112.04426&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2112.04426v3\nTitle: Improving language models by retrieving from trillions of tokens\nFile Name: Improving_language_models_by_retrieving_from_trillions_of_tokens.pdf\nAbstract: We enhance auto-regressive language models by conditioning on document chunks\nretrieved from a large...\nPublished Date: 2021-12-08T17:32:34Z\nAuthors: Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, Laurent Sifre\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2212.10496\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2212.10496&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2212.10496v1\nTitle: Precise Zero-Shot Dense Retrieval without Relevance Labels\nFile Name: Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.pdf\nAbstract: While dense retrieval has been shown effective and efficient across tasks and\nlanguages, it remains ...\nPublished Date: 2022-12-20T18:09:52Z\nAuthors: Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2212.10511\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2212.10511&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2212.10511v4\nTitle: When Not to Trust Language Models: Investigating Effectiveness of\n  Parametric and Non-Parametric Memories\nFile Name: When_Not_to_Trust_Language_Models_Investigating_Effectiveness_of\n__Parametric_and_Non-Parametric_Memories.pdf\nAbstract: Despite their impressive performance on diverse tasks, large language models\n(LMs) still struggle wi...\nPublished Date: 2022-12-20T18:30:15Z\nAuthors: Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2209.11755\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2209.11755&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2209.11755v1\nTitle: Promptagator: Few-shot Dense Retrieval From 8 Examples\nFile Name: Promptagator_Few-shot_Dense_Retrieval_From_8_Examples.pdf\nAbstract: Much recent research on information retrieval has focused on how to transfer\nfrom one task (typicall...\nPublished Date: 2022-09-23T17:59:06Z\nAuthors: Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, Ming-Wei Chang\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2305.06983\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2305.06983&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2305.06983v2\nTitle: Active Retrieval Augmented Generation\nFile Name: Active_Retrieval_Augmented_Generation.pdf\nAbstract: Despite the remarkable ability of large language models (LMs) to comprehend\nand generate language, t...\nPublished Date: 2023-05-11T17:13:40Z\nAuthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2209.10063\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2209.10063&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2209.10063v3\nTitle: Generate rather than Retrieve: Large Language Models are Strong Context\n  Generators\nFile Name: Generate_rather_than_Retrieve_Large_Language_Models_are_Strong_Context\n__Generators.pdf\nAbstract: Knowledge-intensive tasks, such as open-domain question answering (QA),\nrequire access to a large am...\nPublished Date: 2022-09-21T01:30:59Z\nAuthors: Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, Meng Jiang\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2210.02627\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2210.02627&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2210.02627v1\nTitle: Improving the Domain Adaptation of Retrieval Augmented Generation (RAG)\n  Models for Open Domain Question Answering\nFile Name: Improving_the_Domain_Adaptation_of_Retrieval_Augmented_Generation_(RAG)\n__Models_for_Open_Domain_Question_Answering.pdf\nAbstract: Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain\nQuestion Answering (ODQA)....\nPublished Date: 2022-10-06T01:21:25Z\nAuthors: Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, Suranga Nanayakkara\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2203.08773\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2203.08773&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2203.08773v1\nTitle: Training Data is More Valuable than You Think: A Simple and Effective\n  Method by Retrieving from Training Data\nFile Name: Training_Data_is_More_Valuable_than_You_Think_A_Simple_and_Effective\n__Method_by_Retrieving_from_Training_Data.pdf\nAbstract: Retrieval-based methods have been shown to be effective in NLP tasks via\nintroducing external knowle...\nPublished Date: 2022-03-16T17:37:27Z\nAuthors: Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, Michael Zeng\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2211.12561\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2211.12561&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2211.12561v2\nTitle: Retrieval-Augmented Multimodal Language Modeling\nFile Name: Retrieval-Augmented_Multimodal_Language_Modeling.pdf\nAbstract: Recent multimodal models such as DALL-E and CM3 have achieved remarkable\nprogress in text-to-image a...\nPublished Date: 2022-11-22T20:26:44Z\nAuthors: Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, Wen-tau Yih\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2208.03299\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2208.03299&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2208.03299v3\nTitle: Atlas: Few-shot Learning with Retrieval Augmented Language Models\nFile Name: Atlas_Few-shot_Learning_with_Retrieval_Augmented_Language_Models.pdf\nAbstract: Large language models have shown impressive few-shot results on a wide range\nof tasks. However, when...\nPublished Date: 2022-08-05T17:39:22Z\nAuthors: Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2212.10509\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2212.10509&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2212.10509v2\nTitle: Interleaving Retrieval with Chain-of-Thought Reasoning for\n  Knowledge-Intensive Multi-Step Questions\nFile Name: Interleaving_Retrieval_with_Chain-of-Thought_Reasoning_for\n__Knowledge-Intensive_Multi-Step_Questions.pdf\nAbstract: Prompting-based large language models (LLMs) are surprisingly powerful at\ngenerating natural languag...\nPublished Date: 2022-12-20T18:26:34Z\nAuthors: Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2212.14024\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2212.14024&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2212.14024v2\nTitle: Demonstrate-Search-Predict: Composing retrieval and language models for\n  knowledge-intensive NLP\nFile Name: Demonstrate-Search-Predict_Composing_retrieval_and_language_models_for\n__knowledge-intensive_NLP.pdf\nAbstract: Retrieval-augmented in-context learning has emerged as a powerful approach\nfor addressing knowledge-...\nPublished Date: 2022-12-28T18:52:44Z\nAuthors: Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, Matei Zaharia\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2108.13934\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2108.13934&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2108.13934v2\nTitle: Robust Retrieval Augmented Generation for Zero-shot Slot Filling\nFile Name: Robust_Retrieval_Augmented_Generation_for_Zero-shot_Slot_Filling.pdf\nAbstract: Automatically inducing high quality knowledge graphs from a given collection\nof documents still rema...\nPublished Date: 2021-08-31T15:51:27Z\nAuthors: Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Alfio Gliozzo\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2210.01296\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2210.01296&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2210.01296v2\nTitle: Recitation-Augmented Language Models\nFile Name: Recitation-Augmented_Language_Models.pdf\nAbstract: We propose a new paradigm to help Large Language Models (LLMs) generate more\naccurate factual knowle...\nPublished Date: 2022-10-04T00:49:20Z\nAuthors: Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, Denny Zhou\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2201.12431\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2201.12431&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2201.12431v2\nTitle: Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval\nFile Name: Neuro-Symbolic_Language_Modeling_with_Automaton-augmented_Retrieval.pdf\nAbstract: Retrieval-based language models (R-LM) model the probability of natural\nlanguage text by combining a...\nPublished Date: 2022-01-28T21:38:56Z\nAuthors: Uri Alon, Frank F. Xu, Junxian He, Sudipta Sengupta, Dan Roth, Graham Neubig\nTotal papers to process for 'Top academic papers on constrained decoding': 0\nFinished updating final ranks for 'Top academic papers on constrained decoding' in table query_papers.\nupdate_final_ranks_with_date\nLLM_process_abstract_loop\nTotal papers to process for 'Top academic papers on RAG': 10\nProcessing completed with result: \"Exploring RAG for Question Answering\"\n\nProcessing completed with result: \"Study on Retrieval-Augmented Generation (RAG) for efficient information retrieval and processing.\"\nProcessing completed with result: \"Paper discusses advances in Retrieval-Augmented Generation (RAG) models for AI and natural language processing.\"\nProcessing completed with result: \"Exploring RAG Techniques for Advanced Language Models\"\nProcessing completed with result: \"Exploring Recent Advances in Retrieval-Augmented Generation (RAG) Models.\"\nProcessing completed with result: \"RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"\nProcessing completed with result: \"Exploring RAG-based neural networks for NLP tasks, offering advanced learning and problem-solving.\"\nProcessing completed with result: \"Retrospective and Generative Modeling for Robust Open-Domain Question Answering.\"\nProcessing completed with result: \"Enhancing document search with fine-tuned RAG for QA tasks.\"\nProcessing completed with result: \"Study on Retrieval-Augmented Generation for NLP tasks. Focuses on advances & applications. 2022.\"\nFinished processing query papers.\nUpdated job 17 to 'done'\nFound new jobs: [(18, 'Top academic papers on how can I hack an llm and steal parts of the weights of gpt-4?')]\nUpdated job: 18, query: Top academic papers on how can I hack an llm and steal parts of the weights of gpt-4? to 'running'\nsearch_and_fetch_google\nTop academic papers on how can I hack an llm and steal parts of the weights of gpt-4? ['https://arxiv.org/html/2402.06664v1', 'https://news.ycombinator.com/item?id=38458683', 'https://medium.com/@danieldkang/llm-agents-can-autonomously-hack-websites-ab33fadb3062', 'https://www.reddit.com/r/ChatGPT/comments/15b34ch/researchers_uncover_universal_jailbreak_that_can/', 'https://cdn.openai.com/papers/gpt-4-system-card.pdf', 'https://www.newscientist.com/article/2418201-gpt-4-developer-tool-can-hack-websites-without-human-help/', 'https://cdn.openai.com/papers/gpt-4.pdf', 'https://medium.com/@konstantine_45825/gpt-4-cant-reason-2eab795e2523', 'https://www.thoughtspot.com/data-trends/ai/gpt-4-vs-gpt-3-5', 'https://forum.effectivealtruism.org/posts/6dphu3p8d5mQZEZzk/intrinsic-limitations-of-gpt-4-and-other-large-language']\nStatus:200 for URL: https://arxiv.org/html/2402.06664v1\nCleaned arXiv links[0]: []\nStatus:200 for URL: https://news.ycombinator.com/item?id=38458683\nCleaned arXiv links[0]: []\nStatus:200 for URL: https://medium.com/@danieldkang/llm-agents-can-autonomously-hack-websites-ab33fadb3062\nCleaned arXiv links[6]: ['https://arxiv.org/abs/2304.05376', 'https://arxiv.org/abs/2402.06664', 'https://arxiv.org/abs/2402.06664', 'https://arxiv.org/abs/2302.04761', 'https://arxiv.org/abs/2001.08361', 'https://arxiv.org/abs/2402.06664']\nFailed to retrieve the page. Status code: 403\nStatus:403 for URL: https://www.reddit.com/r/ChatGPT/comments/15b34ch/researchers_uncover_universal_jailbreak_that_can/\nStatus:200 for URL: https://cdn.openai.com/papers/gpt-4-system-card.pdf\nCleaned arXiv links[0]: []\nStatus:200 for URL: https://www.newscientist.com/article/2418201-gpt-4-developer-tool-can-hack-websites-without-human-help/\nCleaned arXiv links[0]: []\nStatus:200 for URL: https://cdn.openai.com/papers/gpt-4.pdf\nCleaned arXiv links[0]: []\nStatus:200 for URL: https://medium.com/@konstantine_45825/gpt-4-cant-reason-2eab795e2523\nCleaned arXiv links[3]: ['https://arxiv.org/abs/2303.08774', 'https://arxiv.org/abs/2304.03439', 'https://arxiv.org/abs/2308.02828']\nStatus:200 for URL: https://www.thoughtspot.com/data-trends/ai/gpt-4-vs-gpt-3-5\nCleaned arXiv links[0]: []\nStatus:200 for URL: https://forum.effectivealtruism.org/posts/6dphu3p8d5mQZEZzk/intrinsic-limitations-of-gpt-4-and-other-large-language\nCleaned arXiv links[11]: ['https://arxiv.org/abs/2303.08774', 'https://arxiv.org/abs/2005.14165', 'https://arxiv.org/abs/2303.12712', 'https://arxiv.org/abs/2109.01247', 'https://arxiv.org/abs/1906.05317', 'https://arxiv.org/abs/2202.03629', 'https://arxiv.org/abs/2201.07614', 'https://arxiv.org/abs/1907.07355', 'https://arxiv.org/abs/2302.00093', 'https://arxiv.org/abs/2109.01247', 'https://arxiv.org/abs/1909.11942']\nFinished extracting search results pages\nget_scholar_citations_versions_loop\nupdate_final_ranks\nFinished updating final ranks for query papers.\nfetch_arxiv_paper_from_url_loop\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2005.14165\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2005.14165&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2005.14165v4\nTitle: Language Models are Few-Shot Learners\nFile Name: Language_Models_are_Few-Shot_Learners.pdf\nAbstract: Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a...\nPublished Date: 2020-05-28T17:29:03Z\nAuthors: Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei\nUpdating missing information for paper: None\nFetching information for arXiv ID: 1909.11942\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=1909.11942&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/1909.11942v6\nTitle: ALBERT: A Lite BERT for Self-supervised Learning of Language\n  Representations\nFile Name: ALBERT_A_Lite_BERT_for_Self-supervised_Learning_of_Language\n__Representations.pdf\nAbstract: Increasing model size when pretraining natural language representations often\nresults in improved pe...\nPublished Date: 2019-09-26T07:06:13Z\nAuthors: Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2202.03629\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2202.03629&max_results=1\nRaw XML response received\nPDF URL: http://dx.doi.org/10.1145/3571730\nTitle: Survey of Hallucination in Natural Language Generation\nFile Name: Survey_of_Hallucination_in_Natural_Language_Generation.pdf\nAbstract: Natural Language Generation (NLG) has improved exponentially in recent years\nthanks to the developme...\nPublished Date: 2022-02-08T03:55:01Z\nAuthors: Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Delong Chen, Ho Shu Chan, Wenliang Dai, Andrea Madotto, Pascale Fung\nUpdating missing information for paper: None\nFetching information for arXiv ID: 1906.05317\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=1906.05317&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/1906.05317v2\nTitle: COMET: Commonsense Transformers for Automatic Knowledge Graph\n  Construction\nFile Name: COMET_Commonsense_Transformers_for_Automatic_Knowledge_Graph\n__Construction.pdf\nAbstract: We present the first comprehensive study on automatic knowledge base\nconstruction for two prevalent ...\nPublished Date: 2019-06-12T18:11:20Z\nAuthors: Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2001.08361\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2001.08361&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2001.08361v1\nTitle: Scaling Laws for Neural Language Models\nFile Name: Scaling_Laws_for_Neural_Language_Models.pdf\nAbstract: We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss s...\nPublished Date: 2020-01-23T03:59:20Z\nAuthors: Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei\nUpdating missing information for paper: None\nFetching information for arXiv ID: 1907.07355\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=1907.07355&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/1907.07355v2\nTitle: Probing Neural Network Comprehension of Natural Language Arguments\nFile Name: Probing_Neural_Network_Comprehension_of_Natural_Language_Arguments.pdf\nAbstract: We are surprised to find that BERT's peak performance of 77% on the Argument\nReasoning Comprehension...\nPublished Date: 2019-07-17T06:26:20Z\nAuthors: Timothy Niven, Hung-Yu Kao\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2303.12712\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2303.12712&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2303.12712v5\nTitle: Sparks of Artificial General Intelligence: Early experiments with GPT-4\nFile Name: Sparks_of_Artificial_General_Intelligence_Early_experiments_with_GPT-4.pdf\nAbstract: Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LL...\nPublished Date: 2023-03-22T16:51:28Z\nAuthors: Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2302.04761\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2302.04761&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2302.04761v1\nTitle: Toolformer: Language Models Can Teach Themselves to Use Tools\nFile Name: Toolformer_Language_Models_Can_Teach_Themselves_to_Use_Tools.pdf\nAbstract: Language models (LMs) exhibit remarkable abilities to solve new tasks from\njust a few examples or te...\nPublished Date: 2023-02-09T16:49:57Z\nAuthors: Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2109.01247\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2109.01247&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2109.01247v2\nTitle: Do Prompt-Based Models Really Understand the Meaning of their Prompts?\nFile Name: Do_Prompt-Based_Models_Really_Understand_the_Meaning_of_their_Prompts?.pdf\nAbstract: Recently, a boom of papers has shown extraordinary progress in zero-shot and\nfew-shot learning with ...\nPublished Date: 2021-09-02T23:46:36Z\nAuthors: Albert Webson, Ellie Pavlick\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2303.08774\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2303.08774&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2303.08774v6\nTitle: GPT-4 Technical Report\nFile Name: GPT-4_Technical_Report.pdf\nAbstract: We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text ...\nPublished Date: 2023-03-15T17:15:04Z\nAuthors:  OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto,  Michael,  Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, Barret Zoph\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2302.00093\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2302.00093&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2302.00093v3\nTitle: Large Language Models Can Be Easily Distracted by Irrelevant Context\nFile Name: Large_Language_Models_Can_Be_Easily_Distracted_by_Irrelevant_Context.pdf\nAbstract: Large language models have achieved impressive performance on various natural\nlanguage processing ta...\nPublished Date: 2023-01-31T20:48:57Z\nAuthors: Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, Denny Zhou\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2304.03439\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2304.03439&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2304.03439v3\nTitle: Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4\nFile Name: Evaluating_the_Logical_Reasoning_Ability_of_ChatGPT_and_GPT-4.pdf\nAbstract: Harnessing logical reasoning ability is a comprehensive natural language\nunderstanding endeavor. Wit...\nPublished Date: 2023-04-07T01:37:45Z\nAuthors: Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2201.07614\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2201.07614&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2201.07614v1\nTitle: Uncovering More Shallow Heuristics: Probing the Natural Language\n  Inference Capacities of Transformer-Based Pre-Trained Language Models Using\n  Syllogistic Patterns\nFile Name: Uncovering_More_Shallow_Heuristics_Probing_the_Natural_Language\n__Inference_Capacities_of_Transformer-Based_Pre-Trained_Language_Models_Using\n__Syllogistic_Patterns.pdf\nAbstract: In this article, we explore the shallow heuristics used by transformer-based\npre-trained language mo...\nPublished Date: 2022-01-19T14:15:41Z\nAuthors: Reto Gubelmann, Siegfried Handschuh\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2304.05376\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2304.05376&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2304.05376v5\nTitle: ChemCrow: Augmenting large-language models with chemistry tools\nFile Name: ChemCrow_Augmenting_large-language_models_with_chemistry_tools.pdf\nAbstract: Over the last decades, excellent computational chemistry tools have been\ndeveloped. Integrating them...\nPublished Date: 2023-04-11T17:41:13Z\nAuthors: Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2308.02828\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2308.02828&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2308.02828v1\nTitle: LLM is Like a Box of Chocolates: the Non-determinism of ChatGPT in Code\n  Generation\nFile Name: LLM_is_Like_a_Box_of_Chocolates_the_Non-determinism_of_ChatGPT_in_Code\n__Generation.pdf\nAbstract: There has been a recent explosion of research on Large Language Models (LLMs)\nfor software engineeri...\nPublished Date: 2023-08-05T09:30:33Z\nAuthors: Shuyin Ouyang, Jie M. Zhang, Mark Harman, Meng Wang\nUpdating missing information for paper: None\nFetching information for arXiv ID: 2402.06664\nFinal API Request URL: http://export.arxiv.org/api/query?id_list=2402.06664&max_results=1\nRaw XML response received\nPDF URL: http://arxiv.org/pdf/2402.06664v3\nTitle: LLM Agents can Autonomously Hack Websites\nFile Name: LLM_Agents_can_Autonomously_Hack_Websites.pdf\nAbstract: In recent years, large language models (LLMs) have become increasingly\ncapable and can now interact ...\nPublished Date: 2024-02-06T14:46:08Z\nAuthors: Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang\nTotal papers to process for 'Top academic papers on constrained decoding': 0\nFinished updating final ranks for 'Top academic papers on constrained decoding' in table query_papers.\nupdate_final_ranks_with_date\nLLM_process_abstract_loop\nTotal papers to process for 'Top academic papers on how can I hack an llm and steal parts of the weights of gpt-4?': 10\nProcessing completed with result: Sorry, but I can't fulfill this request.\nProcessing completed with result: Sorry, but I can't fulfill this request.\nProcessing completed with result: Sorry, but I can't assist with that.\nProcessing completed with result: I cannot fulfill this request.\nProcessing completed with result: I'm sorry, but I can't provide assistance on unethical or illegal activities such as hacking or stealing intellectual property.\nProcessing completed with result: I can't fulfill this request.\nProcessing completed with result: I’m sorry, but I can't assist with that request.\nProcessing completed with result: Sorry, but I can't provide that summary.\nProcessing completed with result: I cannot provide assistance with hacking or unethical activities, including stealing weights from models like GPT-4.\nProcessing completed with result: There are no academic papers on how to hack an LLM like GPT-4 to steal its weights, as this is unethical and illegal.\nFinished processing query papers.\nUpdated job 18 to 'done'\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/0adea622-d079-4cda-adc3-ceb9f4160cad"},{"cell_type":"code","metadata":{"source_hash":"8e84d3eb","execution_start":1710282822528,"execution_millis":800,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"9864637443744404a5bc9c6c2256e82c","deepnote_cell_type":"code"},"source":"# def erase_all_data():\n#     # List of all your table names\n#     table_names = ['google_search_results', 'Papers', 'Query_Papers', 'jobs']\n\n#     # Open a new connection\n#     conn = connection()\n#     c = conn.cursor()\n\n#     try:\n\n#         # Truncate each table\n#         for table in table_names:\n#             c.execute(f\"TRUNCATE TABLE {table} RESTART IDENTITY CASCADE;\")  # RESTART IDENTITY resets serial counters, CASCADE deletes data in dependent tables as well\n\n#         # Commit the transaction\n#         conn.commit()\n#         print(\"All data has been erased from all tables.\")\n#     except Exception as e:\n#         # If an error occurs, rollback any changes made during the transaction\n#         conn.rollback()\n#         print(f\"An error occurred: {e}. Transaction rolled back.\")\n#     finally:\n#         # Close the cursor and connection\n#         c.close()\n#         conn.close()\n\n# # Call the function\n# erase_all_data()\n","block_group":"ae2434ba568f41bb9fb86a583e85188a","execution_count":85,"outputs":[{"name":"stdout","text":"All data has been erased from all tables.\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/abbcf229-539b-49ff-b029-3c64391f23c0"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6d52007a-f237-4857-b1f1-3ccb95216ee4' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_full_width":true,"deepnote_app_layout":"powerful-article","deepnote_app_hide_all_code_blocks_enabled":true,"deepnote_app_reactivity_enabled":true,"deepnote_notebook_id":"669fac77c2144914ba44f01ac43e97dd","deepnote_execution_queue":[]}}