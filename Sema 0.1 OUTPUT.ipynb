{"cells":[{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1709771878501,"execution_millis":83,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"15fdb985f7c742fb9532154af957d9be","deepnote_cell_type":"code"},"source":"","block_group":"15fdb985f7c742fb9532154af957d9be","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"3476add1","execution_start":1710194095309,"execution_millis":127,"deepnote_input_label":"Your search query here:","deepnote_variable_name":"input_query","deepnote_variable_value":"LLMs","deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"0d12df444d3941ea9782c85b51f6aa9b","deepnote_cell_type":"input-text"},"source":"input_query = 'LLMs'","block_group":"e19f0be04c9b49a192fa39a3858da19f","execution_count":182,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"bb9bc971","execution_start":1710194096408,"execution_millis":510,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"15a9423a4f54485497ecb05b9e9158c3","deepnote_cell_type":"code"},"source":"import psycopg2\nimport os\n\ndef connection():\n    \"\"\"Creates and returns a new database connection.\"\"\"\n    try:\n        conn = psycopg2.connect(\n            user=os.environ[\"MY_INTEGRATION_USER\"],\n            password=os.environ[\"MY_INTEGRATION_PASSWORD\"],\n            host=os.environ[\"MY_INTEGRATION_HOST\"],\n            port=os.environ[\"MY_INTEGRATION_PORT\"],\n            database=os.environ[\"MY_INTEGRATION_DATABASE\"]\n        )\n        \n        # Test the connection\n        with conn.cursor() as cursor:\n            cursor.execute(\"SELECT version();\")\n            record = cursor.fetchone()\n        \n        return conn  # Return the connection object if successful\n\n    except (Exception, psycopg2.Error) as error:\n        print(\"Error while connecting to database\", error)\n        return None  # Return None if connection was not successful\n\nconn = connection()","block_group":"35d1b25ccdc845448a6a60cc7807e612","execution_count":183,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"209a9f89","execution_start":1710194097562,"execution_millis":135,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"c2c4c9c84d3f42d78bfa325545dca282","deepnote_cell_type":"code"},"source":"import json\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich.text import Text\n\ndef display_query_papers(job_id):  # Change parameter to job_id\n    # Create a console object for Rich output\n    console = Console()\n\n    # Connect to the database\n    conn = connection()\n    c = conn.cursor()\n    \n    # Fetch the query and the number of ranks already printed for the given job_id\n    c.execute(\"SELECT query, printed_ranks FROM jobs WHERE job_id = %s\", (job_id,))\n    result = c.fetchone()\n    if result:\n        job_query, printed_ranks = result  # Unpack the result\n    else:\n        print(f\"No job found with ID: {job_id}\")\n        return  # Exit the function if no job is found\n\n    start_rank = printed_ranks # Start from the next rank\n\n    # Fetch records for the given query starting from the next rank to be printed\n    c.execute(\"\"\"\n        SELECT * FROM Query_Papers \n        WHERE query = %s AND final_rank >= %s AND final_rank IS NOT NULL \n        AND relevant_answer IS NOT NULL AND paper_stats IS NOT NULL \n        AND paper_metadata_filtered IS NOT NULL AND download_link IS NOT NULL\n        ORDER BY final_rank ASC\n        LIMIT 10\n    \"\"\", (job_query, start_rank))\n\n    # Fetch the column names\n    columns = [description[0] for description in c.description]\n\n    rows = c.fetchall()\n\n    # Counter for the number of ranks printed during this function call\n    ranks_printed_now = 0\n\n    if rows:\n        # Initialize a Rich table with improved formatting\n        table = Table(show_header=True, title=input_query, expand=True, leading=1, show_lines=True)\n        table.add_column(\"No.\", style=\"cyan\", justify=\"right\", ratio=1)\n        table.add_column(\"Paper\", overflow=\"fold\", ratio=20)  # This has twice the ratio of \"Details\", meaning it will be larger\n        table.add_column(\"Details\", overflow=\"fold\", ratio=8)  # Half the 'ratio' of \"Paper\", making it relatively smaller\n        table.add_column(\"Link\", justify=\"center\", ratio=2)\n        for row in rows:\n            # Extract the necessary fields from the row\n            final_rank = row[columns.index('final_rank')]\n            arxiv_link = row[columns.index('arxiv_link')]\n            relevant_answer = row[columns.index('relevant_answer')]\n            paper_stats = json.loads(row[columns.index('paper_stats')])\n            paper_metadata_filtered = json.loads(row[columns.index('paper_metadata_filtered')])\n            \n            # Format extracted data\n            title = paper_metadata_filtered.get('title', 'N/A')\n            abstract = paper_metadata_filtered.get('abstract', 'N/A')\n            abstract = (abstract[:197] + '...') if len(abstract) > 200 else abstract\n            published_date = paper_metadata_filtered.get('published_date', 'N/A').split('T')[0] if paper_metadata_filtered.get('published_date', 'N/A') != 'N/A' else 'N/A'\n            authors = paper_metadata_filtered.get('authors', ['N/A'])\n            authors_str = \", \".join(authors[:3]) + (\"...\" if len(authors) > 3 else \"\")\n            citations = paper_stats.get('citations', 'N/A')\n            versions = paper_stats.get('versions', 'N/A')\n\n            # Add the clickable 'Link' text\n            link_text = f\"[link={arxiv_link}]Link[/link]\"\n\n            # Format the Paper and Details columns\n            paper_column = Text(f\"{title}\\n\\nLLM response: {relevant_answer}\\n\\nAbstract: {abstract}\", justify=\"left\")\n            details_column = Text(f\"Citations: {citations}\\nVersions: {versions}\\nDate Published: {published_date}\\nAuthors: {authors_str} \\n\", justify=\"left\")\n            \n            # Add row with formatted data\n            table.add_row(str(final_rank), paper_column, details_column, link_text)\n            table.add_section()\n            table.add_row()\n\n            ranks_printed_now += 1\n\n        # Update the number of printed ranks in the jobs table for this query\n        new_total_printed = printed_ranks + ranks_printed_now\n        c.execute(\"UPDATE jobs SET printed_ranks = %s WHERE query = %s\", (new_total_printed, job_query))\n        conn.commit()\n    # Print the table to the console\n    if ranks_printed_now > 0:\n        console.print(table)\n    # Closing database connections\n    c.close()\n    conn.close()\n\n    return ranks_printed_now  # Optionally return the number of ranks printed in this call\n\n# # Example usage\n# job_query = \"Top academic papers on ReAct framework for agents\"\n# ranks_printed_now = display_query_papers(job_query)\n# print(f\"Ranks printed this time: {ranks_printed_now}\")\n","block_group":"1ee78ed7927d45708541c2dd376b88e7","execution_count":184,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"788900de","execution_start":1710196658111,"execution_millis":611,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"ca5e4b7dbccb46c98171167049eedb67","deepnote_cell_type":"code"},"source":"def add_new_job(query):\n    # Connect to the database\n    conn = connection()\n    c = conn.cursor()\n\n    # SQL statement to insert a new job and return its id\n    c.execute(\"INSERT INTO jobs (query, job_status) VALUES (%s, 'new') RETURNING job_id\", (query,))\n\n    # Fetch the job_id of the newly inserted job\n    job_id = c.fetchone()[0]\n    print(\"\\nHello there, we're busy working on your query: '{}'. Please consider that processing time is between 30 and 90 seconds.\".format(input_query))\n\n    # Commit the changes and close the connection\n    conn.commit()\n    c.close()\n    conn.close()\n\n    # Return the job_id for further use\n    return job_id\n\n# Usage\n# input_query = \"your input query here\" \njob_query = \"Top academic papers on \" + input_query\njob_id = add_new_job(job_query)\n# print(\"Job ID for query '{}': {}\".format(job_query, job_id))\n","block_group":"90e9c9b2cc7140b9900be848b05e99b0","execution_count":196,"outputs":[{"name":"stdout","text":"\nHello there, we're busy working on your query: 'LLMs'\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/417eab2e-2247-4cb5-b07f-63b63c3f15a1"},{"cell_type":"code","metadata":{"source_hash":"ec96d3dd","execution_start":1710196659345,"execution_millis":47949,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"a70c2578389e4b14b3a47a0798616c78","deepnote_cell_type":"code"},"source":"import time\nimport datetime  # Import the datetime module\nimport os  # Import the os module for clearing the terminal\nfrom IPython.display import clear_output\n\ndef wait_for_job_completion(job_id):  # Use job_id instead of job_query as the function parameter\n    # Connect to the database\n    conn = connection()\n    c = conn.cursor()\n    counter = 0  # Initialize the counter\n    last_read_line = 0\n    last_log_message = \"\"\n\n    try:\n        while True:  # Keep checking until the job is done\n            # SQL statement to find the status of a job given its job_id\n            c.execute(\"SELECT job_status, terminal_output FROM jobs WHERE job_id = %s\", (job_id,))  # Use job_id to search\n            result = c.fetchone()\n\n            if result:\n                job_status, terminal_output = result  # Unpack the result into job_status and terminal_output\n                if job_status == 'done':\n                    # clear_output(wait=True)  # Clear output and wait for the next\n                    display_query_papers(job_id)  # Assume this function should now work with job_query\n                    break  # Exit the loop if the job is done\n                elif job_status == 'running':\n                    clear_output(wait=True)  # Clear output and wait for the next\n                    counter += 1\n                    print(f\"Checking status ({counter} sec.): \", end=' ')  # Print counter with \"Running...\"\n                    print(f\"The status of the job with ID '{job_id}' is currently '{job_status}'. Waiting for completion...\")\n\n                    # Path to the log file\n                    log_file_path = os.path.join(os.getcwd(), 'logs', f\"{job_id}.log\")\n                    if os.path.exists(log_file_path):\n                        with open(log_file_path, 'r') as file:\n                            log_contents = file.readlines()\n                            # Check if there are any new log messages\n                            if last_read_line < len(log_contents):\n                                # Update to the new log message\n                                last_log_message = log_contents[-1].split('] - ', 1)[-1].strip() if '] - ' in log_contents[-1] else log_contents[-1]\n                                last_read_line = len(log_contents)\n                            # Print the most recent log message\n                            print('Logs: ' + last_log_message)\n                    else:\n                        print(\"Log file not found.\")\n\n                    time.sleep(1)  # Sleep for a while before checking again\n            else:\n                print(f\"\\nNo job found with ID: '{job_id}'.\")\n                break  # Exit the loop if no such job exists\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        # Close database resources\n        if conn:\n            c.close()\n            conn.close()\n\n# Usage example\n# job_id = 7\nwait_for_job_completion(job_id)\n","block_group":"2bf8387b66b34b4986a7603189c6dfd1","execution_count":197,"outputs":[{"name":"stdout","text":"Checking status (38 sec.):  The status of the job with ID '5' is currently 'running'. Waiting for completion...\nLast log message: LLM_process_abstract_loop\n","output_type":"stream"},{"data":{"text/plain":"\u001b[3m                                                       LLMs                                                        \u001b[0m\n┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mN…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPaper                                                                \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDetails                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLink \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m 1\u001b[0m\u001b[36m \u001b[0m│ Attention Is All You Need                                             │ Citations: 111587          │ \u001b]8;id=113856;https://arxiv.org/abs/1706.03762\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│                                                                       │ Versions: 87               │       │\n│\u001b[36m    \u001b[0m│ LLM response: \"Attention Is All You Need\" paper discusses             │ Date Published: 2017-06-12 │       │\n│\u001b[36m    \u001b[0m│ Transformer, a network architecture based on attention mechanisms.    │ Authors: Ashish Vaswani,   │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Noam Shazeer, Niki         │       │\n│\u001b[36m    \u001b[0m│ Abstract: The dominant sequence transduction models are based on      │ Parmar...                  │       │\n│\u001b[36m    \u001b[0m│ complex recurrent or                                                  │                            │       │\n│\u001b[36m    \u001b[0m│ convolutional neural networks in an encoder-decoder configuration.    │                            │       │\n│\u001b[36m    \u001b[0m│ The best                                                              │                            │       │\n│\u001b[36m    \u001b[0m│ performing models also connect the encoder an...                      │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m 2\u001b[0m\u001b[36m \u001b[0m│ You Only Look Once: Unified, Real-Time Object Detection               │ Citations: 45087           │ \u001b]8;id=107162;https://arxiv.org/abs/1506.02640\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│                                                                       │ Versions: 51               │       │\n│\u001b[36m    \u001b[0m│ LLM response: \"YOLO: New object detection approach frames detection   │ Date Published: 2015-06-08 │       │\n│\u001b[36m    \u001b[0m│ as a regression problem, improving speed & accuracy.\"                 │ Authors: Joseph Redmon,    │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Santosh Divvala, Ross      │       │\n│\u001b[36m    \u001b[0m│ Abstract: We present YOLO, a new approach to object detection. Prior  │ Girshick...                │       │\n│\u001b[36m    \u001b[0m│ work on object                                                        │                            │       │\n│\u001b[36m    \u001b[0m│ detection repurposes classifiers to perform detection. Instead, we    │                            │       │\n│\u001b[36m    \u001b[0m│ frame object                                                          │                            │       │\n│\u001b[36m    \u001b[0m│ detection as a regression problem to spatia...                        │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m 3\u001b[0m\u001b[36m \u001b[0m│ BERT: Pre-training of Deep Bidirectional Transformers for Language    │ Citations: 94319           │ \u001b]8;id=512953;https://arxiv.org/abs/1810.04805\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│   Understanding                                                       │ Versions: 49               │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Date Published: 2018-10-11 │       │\n│\u001b[36m    \u001b[0m│ LLM response: \"BERT: Pre-training of Deep Bidirectional Transformers  │ Authors: Jacob Devlin,     │       │\n│\u001b[36m    \u001b[0m│ for Language Understanding\" presents a language model improving NLP   │ Ming-Wei Chang, Kenton     │       │\n│\u001b[36m    \u001b[0m│ tasks.                                                                │ Lee...                     │       │\n│\u001b[36m    \u001b[0m│                                                                       │                            │       │\n│\u001b[36m    \u001b[0m│ Abstract: We introduce a new language representation model called     │                            │       │\n│\u001b[36m    \u001b[0m│ BERT, which stands                                                    │                            │       │\n│\u001b[36m    \u001b[0m│ for Bidirectional Encoder Representations from Transformers. Unlike   │                            │       │\n│\u001b[36m    \u001b[0m│ recent                                                                │                            │       │\n│\u001b[36m    \u001b[0m│ language representation models, BERT is designe...                    │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m 4\u001b[0m\u001b[36m \u001b[0m│ Language Models are Few-Shot Learners                                 │ Citations: 22336           │ \u001b]8;id=484114;https://arxiv.org/abs/2005.14165\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│                                                                       │ Versions: 33               │       │\n│\u001b[36m    \u001b[0m│ LLM response: \"Language Models are Few-Shot Learners\" paper discusses │ Date Published: 2020-05-28 │       │\n│\u001b[36m    \u001b[0m│ how scaling up language models like GPT-3 improves few-shot           │ Authors: Tom B. Brown,     │       │\n│\u001b[36m    \u001b[0m│ performance in NLP tasks.                                             │ Benjamin Mann, Nick        │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Ryder...                   │       │\n│\u001b[36m    \u001b[0m│ Abstract: Recent work has demonstrated substantial gains on many NLP  │                            │       │\n│\u001b[36m    \u001b[0m│ tasks and                                                             │                            │       │\n│\u001b[36m    \u001b[0m│ benchmarks by pre-training on a large corpus of text followed by      │                            │       │\n│\u001b[36m    \u001b[0m│ fine-tuning on                                                        │                            │       │\n│\u001b[36m    \u001b[0m│ a specific task. While typically task-agnostic i...                   │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m 5\u001b[0m\u001b[36m \u001b[0m│ Exploring the Limits of Transfer Learning with a Unified Text-to-Text │ Citations: 13834           │ \u001b]8;id=551434;https://arxiv.org/abs/1910.10683\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│   Transformer                                                         │ Versions: 16               │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Date Published: 2019-10-23 │       │\n│\u001b[36m    \u001b[0m│ LLM response: \"Exploring the Limits of Transfer Learning with a       │ Authors: Colin Raffel,     │       │\n│\u001b[36m    \u001b[0m│ Unified Text-to-Text Transformer\" paper explores NLP techniques.      │ Noam Shazeer, Adam         │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Roberts...                 │       │\n│\u001b[36m    \u001b[0m│ Abstract: Transfer learning, where a model is first pre-trained on a  │                            │       │\n│\u001b[36m    \u001b[0m│ data-rich task                                                        │                            │       │\n│\u001b[36m    \u001b[0m│ before being fine-tuned on a downstream task, has emerged as a        │                            │       │\n│\u001b[36m    \u001b[0m│ powerful                                                              │                            │       │\n│\u001b[36m    \u001b[0m│ technique in natural language processing (NLP). The...                │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m 6\u001b[0m\u001b[36m \u001b[0m│ Training language models to follow instructions with human feedback   │ Citations: 5366            │ \u001b]8;id=248438;https://arxiv.org/abs/2203.02155\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│                                                                       │ Versions: 17               │       │\n│\u001b[36m    \u001b[0m│ LLM response: Paper details fine-tuning language models with human    │ Date Published: 2022-03-04 │       │\n│\u001b[36m    \u001b[0m│ feedback for better alignment with user intent, mentions InstructGPT. │ Authors: Long Ouyang, Jeff │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Wu, Xu Jiang...            │       │\n│\u001b[36m    \u001b[0m│ Abstract: Making language models bigger does not inherently make them │                            │       │\n│\u001b[36m    \u001b[0m│ better at                                                             │                            │       │\n│\u001b[36m    \u001b[0m│ following a user's intent. For example, large language models can     │                            │       │\n│\u001b[36m    \u001b[0m│ generate                                                              │                            │       │\n│\u001b[36m    \u001b[0m│ outputs that are untruthful, toxic, or simply not he...               │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m 7\u001b[0m\u001b[36m \u001b[0m│ Outrageously Large Neural Networks: The Sparsely-Gated                │ Citations: 1702            │ \u001b]8;id=326574;https://arxiv.org/abs/1701.06538\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│   Mixture-of-Experts Layer                                            │ Versions: 16               │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Date Published: 2017-01-23 │       │\n│\u001b[36m    \u001b[0m│ LLM response: Paper discusses increasing neural network capacity via  │ Authors: Noam Shazeer,     │       │\n│\u001b[36m    \u001b[0m│ Sparsely-Gated Mixture-of-Experts layer, improving results.           │ Azalia Mirhoseini,         │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Krzysztof Maziarz...       │       │\n│\u001b[36m    \u001b[0m│ Abstract: The capacity of a neural network to absorb information is   │                            │       │\n│\u001b[36m    \u001b[0m│ limited by its                                                        │                            │       │\n│\u001b[36m    \u001b[0m│ number of parameters. Conditional computation, where parts of the     │                            │       │\n│\u001b[36m    \u001b[0m│ network are                                                           │                            │       │\n│\u001b[36m    \u001b[0m│ active on a per-example basis, has been propos...                     │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m 8\u001b[0m\u001b[36m \u001b[0m│ BART: Denoising Sequence-to-Sequence Pre-training for Natural         │ Citations: 8554            │ \u001b]8;id=989016;https://arxiv.org/abs/1910.13461\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│ Language                                                              │ Versions: 7                │       │\n│\u001b[36m    \u001b[0m│   Generation, Translation, and Comprehension                          │ Date Published: 2019-10-29 │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Authors: Mike Lewis,       │       │\n│\u001b[36m    \u001b[0m│ LLM response: \"BART: Denoising Sequence-to-Sequence Pre-training for  │ Yinhan Liu, Naman Goyal... │       │\n│\u001b[36m    \u001b[0m│ Natural Language Generation, Translation, and Comprehension\" by Lewis │                            │       │\n│\u001b[36m    \u001b[0m│ et al.                                                                │                            │       │\n│\u001b[36m    \u001b[0m│                                                                       │                            │       │\n│\u001b[36m    \u001b[0m│ Abstract: We present BART, a denoising autoencoder for pretraining    │                            │       │\n│\u001b[36m    \u001b[0m│ sequence-to-sequence                                                  │                            │       │\n│\u001b[36m    \u001b[0m│ models. BART is trained by (1) corrupting text with an arbitrary      │                            │       │\n│\u001b[36m    \u001b[0m│ noising                                                               │                            │       │\n│\u001b[36m    \u001b[0m│ function, and (2) learning a model to reconstr...                     │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m 9\u001b[0m\u001b[36m \u001b[0m│ Multitask Prompted Training Enables Zero-Shot Task Generalization     │ Citations: 1136            │ \u001b]8;id=745091;https://arxiv.org/abs/2110.08207\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│                                                                       │ Versions: 13               │       │\n│\u001b[36m    \u001b[0m│ LLM response: Study demonstrates that multitask prompted training     │ Date Published: 2021-10-15 │       │\n│\u001b[36m    \u001b[0m│ enhances zero-shot task generalization in large language models.      │ Authors: Victor Sanh,      │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Albert Webson, Colin       │       │\n│\u001b[36m    \u001b[0m│ Abstract: Large language models have recently been shown to attain    │ Raffel...                  │       │\n│\u001b[36m    \u001b[0m│ reasonable zero-shot                                                  │                            │       │\n│\u001b[36m    \u001b[0m│ generalization on a diverse set of tasks (Brown et al., 2020). It has │                            │       │\n│\u001b[36m    \u001b[0m│ been                                                                  │                            │       │\n│\u001b[36m    \u001b[0m│ hypothesized that this is a consequence of i...                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m10\u001b[0m\u001b[36m \u001b[0m│ Scaling Laws for Neural Language Models                               │ Citations: 1017            │ \u001b]8;id=390167;https://arxiv.org/abs/2001.08361\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│                                                                       │ Versions: 7                │       │\n│\u001b[36m    \u001b[0m│ LLM response: \"Scaling Laws for Neural Language Models\" studies how   │ Date Published: 2020-01-23 │       │\n│\u001b[36m    \u001b[0m│ model size, dataset size, and compute affect language model           │ Authors: Jared Kaplan, Sam │       │\n│\u001b[36m    \u001b[0m│ performance.                                                          │ McCandlish, Tom            │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Henighan...                │       │\n│\u001b[36m    \u001b[0m│ Abstract: We study empirical scaling laws for language model          │                            │       │\n│\u001b[36m    \u001b[0m│ performance on the                                                    │                            │       │\n│\u001b[36m    \u001b[0m│ cross-entropy loss. The loss scales as a power-law with model size,   │                            │       │\n│\u001b[36m    \u001b[0m│ dataset                                                               │                            │       │\n│\u001b[36m    \u001b[0m│ size, and the amount of compute used for training, ...                │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n└────┴───────────────────────────────────────────────────────────────────────┴────────────────────────────┴───────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                       LLMs                                                        </span>\n┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┓\n┃<span style=\"font-weight: bold\"> N… </span>┃<span style=\"font-weight: bold\"> Paper                                                                 </span>┃<span style=\"font-weight: bold\"> Details                    </span>┃<span style=\"font-weight: bold\"> Link  </span>┃\n┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">  1 </span>│ Attention Is All You Need                                             │ Citations: 111587          │ <a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Versions: 87               │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: \"Attention Is All You Need\" paper discusses             │ Date Published: 2017-06-12 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Transformer, a network architecture based on attention mechanisms.    │ Authors: Ashish Vaswani,   │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Noam Shazeer, Niki         │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: The dominant sequence transduction models are based on      │ Parmar...                  │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ complex recurrent or                                                  │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ convolutional neural networks in an encoder-decoder configuration.    │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ The best                                                              │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ performing models also connect the encoder an...                      │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">  2 </span>│ You Only Look Once: Unified, Real-Time Object Detection               │ Citations: 45087           │ <a href=\"https://arxiv.org/abs/1506.02640\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Versions: 51               │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: \"YOLO: New object detection approach frames detection   │ Date Published: 2015-06-08 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ as a regression problem, improving speed &amp; accuracy.\"                 │ Authors: Joseph Redmon,    │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Santosh Divvala, Ross      │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: We present YOLO, a new approach to object detection. Prior  │ Girshick...                │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ work on object                                                        │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ detection repurposes classifiers to perform detection. Instead, we    │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ frame object                                                          │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ detection as a regression problem to spatia...                        │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">  3 </span>│ BERT: Pre-training of Deep Bidirectional Transformers for Language    │ Citations: 94319           │ <a href=\"https://arxiv.org/abs/1810.04805\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│   Understanding                                                       │ Versions: 49               │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Date Published: 2018-10-11 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: \"BERT: Pre-training of Deep Bidirectional Transformers  │ Authors: Jacob Devlin,     │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ for Language Understanding\" presents a language model improving NLP   │ Ming-Wei Chang, Kenton     │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ tasks.                                                                │ Lee...                     │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: We introduce a new language representation model called     │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ BERT, which stands                                                    │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ for Bidirectional Encoder Representations from Transformers. Unlike   │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ recent                                                                │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ language representation models, BERT is designe...                    │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">  4 </span>│ Language Models are Few-Shot Learners                                 │ Citations: 22336           │ <a href=\"https://arxiv.org/abs/2005.14165\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Versions: 33               │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: \"Language Models are Few-Shot Learners\" paper discusses │ Date Published: 2020-05-28 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ how scaling up language models like GPT-3 improves few-shot           │ Authors: Tom B. Brown,     │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ performance in NLP tasks.                                             │ Benjamin Mann, Nick        │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Ryder...                   │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: Recent work has demonstrated substantial gains on many NLP  │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ tasks and                                                             │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ benchmarks by pre-training on a large corpus of text followed by      │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ fine-tuning on                                                        │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ a specific task. While typically task-agnostic i...                   │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">  5 </span>│ Exploring the Limits of Transfer Learning with a Unified Text-to-Text │ Citations: 13834           │ <a href=\"https://arxiv.org/abs/1910.10683\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│   Transformer                                                         │ Versions: 16               │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Date Published: 2019-10-23 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: \"Exploring the Limits of Transfer Learning with a       │ Authors: Colin Raffel,     │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Unified Text-to-Text Transformer\" paper explores NLP techniques.      │ Noam Shazeer, Adam         │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Roberts...                 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: Transfer learning, where a model is first pre-trained on a  │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ data-rich task                                                        │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ before being fine-tuned on a downstream task, has emerged as a        │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ powerful                                                              │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ technique in natural language processing (NLP). The...                │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">  6 </span>│ Training language models to follow instructions with human feedback   │ Citations: 5366            │ <a href=\"https://arxiv.org/abs/2203.02155\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Versions: 17               │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: Paper details fine-tuning language models with human    │ Date Published: 2022-03-04 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ feedback for better alignment with user intent, mentions InstructGPT. │ Authors: Long Ouyang, Jeff │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Wu, Xu Jiang...            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: Making language models bigger does not inherently make them │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ better at                                                             │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ following a user's intent. For example, large language models can     │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ generate                                                              │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ outputs that are untruthful, toxic, or simply not he...               │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">  7 </span>│ Outrageously Large Neural Networks: The Sparsely-Gated                │ Citations: 1702            │ <a href=\"https://arxiv.org/abs/1701.06538\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│   Mixture-of-Experts Layer                                            │ Versions: 16               │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Date Published: 2017-01-23 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: Paper discusses increasing neural network capacity via  │ Authors: Noam Shazeer,     │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Sparsely-Gated Mixture-of-Experts layer, improving results.           │ Azalia Mirhoseini,         │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Krzysztof Maziarz...       │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: The capacity of a neural network to absorb information is   │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ limited by its                                                        │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ number of parameters. Conditional computation, where parts of the     │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ network are                                                           │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ active on a per-example basis, has been propos...                     │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">  8 </span>│ BART: Denoising Sequence-to-Sequence Pre-training for Natural         │ Citations: 8554            │ <a href=\"https://arxiv.org/abs/1910.13461\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Language                                                              │ Versions: 7                │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│   Generation, Translation, and Comprehension                          │ Date Published: 2019-10-29 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Authors: Mike Lewis,       │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: \"BART: Denoising Sequence-to-Sequence Pre-training for  │ Yinhan Liu, Naman Goyal... │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Natural Language Generation, Translation, and Comprehension\" by Lewis │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ et al.                                                                │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: We present BART, a denoising autoencoder for pretraining    │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ sequence-to-sequence                                                  │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ models. BART is trained by (1) corrupting text with an arbitrary      │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ noising                                                               │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ function, and (2) learning a model to reconstr...                     │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">  9 </span>│ Multitask Prompted Training Enables Zero-Shot Task Generalization     │ Citations: 1136            │ <a href=\"https://arxiv.org/abs/2110.08207\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Versions: 13               │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: Study demonstrates that multitask prompted training     │ Date Published: 2021-10-15 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ enhances zero-shot task generalization in large language models.      │ Authors: Victor Sanh,      │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Albert Webson, Colin       │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: Large language models have recently been shown to attain    │ Raffel...                  │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ reasonable zero-shot                                                  │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ generalization on a diverse set of tasks (Brown et al., 2020). It has │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ been                                                                  │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ hypothesized that this is a consequence of i...                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\"> 10 </span>│ Scaling Laws for Neural Language Models                               │ Citations: 1017            │ <a href=\"https://arxiv.org/abs/2001.08361\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Versions: 7                │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: \"Scaling Laws for Neural Language Models\" studies how   │ Date Published: 2020-01-23 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ model size, dataset size, and compute affect language model           │ Authors: Jared Kaplan, Sam │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ performance.                                                          │ McCandlish, Tom            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Henighan...                │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: We study empirical scaling laws for language model          │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ performance on the                                                    │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ cross-entropy loss. The loss scales as a power-law with model size,   │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ dataset                                                               │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ size, and the amount of compute used for training, ...                │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n└────┴───────────────────────────────────────────────────────────────────────┴────────────────────────────┴───────┘\n</pre>\n"},"metadata":{},"output_type":"display_data"}],"outputs_reference":"s3:deepnote-cell-outputs-production/997c1b52-7d91-445c-bac8-51d61e82253c"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6d52007a-f237-4857-b1f1-3ccb95216ee4' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_full_width":true,"deepnote_app_clear_outputs":false,"deepnote_app_layout":"powerful-article","deepnote_app_hide_all_code_blocks_enabled":true,"deepnote_app_width":"full-width","deepnote_app_reactivity_enabled":true,"deepnote_notebook_id":"86f0990f6f9e4298bc5338753986dd91","deepnote_execution_queue":[]}}