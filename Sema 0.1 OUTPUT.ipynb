{"cells":[{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1709771878501,"execution_millis":83,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"15fdb985f7c742fb9532154af957d9be","deepnote_cell_type":"code"},"source":"","block_group":"15fdb985f7c742fb9532154af957d9be","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"3476add1","execution_start":1710042732513,"execution_millis":38,"deepnote_input_label":"Your search query here:","deepnote_variable_name":"input_query","deepnote_variable_value":"LLMs","deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"0d12df444d3941ea9782c85b51f6aa9b","deepnote_cell_type":"input-text"},"source":"input_query = 'LLMs'","block_group":"e19f0be04c9b49a192fa39a3858da19f","execution_count":115,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"bb9bc971","execution_start":1710042734375,"execution_millis":615,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"15a9423a4f54485497ecb05b9e9158c3","deepnote_cell_type":"code"},"source":"import psycopg2\nimport os\n\ndef connection():\n    \"\"\"Creates and returns a new database connection.\"\"\"\n    try:\n        conn = psycopg2.connect(\n            user=os.environ[\"MY_INTEGRATION_USER\"],\n            password=os.environ[\"MY_INTEGRATION_PASSWORD\"],\n            host=os.environ[\"MY_INTEGRATION_HOST\"],\n            port=os.environ[\"MY_INTEGRATION_PORT\"],\n            database=os.environ[\"MY_INTEGRATION_DATABASE\"]\n        )\n        \n        # Test the connection\n        with conn.cursor() as cursor:\n            cursor.execute(\"SELECT version();\")\n            record = cursor.fetchone()\n        \n        return conn  # Return the connection object if successful\n\n    except (Exception, psycopg2.Error) as error:\n        print(\"Error while connecting to database\", error)\n        return None  # Return None if connection was not successful\n\nconn = connection()","block_group":"35d1b25ccdc845448a6a60cc7807e612","execution_count":116,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"ed89c042","execution_start":1710042741481,"execution_millis":62,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"c2c4c9c84d3f42d78bfa325545dca282","deepnote_cell_type":"code"},"source":"import json\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich.text import Text\n\ndef display_query_papers(job_id):  # Change parameter to job_id\n    # Create a console object for Rich output\n    console = Console()\n\n    # Connect to the database\n    conn = connection()\n    c = conn.cursor()\n    \n    # Fetch the query and the number of ranks already printed for the given job_id\n    c.execute(\"SELECT query, printed_ranks FROM jobs WHERE job_id = %s\", (job_id,))\n    result = c.fetchone()\n    if result:\n        job_query, printed_ranks = result  # Unpack the result\n    else:\n        print(f\"No job found with ID: {job_id}\")\n        return  # Exit the function if no job is found\n\n    start_rank = printed_ranks # Start from the next rank\n\n    # Fetch records for the given query starting from the next rank to be printed\n    c.execute(\"\"\"\n        SELECT * FROM Query_Papers \n        WHERE query = %s AND final_rank >= %s AND final_rank IS NOT NULL \n        AND relevant_answer IS NOT NULL AND paper_stats IS NOT NULL \n        AND paper_metadata_filtered IS NOT NULL AND download_link IS NOT NULL\n        ORDER BY final_rank ASC\n        LIMIT 10\n    \"\"\", (job_query, start_rank))\n\n    # Fetch the column names\n    columns = [description[0] for description in c.description]\n\n    rows = c.fetchall()\n\n    # Counter for the number of ranks printed during this function call\n    ranks_printed_now = 0\n\n    if rows:\n        # Initialize a Rich table with improved formatting\n        table = Table(show_header=True, title=job_query, expand=True, leading=1, show_lines=True)\n        table.add_column(\"No.\", style=\"cyan\", justify=\"right\", ratio=1)\n        table.add_column(\"Paper\", overflow=\"fold\", ratio=20)  # This has twice the ratio of \"Details\", meaning it will be larger\n        table.add_column(\"Details\", overflow=\"fold\", ratio=8)  # Half the 'ratio' of \"Paper\", making it relatively smaller\n        table.add_column(\"Link\", justify=\"center\", ratio=2)\n        for row in rows:\n            # Extract the necessary fields from the row\n            final_rank = row[columns.index('final_rank')]\n            arxiv_link = row[columns.index('arxiv_link')]\n            relevant_answer = row[columns.index('relevant_answer')]\n            paper_stats = json.loads(row[columns.index('paper_stats')])\n            paper_metadata_filtered = json.loads(row[columns.index('paper_metadata_filtered')])\n            \n            # Format extracted data\n            title = paper_metadata_filtered.get('title', 'N/A')\n            abstract = paper_metadata_filtered.get('abstract', 'N/A')\n            abstract = (abstract[:197] + '...') if len(abstract) > 200 else abstract\n            published_date = paper_metadata_filtered.get('published_date', 'N/A').split('T')[0] if paper_metadata_filtered.get('published_date', 'N/A') != 'N/A' else 'N/A'\n            authors = paper_metadata_filtered.get('authors', ['N/A'])\n            authors_str = \", \".join(authors[:3]) + (\"...\" if len(authors) > 3 else \"\")\n            citations = paper_stats.get('citations', 'N/A')\n            versions = paper_stats.get('versions', 'N/A')\n\n            # Add the clickable 'Link' text\n            link_text = f\"[link={arxiv_link}]Link[/link]\"\n\n            # Format the Paper and Details columns\n            paper_column = Text(f\"{title}\\n\\nLLM response: {relevant_answer}\\n\\nAbstract: {abstract}\", justify=\"left\")\n            details_column = Text(f\"Citations: {citations}\\nVersions: {versions}\\nDate Published: {published_date}\\nAuthors: {authors_str} \\n\", justify=\"left\")\n            \n            # Add row with formatted data\n            table.add_row(str(final_rank), paper_column, details_column, link_text)\n            table.add_section()\n            table.add_row()\n\n            ranks_printed_now += 1\n\n        # Update the number of printed ranks in the jobs table for this query\n        new_total_printed = printed_ranks + ranks_printed_now\n        c.execute(\"UPDATE jobs SET printed_ranks = %s WHERE query = %s\", (new_total_printed, job_query))\n        conn.commit()\n    # Print the table to the console\n    if ranks_printed_now > 0:\n        console.print(table)\n    # Closing database connections\n    c.close()\n    conn.close()\n\n    return ranks_printed_now  # Optionally return the number of ranks printed in this call\n\n# # Example usage\n# job_query = \"Top academic papers on ReAct framework for agents\"\n# ranks_printed_now = display_query_papers(job_query)\n# print(f\"Ranks printed this time: {ranks_printed_now}\")\n","block_group":"1ee78ed7927d45708541c2dd376b88e7","execution_count":117,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"source_hash":"5073d8f8","execution_start":1710042743684,"execution_millis":795,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"ca5e4b7dbccb46c98171167049eedb67","deepnote_cell_type":"code"},"source":"def add_new_job(query):\n    # Connect to the database\n    conn = connection()\n    c = conn.cursor()\n\n    # SQL statement to insert a new job and return its id\n    c.execute(\"INSERT INTO jobs (query, job_status) VALUES (%s, 'new') RETURNING job_id\", (query,))\n\n    # Fetch the job_id of the newly inserted job\n    job_id = c.fetchone()[0]\n    print(\"\\nHello there, we're busy working on your query: '{}'. Job ID: {}\".format(query, job_id))\n\n    # Commit the changes and close the connection\n    conn.commit()\n    c.close()\n    conn.close()\n\n    # Return the job_id for further use\n    return job_id\n\n# Usage\n# input_query = \"your input query here\" \njob_query = \"Top academic papers on \" + input_query\njob_id = add_new_job(job_query)\n# print(\"Job ID for query '{}': {}\".format(job_query, job_id))\n","block_group":"90e9c9b2cc7140b9900be848b05e99b0","execution_count":118,"outputs":[{"name":"stdout","text":"\nHello there, we're busy working on your query: 'Top academic papers on LLMs'. Job ID: 7\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/553aacee-01e5-416a-997b-15047149ba7e"},{"cell_type":"code","metadata":{"source_hash":"5d293ae8","execution_start":1710043044452,"execution_millis":1632,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"deepnote_app_is_code_hidden":true,"cell_id":"a70c2578389e4b14b3a47a0798616c78","deepnote_cell_type":"code"},"source":"import sqlite3\nimport time\nimport datetime  # Import the datetime module\nimport os  # Import the os module for clearing the terminal\nfrom IPython.display import clear_output\n\ndef wait_for_job_completion(job_id):  # Use job_id instead of job_query as the function parameter\n    # Connect to the database\n    conn = connection()\n    c = conn.cursor()\n    counter = 0  # Initialize the counter\n\n    try:\n        while True:  # Keep checking until the job is done\n            # SQL statement to find the status of a job given its job_id\n            c.execute(\"SELECT job_status, query FROM jobs WHERE job_id = %s\", (job_id,))  # Use job_id to search\n            result = c.fetchone()\n\n            if result:\n                job_status, job_query = result  # Unpack the result into job_status and job_query\n                if job_status == 'done':\n                    clear_output(wait=True)  # Clear output and wait for the next\n                    display_query_papers(job_id)  # Assume this function should now work with job_query\n                    break  # Exit the loop if the job is done\n                elif job_status == 'running':\n                    clear_output(wait=True)  # Clear output and wait for the next\n                    counter += 1\n                    print(f\"Checking status ({counter}): \", end=' ')  # Print counter with \"Running...\"\n                    print(f\"The status of the job with ID '{job_id}' is currently '{job_status}'. Waiting for completion...\")\n                    time.sleep(1)  # Sleep for a while before checking again\n            else:\n                print(f\"\\nNo job found with ID: '{job_id}'.\")\n                break  # Exit the loop if no such job exists\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        # Close database resources\n        if conn:\n            c.close()\n            conn.close()\n\n# Usage example\nwait_for_job_completion(job_id)\n","block_group":"2bf8387b66b34b4986a7603189c6dfd1","execution_count":122,"outputs":[{"data":{"text/plain":"\u001b[3m                                            Top academic papers on LLMs                                            \u001b[0m\n┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mN…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPaper                                                                \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDetails                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLink \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m 1\u001b[0m\u001b[36m \u001b[0m│ Attention Is All You Need                                             │ Citations: 111166          │ \u001b]8;id=972161;https://arxiv.org/abs/1706.03762\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│                                                                       │ Versions: 87               │       │\n│\u001b[36m    \u001b[0m│ LLM response: \"Attention is All You Need” paper introduces a novel    │ Date Published: 2017-06-12 │       │\n│\u001b[36m    \u001b[0m│ architecture, the Transformer, to improve machine translation.        │ Authors: Ashish Vaswani,   │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Noam Shazeer, Niki         │       │\n│\u001b[36m    \u001b[0m│ Abstract: The dominant sequence transduction models are based on      │ Parmar...                  │       │\n│\u001b[36m    \u001b[0m│ complex recurrent or                                                  │                            │       │\n│\u001b[36m    \u001b[0m│ convolutional neural networks in an encoder-decoder configuration.    │                            │       │\n│\u001b[36m    \u001b[0m│ The best                                                              │                            │       │\n│\u001b[36m    \u001b[0m│ performing models also connect the encoder an...                      │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m 2\u001b[0m\u001b[36m \u001b[0m│ BERT: Pre-training of Deep Bidirectional Transformers for Language    │ Citations: 94058           │ \u001b]8;id=834434;https://arxiv.org/abs/1810.04805\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│   Understanding                                                       │ Versions: 49               │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Date Published: 2018-10-11 │       │\n│\u001b[36m    \u001b[0m│ LLM response: \"BERT\" paper presents a new model for deep              │ Authors: Jacob Devlin,     │       │\n│\u001b[36m    \u001b[0m│ bidirectional language understanding, improving various NLP tasks.    │ Ming-Wei Chang, Kenton     │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Lee...                     │       │\n│\u001b[36m    \u001b[0m│ Abstract: We introduce a new language representation model called     │                            │       │\n│\u001b[36m    \u001b[0m│ BERT, which stands                                                    │                            │       │\n│\u001b[36m    \u001b[0m│ for Bidirectional Encoder Representations from Transformers. Unlike   │                            │       │\n│\u001b[36m    \u001b[0m│ recent                                                                │                            │       │\n│\u001b[36m    \u001b[0m│ language representation models, BERT is designe...                    │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m 3\u001b[0m\u001b[36m \u001b[0m│ You Only Look Once: Unified, Real-Time Object Detection               │ Citations: 44968           │ \u001b]8;id=213480;https://arxiv.org/abs/1506.02640\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│                                                                       │ Versions: 51               │       │\n│\u001b[36m    \u001b[0m│ LLM response: The academic paper \"YOLO: Unified, Real-Time Object     │ Date Published: 2015-06-08 │       │\n│\u001b[36m    \u001b[0m│ Detection\" presents a fast, efficient method for object detection.    │ Authors: Joseph Redmon,    │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Santosh Divvala, Ross      │       │\n│\u001b[36m    \u001b[0m│ Abstract: We present YOLO, a new approach to object detection. Prior  │ Girshick...                │       │\n│\u001b[36m    \u001b[0m│ work on object                                                        │                            │       │\n│\u001b[36m    \u001b[0m│ detection repurposes classifiers to perform detection. Instead, we    │                            │       │\n│\u001b[36m    \u001b[0m│ frame object                                                          │                            │       │\n│\u001b[36m    \u001b[0m│ detection as a regression problem to spatia...                        │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m 4\u001b[0m\u001b[36m \u001b[0m│ Language Models are Few-Shot Learners                                 │ Citations: 22215           │ \u001b]8;id=675823;https://arxiv.org/abs/2005.14165\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│                                                                       │ Versions: 33               │       │\n│\u001b[36m    \u001b[0m│ LLM response: The paper discusses the impressive performance of       │ Date Published: 2020-05-28 │       │\n│\u001b[36m    \u001b[0m│ GPT-3, a language model, on various NLP tasks and its few-shot        │ Authors: Tom B. Brown,     │       │\n│\u001b[36m    \u001b[0m│ learning struggles.                                                   │ Benjamin Mann, Nick        │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Ryder...                   │       │\n│\u001b[36m    \u001b[0m│ Abstract: Recent work has demonstrated substantial gains on many NLP  │                            │       │\n│\u001b[36m    \u001b[0m│ tasks and                                                             │                            │       │\n│\u001b[36m    \u001b[0m│ benchmarks by pre-training on a large corpus of text followed by      │                            │       │\n│\u001b[36m    \u001b[0m│ fine-tuning on                                                        │                            │       │\n│\u001b[36m    \u001b[0m│ a specific task. While typically task-agnostic i...                   │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m 5\u001b[0m\u001b[36m \u001b[0m│ Exploring the Limits of Transfer Learning with a Unified Text-to-Text │ Citations: 13778           │ \u001b]8;id=371851;https://arxiv.org/abs/1910.10683\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│   Transformer                                                         │ Versions: 16               │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Date Published: 2019-10-23 │       │\n│\u001b[36m    \u001b[0m│ LLM response: \"Exploring the Limits of Transfer Learning with a       │ Authors: Colin Raffel,     │       │\n│\u001b[36m    \u001b[0m│ Unified Text-to-Text Transformer\" paper studies NLP transfer          │ Noam Shazeer, Adam         │       │\n│\u001b[36m    \u001b[0m│ learning.                                                             │ Roberts...                 │       │\n│\u001b[36m    \u001b[0m│                                                                       │                            │       │\n│\u001b[36m    \u001b[0m│ Abstract: Transfer learning, where a model is first pre-trained on a  │                            │       │\n│\u001b[36m    \u001b[0m│ data-rich task                                                        │                            │       │\n│\u001b[36m    \u001b[0m│ before being fine-tuned on a downstream task, has emerged as a        │                            │       │\n│\u001b[36m    \u001b[0m│ powerful                                                              │                            │       │\n│\u001b[36m    \u001b[0m│ technique in natural language processing (NLP). The...                │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m 6\u001b[0m\u001b[36m \u001b[0m│ Training language models to follow instructions with human feedback   │ Citations: 5319            │ \u001b]8;id=164261;https://arxiv.org/abs/2203.02155\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│                                                                       │ Versions: 17               │       │\n│\u001b[36m    \u001b[0m│ LLM response: Study shows InstructGPT models, fine-tuned using human  │ Date Published: 2022-03-04 │       │\n│\u001b[36m    \u001b[0m│ feedback, improve in truthfulness and reduce toxicity.                │ Authors: Long Ouyang, Jeff │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Wu, Xu Jiang...            │       │\n│\u001b[36m    \u001b[0m│ Abstract: Making language models bigger does not inherently make them │                            │       │\n│\u001b[36m    \u001b[0m│ better at                                                             │                            │       │\n│\u001b[36m    \u001b[0m│ following a user's intent. For example, large language models can     │                            │       │\n│\u001b[36m    \u001b[0m│ generate                                                              │                            │       │\n│\u001b[36m    \u001b[0m│ outputs that are untruthful, toxic, or simply not he...               │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m 7\u001b[0m\u001b[36m \u001b[0m│ Outrageously Large Neural Networks: The Sparsely-Gated                │ Citations: 1699            │ \u001b]8;id=782248;https://arxiv.org/abs/1701.06538\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│   Mixture-of-Experts Layer                                            │ Versions: 16               │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Date Published: 2017-01-23 │       │\n│\u001b[36m    \u001b[0m│ LLM response: Paper: Sparsely-Gated Mixture-of-Experts (MoE) layer    │ Authors: Noam Shazeer,     │       │\n│\u001b[36m    \u001b[0m│ increases neural network model capacity, improving results.           │ Azalia Mirhoseini,         │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Krzysztof Maziarz...       │       │\n│\u001b[36m    \u001b[0m│ Abstract: The capacity of a neural network to absorb information is   │                            │       │\n│\u001b[36m    \u001b[0m│ limited by its                                                        │                            │       │\n│\u001b[36m    \u001b[0m│ number of parameters. Conditional computation, where parts of the     │                            │       │\n│\u001b[36m    \u001b[0m│ network are                                                           │                            │       │\n│\u001b[36m    \u001b[0m│ active on a per-example basis, has been propos...                     │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m 8\u001b[0m\u001b[36m \u001b[0m│ BART: Denoising Sequence-to-Sequence Pre-training for Natural         │ Citations: 8516            │ \u001b]8;id=373944;https://arxiv.org/abs/1910.13461\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│ Language                                                              │ Versions: 7                │       │\n│\u001b[36m    \u001b[0m│   Generation, Translation, and Comprehension                          │ Date Published: 2019-10-29 │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Authors: Mike Lewis,       │       │\n│\u001b[36m    \u001b[0m│ LLM response: \"BART\" paper discusses a denoising autoencoder for      │ Yinhan Liu, Naman Goyal... │       │\n│\u001b[36m    \u001b[0m│ pretraining sequence-to-sequence models, improving language tasks.    │                            │       │\n│\u001b[36m    \u001b[0m│                                                                       │                            │       │\n│\u001b[36m    \u001b[0m│ Abstract: We present BART, a denoising autoencoder for pretraining    │                            │       │\n│\u001b[36m    \u001b[0m│ sequence-to-sequence                                                  │                            │       │\n│\u001b[36m    \u001b[0m│ models. BART is trained by (1) corrupting text with an arbitrary      │                            │       │\n│\u001b[36m    \u001b[0m│ noising                                                               │                            │       │\n│\u001b[36m    \u001b[0m│ function, and (2) learning a model to reconstr...                     │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m 9\u001b[0m\u001b[36m \u001b[0m│ Multitask Prompted Training Enables Zero-Shot Task Generalization     │ Citations: 1130            │ \u001b]8;id=383538;https://arxiv.org/abs/2110.08207\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│                                                                       │ Versions: 13               │       │\n│\u001b[36m    \u001b[0m│ LLM response: \"Multitask Prompted Training Enables Zero-Shot Task     │ Date Published: 2021-10-15 │       │\n│\u001b[36m    \u001b[0m│ Generalization\" paper details enhancing large language models via     │ Authors: Victor Sanh,      │       │\n│\u001b[36m    \u001b[0m│ explicit multitask learning.                                          │ Albert Webson, Colin       │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Raffel...                  │       │\n│\u001b[36m    \u001b[0m│ Abstract: Large language models have recently been shown to attain    │                            │       │\n│\u001b[36m    \u001b[0m│ reasonable zero-shot                                                  │                            │       │\n│\u001b[36m    \u001b[0m│ generalization on a diverse set of tasks (Brown et al., 2020). It has │                            │       │\n│\u001b[36m    \u001b[0m│ been                                                                  │                            │       │\n│\u001b[36m    \u001b[0m│ hypothesized that this is a consequence of i...                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m10\u001b[0m\u001b[36m \u001b[0m│ Scaling Laws for Neural Language Models                               │ Citations: 1015            │ \u001b]8;id=140002;https://arxiv.org/abs/2001.08361\u001b\\Link\u001b]8;;\u001b\\  │\n│\u001b[36m    \u001b[0m│                                                                       │ Versions: 7                │       │\n│\u001b[36m    \u001b[0m│ LLM response: \"Scaling Laws for Neural Language Models\" discusses     │ Date Published: 2020-01-23 │       │\n│\u001b[36m    \u001b[0m│ model performance, efficiency of large models, and optimal allocation │ Authors: Jared Kaplan, Sam │       │\n│\u001b[36m    \u001b[0m│ of compute budget.                                                    │ McCandlish, Tom            │       │\n│\u001b[36m    \u001b[0m│                                                                       │ Henighan...                │       │\n│\u001b[36m    \u001b[0m│ Abstract: We study empirical scaling laws for language model          │                            │       │\n│\u001b[36m    \u001b[0m│ performance on the                                                    │                            │       │\n│\u001b[36m    \u001b[0m│ cross-entropy loss. The loss scales as a power-law with model size,   │                            │       │\n│\u001b[36m    \u001b[0m│ dataset                                                               │                            │       │\n│\u001b[36m    \u001b[0m│ size, and the amount of compute used for training, ...                │                            │       │\n│    │                                                                       │                            │       │\n│\u001b[36m \u001b[0m\u001b[36m  \u001b[0m\u001b[36m \u001b[0m│                                                                       │                            │       │\n└────┴───────────────────────────────────────────────────────────────────────┴────────────────────────────┴───────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                            Top academic papers on LLMs                                            </span>\n┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┓\n┃<span style=\"font-weight: bold\"> N… </span>┃<span style=\"font-weight: bold\"> Paper                                                                 </span>┃<span style=\"font-weight: bold\"> Details                    </span>┃<span style=\"font-weight: bold\"> Link  </span>┃\n┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">  1 </span>│ Attention Is All You Need                                             │ Citations: 111166          │ <a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Versions: 87               │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: \"Attention is All You Need” paper introduces a novel    │ Date Published: 2017-06-12 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ architecture, the Transformer, to improve machine translation.        │ Authors: Ashish Vaswani,   │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Noam Shazeer, Niki         │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: The dominant sequence transduction models are based on      │ Parmar...                  │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ complex recurrent or                                                  │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ convolutional neural networks in an encoder-decoder configuration.    │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ The best                                                              │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ performing models also connect the encoder an...                      │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">  2 </span>│ BERT: Pre-training of Deep Bidirectional Transformers for Language    │ Citations: 94058           │ <a href=\"https://arxiv.org/abs/1810.04805\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│   Understanding                                                       │ Versions: 49               │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Date Published: 2018-10-11 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: \"BERT\" paper presents a new model for deep              │ Authors: Jacob Devlin,     │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ bidirectional language understanding, improving various NLP tasks.    │ Ming-Wei Chang, Kenton     │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Lee...                     │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: We introduce a new language representation model called     │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ BERT, which stands                                                    │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ for Bidirectional Encoder Representations from Transformers. Unlike   │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ recent                                                                │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ language representation models, BERT is designe...                    │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">  3 </span>│ You Only Look Once: Unified, Real-Time Object Detection               │ Citations: 44968           │ <a href=\"https://arxiv.org/abs/1506.02640\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Versions: 51               │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: The academic paper \"YOLO: Unified, Real-Time Object     │ Date Published: 2015-06-08 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Detection\" presents a fast, efficient method for object detection.    │ Authors: Joseph Redmon,    │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Santosh Divvala, Ross      │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: We present YOLO, a new approach to object detection. Prior  │ Girshick...                │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ work on object                                                        │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ detection repurposes classifiers to perform detection. Instead, we    │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ frame object                                                          │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ detection as a regression problem to spatia...                        │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">  4 </span>│ Language Models are Few-Shot Learners                                 │ Citations: 22215           │ <a href=\"https://arxiv.org/abs/2005.14165\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Versions: 33               │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: The paper discusses the impressive performance of       │ Date Published: 2020-05-28 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ GPT-3, a language model, on various NLP tasks and its few-shot        │ Authors: Tom B. Brown,     │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ learning struggles.                                                   │ Benjamin Mann, Nick        │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Ryder...                   │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: Recent work has demonstrated substantial gains on many NLP  │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ tasks and                                                             │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ benchmarks by pre-training on a large corpus of text followed by      │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ fine-tuning on                                                        │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ a specific task. While typically task-agnostic i...                   │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">  5 </span>│ Exploring the Limits of Transfer Learning with a Unified Text-to-Text │ Citations: 13778           │ <a href=\"https://arxiv.org/abs/1910.10683\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│   Transformer                                                         │ Versions: 16               │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Date Published: 2019-10-23 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: \"Exploring the Limits of Transfer Learning with a       │ Authors: Colin Raffel,     │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Unified Text-to-Text Transformer\" paper studies NLP transfer          │ Noam Shazeer, Adam         │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ learning.                                                             │ Roberts...                 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: Transfer learning, where a model is first pre-trained on a  │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ data-rich task                                                        │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ before being fine-tuned on a downstream task, has emerged as a        │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ powerful                                                              │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ technique in natural language processing (NLP). The...                │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">  6 </span>│ Training language models to follow instructions with human feedback   │ Citations: 5319            │ <a href=\"https://arxiv.org/abs/2203.02155\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Versions: 17               │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: Study shows InstructGPT models, fine-tuned using human  │ Date Published: 2022-03-04 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ feedback, improve in truthfulness and reduce toxicity.                │ Authors: Long Ouyang, Jeff │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Wu, Xu Jiang...            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: Making language models bigger does not inherently make them │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ better at                                                             │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ following a user's intent. For example, large language models can     │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ generate                                                              │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ outputs that are untruthful, toxic, or simply not he...               │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">  7 </span>│ Outrageously Large Neural Networks: The Sparsely-Gated                │ Citations: 1699            │ <a href=\"https://arxiv.org/abs/1701.06538\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│   Mixture-of-Experts Layer                                            │ Versions: 16               │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Date Published: 2017-01-23 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: Paper: Sparsely-Gated Mixture-of-Experts (MoE) layer    │ Authors: Noam Shazeer,     │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ increases neural network model capacity, improving results.           │ Azalia Mirhoseini,         │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Krzysztof Maziarz...       │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: The capacity of a neural network to absorb information is   │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ limited by its                                                        │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ number of parameters. Conditional computation, where parts of the     │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ network are                                                           │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ active on a per-example basis, has been propos...                     │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">  8 </span>│ BART: Denoising Sequence-to-Sequence Pre-training for Natural         │ Citations: 8516            │ <a href=\"https://arxiv.org/abs/1910.13461\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Language                                                              │ Versions: 7                │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│   Generation, Translation, and Comprehension                          │ Date Published: 2019-10-29 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Authors: Mike Lewis,       │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: \"BART\" paper discusses a denoising autoencoder for      │ Yinhan Liu, Naman Goyal... │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ pretraining sequence-to-sequence models, improving language tasks.    │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: We present BART, a denoising autoencoder for pretraining    │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ sequence-to-sequence                                                  │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ models. BART is trained by (1) corrupting text with an arbitrary      │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ noising                                                               │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ function, and (2) learning a model to reconstr...                     │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">  9 </span>│ Multitask Prompted Training Enables Zero-Shot Task Generalization     │ Citations: 1130            │ <a href=\"https://arxiv.org/abs/2110.08207\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Versions: 13               │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: \"Multitask Prompted Training Enables Zero-Shot Task     │ Date Published: 2021-10-15 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Generalization\" paper details enhancing large language models via     │ Authors: Victor Sanh,      │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ explicit multitask learning.                                          │ Albert Webson, Colin       │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Raffel...                  │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: Large language models have recently been shown to attain    │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ reasonable zero-shot                                                  │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ generalization on a diverse set of tasks (Brown et al., 2020). It has │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ been                                                                  │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ hypothesized that this is a consequence of i...                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\"> 10 </span>│ Scaling Laws for Neural Language Models                               │ Citations: 1015            │ <a href=\"https://arxiv.org/abs/2001.08361\" target=\"_blank\">Link</a>  │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Versions: 7                │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ LLM response: \"Scaling Laws for Neural Language Models\" discusses     │ Date Published: 2020-01-23 │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ model performance, efficiency of large models, and optimal allocation │ Authors: Jared Kaplan, Sam │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ of compute budget.                                                    │ McCandlish, Tom            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │ Henighan...                │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ Abstract: We study empirical scaling laws for language model          │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ performance on the                                                    │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ cross-entropy loss. The loss scales as a power-law with model size,   │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ dataset                                                               │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│ size, and the amount of compute used for training, ...                │                            │       │\n│    │                                                                       │                            │       │\n│<span style=\"color: #008080; text-decoration-color: #008080\">    </span>│                                                                       │                            │       │\n└────┴───────────────────────────────────────────────────────────────────────┴────────────────────────────┴───────┘\n</pre>\n"},"metadata":{},"output_type":"display_data"}],"outputs_reference":"s3:deepnote-cell-outputs-production/2d4c3905-478b-4226-b03b-e5ac45261a41"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6d52007a-f237-4857-b1f1-3ccb95216ee4' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_full_width":true,"deepnote_app_clear_outputs":false,"deepnote_app_layout":"powerful-article","deepnote_app_hide_all_code_blocks_enabled":true,"deepnote_app_width":"full-width","deepnote_app_reactivity_enabled":true,"deepnote_notebook_id":"86f0990f6f9e4298bc5338753986dd91","deepnote_execution_queue":[]}}